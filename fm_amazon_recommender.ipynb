{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Recommender System with Amazon SageMaker Factorization Machines and BlazingText\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "- Recommender systems were a catalyst for ML's popularity (Amazon, Netflix Prize)\n",
    "- User item matrix factorization is a core methodology\n",
    "- Factorization machines combine linear prediction with a factorized representation of pairwise feature interaction\n",
    "\n",
    "$$\\hat{r} = w_0 + \\sum_{i} {w_i x_i} + \\sum_{i} {\\sum_{j > i} {\\langle v_i, v_j \\rangle x_i x_j}}$$\n",
    "\n",
    "- SageMaker has a highly scalable factorization machines algorithm built-in\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "1. Spin up SageMaker hosted notebook instance in console\n",
    "2. Add SageMaker IAM policy to this SageMaker notebook to allow S3 read/write access\n",
    "3. Instantiate S3 bucket (first code cell)\n",
    "4. Import necessary libraries (second code cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "base = 'DEMO-loft-recommender'\n",
    "prefix = 'sagemaker/' + base\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import json\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker.predictor import json_deserializer\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data\n",
    "\n",
    "[Amazon Reviews AWS Public Dataset](https://s3.amazonaws.com/amazon-reviews-pds/readme.html)\n",
    "\n",
    "Amazon Customer Reviews (a.k.a. Product Reviews) is one of Amazon’s iconic products. In a period of over two decades since the first review in 1995, millions of Amazon customers have contributed over a hundred million reviews to express opinions and describe their experiences regarding products on the Amazon.com website. This makes Amazon Customer Reviews a rich source of information for academic researchers in the fields of Natural Language Processing (NLP), Information Retrieval (IR), and Machine Learning (ML), amongst others. Accordingly, we are releasing this data to further research in multiple disciplines related to understanding customer product experiences. Specifically, this dataset was constructed to represent a sample of customer evaluations and opinions, variation in the perception of a product across geographical regions, and promotional intent or bias in reviews.\n",
    "\n",
    "- 1 to 5 star ratings\n",
    "- 2M+ Amazon customers\n",
    "- 160K+ digital videos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/tmp/recsys/’: File exists\n",
      "download: s3://amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz to ../../../../tmp/recsys/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "!mkdir /tmp/recsys/\n",
    "!aws s3 cp s3://amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz /tmp/recsys/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 92523: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 343254: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 524626: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 623024: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 977412: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1496867: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1711638: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1787213: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2395306: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2527690: expected 15 fields, saw 22\\n'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>12190288</td>\n",
       "      <td>R3FU16928EP5TC</td>\n",
       "      <td>B00AYB1482</td>\n",
       "      <td>668895143</td>\n",
       "      <td>Enlightened: Season 1</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>I loved it and I wish there was a season 3</td>\n",
       "      <td>I loved it and I wish there was a season 3... ...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>30549954</td>\n",
       "      <td>R1IZHHS1MH3AQ4</td>\n",
       "      <td>B00KQD28OM</td>\n",
       "      <td>246219280</td>\n",
       "      <td>Vicious</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>As always it seems that the best shows come fr...</td>\n",
       "      <td>As always it seems that the best shows come fr...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>52895410</td>\n",
       "      <td>R52R85WC6TIAH</td>\n",
       "      <td>B01489L5LQ</td>\n",
       "      <td>534732318</td>\n",
       "      <td>After Words</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Charming movie</td>\n",
       "      <td>This movie isn't perfect, but it gets a lot of...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>27072354</td>\n",
       "      <td>R7HOOYTVIB0DS</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>239012694</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>excellant this is what tv should be</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>26939022</td>\n",
       "      <td>R1XQ2N5CDOZGNX</td>\n",
       "      <td>B0094LZMT0</td>\n",
       "      <td>535858974</td>\n",
       "      <td>On The Waterfront</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Brilliant film from beginning to end</td>\n",
       "      <td>Brilliant film from beginning to end. All of t...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     12190288  R3FU16928EP5TC  B00AYB1482       668895143   \n",
       "1          US     30549954  R1IZHHS1MH3AQ4  B00KQD28OM       246219280   \n",
       "2          US     52895410   R52R85WC6TIAH  B01489L5LQ       534732318   \n",
       "3          US     27072354   R7HOOYTVIB0DS  B008LOVIIK       239012694   \n",
       "4          US     26939022  R1XQ2N5CDOZGNX  B0094LZMT0       535858974   \n",
       "\n",
       "                           product_title        product_category  star_rating  \\\n",
       "0                  Enlightened: Season 1  Digital_Video_Download            5   \n",
       "1                                Vicious  Digital_Video_Download            5   \n",
       "2                            After Words  Digital_Video_Download            4   \n",
       "3  Masterpiece: Inspector Lewis Season 5  Digital_Video_Download            5   \n",
       "4                      On The Waterfront  Digital_Video_Download            5   \n",
       "\n",
       "   helpful_votes  total_votes vine verified_purchase  \\\n",
       "0              0            0    N                 Y   \n",
       "1              0            0    N                 Y   \n",
       "2             17           18    N                 Y   \n",
       "3              0            0    N                 Y   \n",
       "4              0            0    N                 Y   \n",
       "\n",
       "                                     review_headline  \\\n",
       "0         I loved it and I wish there was a season 3   \n",
       "1  As always it seems that the best shows come fr...   \n",
       "2                                     Charming movie   \n",
       "3                                         Five Stars   \n",
       "4               Brilliant film from beginning to end   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0  I loved it and I wish there was a season 3... ...  2015-08-31  \n",
       "1  As always it seems that the best shows come fr...  2015-08-31  \n",
       "2  This movie isn't perfect, but it gets a lot of...  2015-08-31  \n",
       "3                excellant this is what tv should be  2015-08-31  \n",
       "4  Brilliant film from beginning to end. All of t...  2015-08-31  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/tmp/recsys/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz', delimiter='\\t',error_bad_lines=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset columns:\n",
    "\n",
    "- `marketplace`: 2-letter country code (in this case all \"US\").\n",
    "- `customer_id`: Random identifier that can be used to aggregate reviews written by a single author.\n",
    "- `review_id`: A unique ID for the review.\n",
    "- `product_id`: The Amazon Standard Identification Number (ASIN).  `http://www.amazon.com/dp/<ASIN>` links to the product's detail page.\n",
    "- `product_parent`: The parent of that ASIN.  Multiple ASINs (color or format variations of the same product) can roll up into a single parent parent.\n",
    "- `product_title`: Title description of the product.\n",
    "- `product_category`: Broad product category that can be used to group reviews (in this case digital videos).\n",
    "- `star_rating`: The review's rating (1 to 5 stars).\n",
    "- `helpful_votes`: Number of helpful votes for the review.\n",
    "- `total_votes`: Number of total votes the review received.\n",
    "- `vine`: Was the review written as part of the [Vine](https://www.amazon.com/gp/vine/help) program?\n",
    "- `verified_purchase`: Was the review from a verified purchase?\n",
    "- `review_headline`: The title of the review itself.\n",
    "- `review_body`: The text of the review.\n",
    "- `review_date`: The date the review was written.\n",
    "\n",
    "Drop some fields that won't be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['customer_id', 'product_id', 'product_title', 'star_rating', 'review_date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you execute the following cell, you will see most users don't rate most movies. Check out the long tail.\n",
    "\n",
    "For example of customers, upto 50 percentile (see 0.50 row), the count is 1. It means those users submitted only one review. Among 2 million unique users, more than a half submitted only one review. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customers count\n",
      " 2018155\n",
      "customers\n",
      " 0.00       1.0\n",
      "0.01       1.0\n",
      "0.02       1.0\n",
      "0.03       1.0\n",
      "0.04       1.0\n",
      "0.05       1.0\n",
      "0.10       1.0\n",
      "0.25       1.0\n",
      "0.50       1.0\n",
      "0.75       2.0\n",
      "0.90       4.0\n",
      "0.95       5.0\n",
      "0.96       6.0\n",
      "0.97       7.0\n",
      "0.98       9.0\n",
      "0.99      13.0\n",
      "1.00    2704.0\n",
      "Name: customer_id, dtype: float64\n",
      "products count \n",
      " 166034\n",
      "products\n",
      " 0.00        1.00\n",
      "0.01        1.00\n",
      "0.02        1.00\n",
      "0.03        1.00\n",
      "0.04        1.00\n",
      "0.05        1.00\n",
      "0.10        1.00\n",
      "0.25        1.00\n",
      "0.50        3.00\n",
      "0.75        9.00\n",
      "0.90       31.00\n",
      "0.95       73.00\n",
      "0.96       95.00\n",
      "0.97      130.00\n",
      "0.98      199.00\n",
      "0.99      386.67\n",
      "1.00    32790.00\n",
      "Name: product_id, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "customers = df['customer_id'].value_counts()\n",
    "products = df['product_id'].value_counts()\n",
    "\n",
    "quantiles = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.96, 0.97, 0.98, 0.99, 1]\n",
    "print('customers count\\n', customers.size)\n",
    "print('customers\\n', customers.quantile(quantiles))\n",
    "\n",
    "print('products count \\n', products.size)\n",
    "print('products\\n', products.quantile(quantiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, filter out customers who haven't rated many movies\n",
    "\n",
    "You will keep customers who submitted more than 5 reviews. You will also keep movies which received more than 10 reviews. \n",
    "\n",
    "By doing so, the customer count is now about 140K, and product count is about 38K. You can view that by entering customer_index and product_index in a separate cell and execute it in the subsequent cell.\n",
    "\n",
    "The third line of code in the following cell saves customerId and productId as reduced_df in the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = customers[customers >= 5]\n",
    "products = products[products >= 10]\n",
    "\n",
    "reduced_df = df.merge(pd.DataFrame({'customer_id': customers.index})).merge(pd.DataFrame({'product_id': products.index}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a sequential index for customers and movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = reduced_df['customer_id'].value_counts()\n",
    "products = reduced_df['product_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_title</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_date</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27072354</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>10463</td>\n",
       "      <td>140450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16030865</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-06-20</td>\n",
       "      <td>489</td>\n",
       "      <td>140450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44025160</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-05-27</td>\n",
       "      <td>32100</td>\n",
       "      <td>140450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18602179</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-12-23</td>\n",
       "      <td>2237</td>\n",
       "      <td>140450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14424972</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>32340</td>\n",
       "      <td>140450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  product_id                          product_title  \\\n",
       "0     27072354  B008LOVIIK  Masterpiece: Inspector Lewis Season 5   \n",
       "1     16030865  B008LOVIIK  Masterpiece: Inspector Lewis Season 5   \n",
       "2     44025160  B008LOVIIK  Masterpiece: Inspector Lewis Season 5   \n",
       "3     18602179  B008LOVIIK  Masterpiece: Inspector Lewis Season 5   \n",
       "4     14424972  B008LOVIIK  Masterpiece: Inspector Lewis Season 5   \n",
       "\n",
       "   star_rating review_date   user    item  \n",
       "0            5  2015-08-31  10463  140450  \n",
       "1            5  2014-06-20    489  140450  \n",
       "2            5  2014-05-27  32100  140450  \n",
       "3            5  2014-12-23   2237  140450  \n",
       "4            5  2015-08-31  32340  140450  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_index = pd.DataFrame({'customer_id': customers.index, 'user': np.arange(customers.shape[0])})\n",
    "product_index = pd.DataFrame({'product_id': products.index, \n",
    "                              'item': np.arange(products.shape[0]) + customer_index.shape[0]})\n",
    "\n",
    "reduced_df = reduced_df.merge(customer_index).merge(product_index)\n",
    "reduced_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count days since first review (included as a feature to capture trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df['review_date'] = pd.to_datetime(reduced_df['review_date'])\n",
    "customer_first_date = reduced_df.groupby('customer_id')['review_date'].min().reset_index()\n",
    "customer_first_date.columns = ['customer_id', 'first_review_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = reduced_df.merge(customer_first_date)\n",
    "reduced_df['days_since_first'] = (reduced_df['review_date'] - reduced_df['first_review_date']).dt.days\n",
    "reduced_df['days_since_first'] = reduced_df['days_since_first'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_title</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_date</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>first_review_date</th>\n",
       "      <th>days_since_first</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27072354</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>10463</td>\n",
       "      <td>140450</td>\n",
       "      <td>2015-04-21</td>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27072354</td>\n",
       "      <td>B003V8GGA6</td>\n",
       "      <td>Wallace &amp; Gromit's Cracking Contraptions</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>10463</td>\n",
       "      <td>144933</td>\n",
       "      <td>2015-04-21</td>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27072354</td>\n",
       "      <td>B00SY9HO8U</td>\n",
       "      <td>Suburban Gothic</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-07-22</td>\n",
       "      <td>10463</td>\n",
       "      <td>142602</td>\n",
       "      <td>2015-04-21</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27072354</td>\n",
       "      <td>B008Y7EYSK</td>\n",
       "      <td>The Woman in Black</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-07-22</td>\n",
       "      <td>10463</td>\n",
       "      <td>140712</td>\n",
       "      <td>2015-04-21</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27072354</td>\n",
       "      <td>B0079W7X98</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 2</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-06-16</td>\n",
       "      <td>10463</td>\n",
       "      <td>140411</td>\n",
       "      <td>2015-04-21</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27072354</td>\n",
       "      <td>B004AVPV7C</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 3</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-06-16</td>\n",
       "      <td>10463</td>\n",
       "      <td>140564</td>\n",
       "      <td>2015-04-21</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>27072354</td>\n",
       "      <td>B00NL5R7NO</td>\n",
       "      <td>Ken Burns: The Roosevelts - An Intimate Histor...</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-05-18</td>\n",
       "      <td>10463</td>\n",
       "      <td>140475</td>\n",
       "      <td>2015-04-21</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27072354</td>\n",
       "      <td>B001BWQ0XM</td>\n",
       "      <td>The X-Files Season 1</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-05-18</td>\n",
       "      <td>10463</td>\n",
       "      <td>140813</td>\n",
       "      <td>2015-04-21</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27072354</td>\n",
       "      <td>B005LAN0AG</td>\n",
       "      <td>Masterpiece: Inspector Lewis, Season 4</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-05-18</td>\n",
       "      <td>10463</td>\n",
       "      <td>140557</td>\n",
       "      <td>2015-04-21</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>27072354</td>\n",
       "      <td>B008VVET4S</td>\n",
       "      <td>Black Death</td>\n",
       "      <td>3</td>\n",
       "      <td>2015-05-18</td>\n",
       "      <td>10463</td>\n",
       "      <td>145847</td>\n",
       "      <td>2015-04-21</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>27072354</td>\n",
       "      <td>B006QEZK0O</td>\n",
       "      <td>Decoding Alan Turing</td>\n",
       "      <td>3</td>\n",
       "      <td>2015-05-18</td>\n",
       "      <td>10463</td>\n",
       "      <td>152158</td>\n",
       "      <td>2015-04-21</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>27072354</td>\n",
       "      <td>B00L2GQLL8</td>\n",
       "      <td>The Escape Artist Season 1</td>\n",
       "      <td>3</td>\n",
       "      <td>2015-05-18</td>\n",
       "      <td>10463</td>\n",
       "      <td>140595</td>\n",
       "      <td>2015-04-21</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>27072354</td>\n",
       "      <td>B00LCHENE4</td>\n",
       "      <td>Endeavour Season 2</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-04-30</td>\n",
       "      <td>10463</td>\n",
       "      <td>140537</td>\n",
       "      <td>2015-04-21</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>27072354</td>\n",
       "      <td>B00DUUKV4M</td>\n",
       "      <td>Masterpiece: Endeavour, Season One</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-04-21</td>\n",
       "      <td>10463</td>\n",
       "      <td>140431</td>\n",
       "      <td>2015-04-21</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>27072354</td>\n",
       "      <td>B00MGFX0O0</td>\n",
       "      <td>The Zero Theorem</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-04-21</td>\n",
       "      <td>10463</td>\n",
       "      <td>140954</td>\n",
       "      <td>2015-04-21</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>27072354</td>\n",
       "      <td>B00JB3MVCW</td>\n",
       "      <td>Noah</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-04-21</td>\n",
       "      <td>10463</td>\n",
       "      <td>140379</td>\n",
       "      <td>2015-04-21</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16030865</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-06-20</td>\n",
       "      <td>489</td>\n",
       "      <td>140450</td>\n",
       "      <td>2013-02-10</td>\n",
       "      <td>495.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16030865</td>\n",
       "      <td>B0079W7X98</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 2</td>\n",
       "      <td>5</td>\n",
       "      <td>2013-08-15</td>\n",
       "      <td>489</td>\n",
       "      <td>140411</td>\n",
       "      <td>2013-02-10</td>\n",
       "      <td>186.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>16030865</td>\n",
       "      <td>B00X8UKOUK</td>\n",
       "      <td>Catastrophe - Season 1</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-07-05</td>\n",
       "      <td>489</td>\n",
       "      <td>140353</td>\n",
       "      <td>2013-02-10</td>\n",
       "      <td>875.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>16030865</td>\n",
       "      <td>B00I3MQNWG</td>\n",
       "      <td>Bosch Season 1</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-03-04</td>\n",
       "      <td>489</td>\n",
       "      <td>140344</td>\n",
       "      <td>2013-02-10</td>\n",
       "      <td>387.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>16030865</td>\n",
       "      <td>B00CDZFRAI</td>\n",
       "      <td>Alpha House Season 1</td>\n",
       "      <td>5</td>\n",
       "      <td>2013-12-14</td>\n",
       "      <td>489</td>\n",
       "      <td>140362</td>\n",
       "      <td>2013-02-10</td>\n",
       "      <td>307.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>16030865</td>\n",
       "      <td>B00CW8DRU8</td>\n",
       "      <td>The Newsroom: Season 1</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-30</td>\n",
       "      <td>489</td>\n",
       "      <td>140404</td>\n",
       "      <td>2013-02-10</td>\n",
       "      <td>900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>16030865</td>\n",
       "      <td>B00O5ATHEQ</td>\n",
       "      <td>Alpha House Season 2</td>\n",
       "      <td>4</td>\n",
       "      <td>2014-11-07</td>\n",
       "      <td>489</td>\n",
       "      <td>140367</td>\n",
       "      <td>2013-02-10</td>\n",
       "      <td>635.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>16030865</td>\n",
       "      <td>B000JQYE34</td>\n",
       "      <td>NYPD Blue Season 1</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-03-12</td>\n",
       "      <td>489</td>\n",
       "      <td>140540</td>\n",
       "      <td>2013-02-10</td>\n",
       "      <td>395.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>16030865</td>\n",
       "      <td>B00OOKUC46</td>\n",
       "      <td>The Newsroom Season 2</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-08-08</td>\n",
       "      <td>489</td>\n",
       "      <td>140695</td>\n",
       "      <td>2013-02-10</td>\n",
       "      <td>909.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16030865</td>\n",
       "      <td>B00D1NFIVO</td>\n",
       "      <td>The Glades Season 4</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-06-27</td>\n",
       "      <td>489</td>\n",
       "      <td>140727</td>\n",
       "      <td>2013-02-10</td>\n",
       "      <td>502.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>16030865</td>\n",
       "      <td>B0052NAB3Q</td>\n",
       "      <td>The Glades Season 2</td>\n",
       "      <td>4</td>\n",
       "      <td>2014-04-22</td>\n",
       "      <td>489</td>\n",
       "      <td>140816</td>\n",
       "      <td>2013-02-10</td>\n",
       "      <td>436.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>16030865</td>\n",
       "      <td>B006VRDZ44</td>\n",
       "      <td>Boardwalk Empire: Season 1</td>\n",
       "      <td>3</td>\n",
       "      <td>2015-06-16</td>\n",
       "      <td>489</td>\n",
       "      <td>140383</td>\n",
       "      <td>2013-02-10</td>\n",
       "      <td>856.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>16030865</td>\n",
       "      <td>B000K78Q3Q</td>\n",
       "      <td>NYPD Blue Season 2</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-03-28</td>\n",
       "      <td>489</td>\n",
       "      <td>140977</td>\n",
       "      <td>2013-02-10</td>\n",
       "      <td>411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>16030865</td>\n",
       "      <td>B00JM034NO</td>\n",
       "      <td>Gimme Shelter</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>489</td>\n",
       "      <td>140873</td>\n",
       "      <td>2013-02-10</td>\n",
       "      <td>932.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168925</th>\n",
       "      <td>13378474</td>\n",
       "      <td>B00DU73X7C</td>\n",
       "      <td>Hollywood Exes Season 2</td>\n",
       "      <td>4</td>\n",
       "      <td>2013-08-22</td>\n",
       "      <td>139882</td>\n",
       "      <td>168140</td>\n",
       "      <td>2013-08-22</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168926</th>\n",
       "      <td>16029276</td>\n",
       "      <td>B00G9GYZGO</td>\n",
       "      <td>VICTORiOUS Volume 3</td>\n",
       "      <td>4</td>\n",
       "      <td>2014-05-06</td>\n",
       "      <td>140181</td>\n",
       "      <td>173842</td>\n",
       "      <td>2014-05-06</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168927</th>\n",
       "      <td>31896430</td>\n",
       "      <td>B0044S3KAQ</td>\n",
       "      <td>Family Guy Season 9</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-02-16</td>\n",
       "      <td>140018</td>\n",
       "      <td>165684</td>\n",
       "      <td>2014-02-16</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168928</th>\n",
       "      <td>21204701</td>\n",
       "      <td>B00L3L4GJ6</td>\n",
       "      <td>Kurt Metzger: White Precious</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-04-16</td>\n",
       "      <td>140210</td>\n",
       "      <td>177146</td>\n",
       "      <td>2015-04-16</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168929</th>\n",
       "      <td>51029362</td>\n",
       "      <td>B00H8V0ZRW</td>\n",
       "      <td>Sea of Creepy Monsters</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-08-19</td>\n",
       "      <td>140173</td>\n",
       "      <td>168392</td>\n",
       "      <td>2014-08-19</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168930</th>\n",
       "      <td>25982685</td>\n",
       "      <td>B00O40CSBQ</td>\n",
       "      <td>P-51 Dragon Fighter</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-08-10</td>\n",
       "      <td>139865</td>\n",
       "      <td>175028</td>\n",
       "      <td>2015-08-10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168931</th>\n",
       "      <td>21848453</td>\n",
       "      <td>B008JTXWOK</td>\n",
       "      <td>Wanted</td>\n",
       "      <td>5</td>\n",
       "      <td>2012-09-06</td>\n",
       "      <td>139991</td>\n",
       "      <td>172195</td>\n",
       "      <td>2012-09-06</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168932</th>\n",
       "      <td>10157421</td>\n",
       "      <td>B00HYJECB8</td>\n",
       "      <td>Are You The One? Season 1</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-03-17</td>\n",
       "      <td>139945</td>\n",
       "      <td>175524</td>\n",
       "      <td>2014-03-17</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168933</th>\n",
       "      <td>20490375</td>\n",
       "      <td>B005VA7XVS</td>\n",
       "      <td>Shawn Rescues Darth Vader</td>\n",
       "      <td>4</td>\n",
       "      <td>2012-05-11</td>\n",
       "      <td>138965</td>\n",
       "      <td>174140</td>\n",
       "      <td>2012-05-11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168934</th>\n",
       "      <td>20490375</td>\n",
       "      <td>B005XI57GG</td>\n",
       "      <td>Last Night Gus</td>\n",
       "      <td>5</td>\n",
       "      <td>2012-05-11</td>\n",
       "      <td>138965</td>\n",
       "      <td>173655</td>\n",
       "      <td>2012-05-11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168935</th>\n",
       "      <td>13995168</td>\n",
       "      <td>B00NEQKOVM</td>\n",
       "      <td>Torchlighters: Jim Elliot</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-07-16</td>\n",
       "      <td>139658</td>\n",
       "      <td>177066</td>\n",
       "      <td>2015-07-16</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168936</th>\n",
       "      <td>13995168</td>\n",
       "      <td>B002JDUB7S</td>\n",
       "      <td>Torchlighters: The Gladys Aylward Story</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-16</td>\n",
       "      <td>139658</td>\n",
       "      <td>172994</td>\n",
       "      <td>2015-07-16</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168937</th>\n",
       "      <td>51607757</td>\n",
       "      <td>B0061MOXBS</td>\n",
       "      <td>Beavis and Butt-Head: The Mike Judge Collectio...</td>\n",
       "      <td>5</td>\n",
       "      <td>2011-11-05</td>\n",
       "      <td>139926</td>\n",
       "      <td>175449</td>\n",
       "      <td>2011-11-05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168938</th>\n",
       "      <td>32808340</td>\n",
       "      <td>B003NQO2X4</td>\n",
       "      <td>Love &amp; Monsters</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-06-21</td>\n",
       "      <td>140277</td>\n",
       "      <td>174479</td>\n",
       "      <td>2012-06-21</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168939</th>\n",
       "      <td>44874893</td>\n",
       "      <td>B008COO0AM</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-03-19</td>\n",
       "      <td>140213</td>\n",
       "      <td>176942</td>\n",
       "      <td>2013-03-19</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168940</th>\n",
       "      <td>5827063</td>\n",
       "      <td>B003WJBA82</td>\n",
       "      <td>Vampire Knight Season 1</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-03-04</td>\n",
       "      <td>139899</td>\n",
       "      <td>165527</td>\n",
       "      <td>2015-03-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168941</th>\n",
       "      <td>50249413</td>\n",
       "      <td>B003RS1SCG</td>\n",
       "      <td>Shipwrecked</td>\n",
       "      <td>5</td>\n",
       "      <td>2011-02-05</td>\n",
       "      <td>139995</td>\n",
       "      <td>172983</td>\n",
       "      <td>2011-02-05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168942</th>\n",
       "      <td>52159156</td>\n",
       "      <td>B00951A78U</td>\n",
       "      <td>Asylum of the Daleks Prequel</td>\n",
       "      <td>5</td>\n",
       "      <td>2012-09-18</td>\n",
       "      <td>140056</td>\n",
       "      <td>177387</td>\n",
       "      <td>2012-09-18</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168943</th>\n",
       "      <td>52943192</td>\n",
       "      <td>B000V5SIFS</td>\n",
       "      <td>Duet</td>\n",
       "      <td>5</td>\n",
       "      <td>2011-12-03</td>\n",
       "      <td>138854</td>\n",
       "      <td>176901</td>\n",
       "      <td>2011-12-03</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168944</th>\n",
       "      <td>52943192</td>\n",
       "      <td>B005HEVE0E</td>\n",
       "      <td>Threshold</td>\n",
       "      <td>4</td>\n",
       "      <td>2012-03-17</td>\n",
       "      <td>138854</td>\n",
       "      <td>172248</td>\n",
       "      <td>2011-12-03</td>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168945</th>\n",
       "      <td>12196343</td>\n",
       "      <td>B005HEVE0E</td>\n",
       "      <td>Threshold</td>\n",
       "      <td>3</td>\n",
       "      <td>2012-10-17</td>\n",
       "      <td>139869</td>\n",
       "      <td>172248</td>\n",
       "      <td>2012-10-17</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168946</th>\n",
       "      <td>26116624</td>\n",
       "      <td>B00ERXWVBW</td>\n",
       "      <td>Rainbow Horse</td>\n",
       "      <td>5</td>\n",
       "      <td>2013-10-23</td>\n",
       "      <td>140052</td>\n",
       "      <td>175559</td>\n",
       "      <td>2013-10-23</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168947</th>\n",
       "      <td>20571207</td>\n",
       "      <td>B00IPBQ6JK</td>\n",
       "      <td>Being A Teen Mom</td>\n",
       "      <td>4</td>\n",
       "      <td>2014-06-26</td>\n",
       "      <td>140024</td>\n",
       "      <td>173341</td>\n",
       "      <td>2014-06-26</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168948</th>\n",
       "      <td>10626971</td>\n",
       "      <td>B005HEUF0Y</td>\n",
       "      <td>The Inner Light</td>\n",
       "      <td>5</td>\n",
       "      <td>2012-04-08</td>\n",
       "      <td>139888</td>\n",
       "      <td>172479</td>\n",
       "      <td>2012-04-08</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168949</th>\n",
       "      <td>33055072</td>\n",
       "      <td>B00KWO3R2W</td>\n",
       "      <td>Lucky Duck</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-12</td>\n",
       "      <td>140053</td>\n",
       "      <td>178448</td>\n",
       "      <td>2015-07-12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168950</th>\n",
       "      <td>46242405</td>\n",
       "      <td>B00UETZT5S</td>\n",
       "      <td>Aashiqui 2</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-10</td>\n",
       "      <td>139894</td>\n",
       "      <td>178481</td>\n",
       "      <td>2015-07-10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168951</th>\n",
       "      <td>48734167</td>\n",
       "      <td>B00ODGVQP0</td>\n",
       "      <td>Moveable Feast with Fine Cooking Season 1</td>\n",
       "      <td>3</td>\n",
       "      <td>2014-12-09</td>\n",
       "      <td>140096</td>\n",
       "      <td>178602</td>\n",
       "      <td>2014-12-09</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168952</th>\n",
       "      <td>25281381</td>\n",
       "      <td>B00O5L538Y</td>\n",
       "      <td>Tenore, Track By Track</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-11-15</td>\n",
       "      <td>139931</td>\n",
       "      <td>177301</td>\n",
       "      <td>2014-11-15</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168953</th>\n",
       "      <td>31144547</td>\n",
       "      <td>B00O5L538Y</td>\n",
       "      <td>Tenore, Track By Track</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-11-14</td>\n",
       "      <td>140207</td>\n",
       "      <td>177301</td>\n",
       "      <td>2014-11-14</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168954</th>\n",
       "      <td>39477371</td>\n",
       "      <td>B00ST1DOYG</td>\n",
       "      <td>Minecraft: Into The Nether</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-05-30</td>\n",
       "      <td>140293</td>\n",
       "      <td>178153</td>\n",
       "      <td>2015-05-30</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1168955 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         customer_id  product_id  \\\n",
       "0           27072354  B008LOVIIK   \n",
       "1           27072354  B003V8GGA6   \n",
       "2           27072354  B00SY9HO8U   \n",
       "3           27072354  B008Y7EYSK   \n",
       "4           27072354  B0079W7X98   \n",
       "5           27072354  B004AVPV7C   \n",
       "6           27072354  B00NL5R7NO   \n",
       "7           27072354  B001BWQ0XM   \n",
       "8           27072354  B005LAN0AG   \n",
       "9           27072354  B008VVET4S   \n",
       "10          27072354  B006QEZK0O   \n",
       "11          27072354  B00L2GQLL8   \n",
       "12          27072354  B00LCHENE4   \n",
       "13          27072354  B00DUUKV4M   \n",
       "14          27072354  B00MGFX0O0   \n",
       "15          27072354  B00JB3MVCW   \n",
       "16          16030865  B008LOVIIK   \n",
       "17          16030865  B0079W7X98   \n",
       "18          16030865  B00X8UKOUK   \n",
       "19          16030865  B00I3MQNWG   \n",
       "20          16030865  B00CDZFRAI   \n",
       "21          16030865  B00CW8DRU8   \n",
       "22          16030865  B00O5ATHEQ   \n",
       "23          16030865  B000JQYE34   \n",
       "24          16030865  B00OOKUC46   \n",
       "25          16030865  B00D1NFIVO   \n",
       "26          16030865  B0052NAB3Q   \n",
       "27          16030865  B006VRDZ44   \n",
       "28          16030865  B000K78Q3Q   \n",
       "29          16030865  B00JM034NO   \n",
       "...              ...         ...   \n",
       "1168925     13378474  B00DU73X7C   \n",
       "1168926     16029276  B00G9GYZGO   \n",
       "1168927     31896430  B0044S3KAQ   \n",
       "1168928     21204701  B00L3L4GJ6   \n",
       "1168929     51029362  B00H8V0ZRW   \n",
       "1168930     25982685  B00O40CSBQ   \n",
       "1168931     21848453  B008JTXWOK   \n",
       "1168932     10157421  B00HYJECB8   \n",
       "1168933     20490375  B005VA7XVS   \n",
       "1168934     20490375  B005XI57GG   \n",
       "1168935     13995168  B00NEQKOVM   \n",
       "1168936     13995168  B002JDUB7S   \n",
       "1168937     51607757  B0061MOXBS   \n",
       "1168938     32808340  B003NQO2X4   \n",
       "1168939     44874893  B008COO0AM   \n",
       "1168940      5827063  B003WJBA82   \n",
       "1168941     50249413  B003RS1SCG   \n",
       "1168942     52159156  B00951A78U   \n",
       "1168943     52943192  B000V5SIFS   \n",
       "1168944     52943192  B005HEVE0E   \n",
       "1168945     12196343  B005HEVE0E   \n",
       "1168946     26116624  B00ERXWVBW   \n",
       "1168947     20571207  B00IPBQ6JK   \n",
       "1168948     10626971  B005HEUF0Y   \n",
       "1168949     33055072  B00KWO3R2W   \n",
       "1168950     46242405  B00UETZT5S   \n",
       "1168951     48734167  B00ODGVQP0   \n",
       "1168952     25281381  B00O5L538Y   \n",
       "1168953     31144547  B00O5L538Y   \n",
       "1168954     39477371  B00ST1DOYG   \n",
       "\n",
       "                                             product_title  star_rating  \\\n",
       "0                    Masterpiece: Inspector Lewis Season 5            5   \n",
       "1                 Wallace & Gromit's Cracking Contraptions            5   \n",
       "2                                          Suburban Gothic            1   \n",
       "3                                       The Woman in Black            4   \n",
       "4                    Masterpiece: Inspector Lewis Season 2            5   \n",
       "5                    Masterpiece: Inspector Lewis Season 3            5   \n",
       "6        Ken Burns: The Roosevelts - An Intimate Histor...            4   \n",
       "7                                     The X-Files Season 1            5   \n",
       "8                   Masterpiece: Inspector Lewis, Season 4            4   \n",
       "9                                              Black Death            3   \n",
       "10                                    Decoding Alan Turing            3   \n",
       "11                              The Escape Artist Season 1            3   \n",
       "12                                      Endeavour Season 2            5   \n",
       "13                      Masterpiece: Endeavour, Season One            5   \n",
       "14                                        The Zero Theorem            2   \n",
       "15                                                    Noah            1   \n",
       "16                   Masterpiece: Inspector Lewis Season 5            5   \n",
       "17                   Masterpiece: Inspector Lewis Season 2            5   \n",
       "18                                  Catastrophe - Season 1            4   \n",
       "19                                          Bosch Season 1            5   \n",
       "20                                    Alpha House Season 1            5   \n",
       "21                                  The Newsroom: Season 1            5   \n",
       "22                                    Alpha House Season 2            4   \n",
       "23                                      NYPD Blue Season 1            5   \n",
       "24                                   The Newsroom Season 2            5   \n",
       "25                                     The Glades Season 4            5   \n",
       "26                                     The Glades Season 2            4   \n",
       "27                              Boardwalk Empire: Season 1            3   \n",
       "28                                      NYPD Blue Season 2            5   \n",
       "29                                           Gimme Shelter            5   \n",
       "...                                                    ...          ...   \n",
       "1168925                            Hollywood Exes Season 2            4   \n",
       "1168926                                VICTORiOUS Volume 3            4   \n",
       "1168927                                Family Guy Season 9            5   \n",
       "1168928                       Kurt Metzger: White Precious            5   \n",
       "1168929                             Sea of Creepy Monsters            5   \n",
       "1168930                                P-51 Dragon Fighter            2   \n",
       "1168931                                             Wanted            5   \n",
       "1168932                          Are You The One? Season 1            5   \n",
       "1168933                          Shawn Rescues Darth Vader            4   \n",
       "1168934                                     Last Night Gus            5   \n",
       "1168935                          Torchlighters: Jim Elliot            4   \n",
       "1168936            Torchlighters: The Gladys Aylward Story            5   \n",
       "1168937  Beavis and Butt-Head: The Mike Judge Collectio...            5   \n",
       "1168938                                    Love & Monsters            1   \n",
       "1168939                                               Fall            2   \n",
       "1168940                            Vampire Knight Season 1            5   \n",
       "1168941                                        Shipwrecked            5   \n",
       "1168942                       Asylum of the Daleks Prequel            5   \n",
       "1168943                                               Duet            5   \n",
       "1168944                                          Threshold            4   \n",
       "1168945                                          Threshold            3   \n",
       "1168946                                      Rainbow Horse            5   \n",
       "1168947                                   Being A Teen Mom            4   \n",
       "1168948                                    The Inner Light            5   \n",
       "1168949                                         Lucky Duck            5   \n",
       "1168950                                         Aashiqui 2            5   \n",
       "1168951          Moveable Feast with Fine Cooking Season 1            3   \n",
       "1168952                             Tenore, Track By Track            5   \n",
       "1168953                             Tenore, Track By Track            5   \n",
       "1168954                         Minecraft: Into The Nether            1   \n",
       "\n",
       "        review_date    user    item first_review_date  days_since_first  \n",
       "0        2015-08-31   10463  140450        2015-04-21             132.0  \n",
       "1        2015-08-31   10463  144933        2015-04-21             132.0  \n",
       "2        2015-07-22   10463  142602        2015-04-21              92.0  \n",
       "3        2015-07-22   10463  140712        2015-04-21              92.0  \n",
       "4        2015-06-16   10463  140411        2015-04-21              56.0  \n",
       "5        2015-06-16   10463  140564        2015-04-21              56.0  \n",
       "6        2015-05-18   10463  140475        2015-04-21              27.0  \n",
       "7        2015-05-18   10463  140813        2015-04-21              27.0  \n",
       "8        2015-05-18   10463  140557        2015-04-21              27.0  \n",
       "9        2015-05-18   10463  145847        2015-04-21              27.0  \n",
       "10       2015-05-18   10463  152158        2015-04-21              27.0  \n",
       "11       2015-05-18   10463  140595        2015-04-21              27.0  \n",
       "12       2015-04-30   10463  140537        2015-04-21               9.0  \n",
       "13       2015-04-21   10463  140431        2015-04-21               0.0  \n",
       "14       2015-04-21   10463  140954        2015-04-21               0.0  \n",
       "15       2015-04-21   10463  140379        2015-04-21               0.0  \n",
       "16       2014-06-20     489  140450        2013-02-10             495.0  \n",
       "17       2013-08-15     489  140411        2013-02-10             186.0  \n",
       "18       2015-07-05     489  140353        2013-02-10             875.0  \n",
       "19       2014-03-04     489  140344        2013-02-10             387.0  \n",
       "20       2013-12-14     489  140362        2013-02-10             307.0  \n",
       "21       2015-07-30     489  140404        2013-02-10             900.0  \n",
       "22       2014-11-07     489  140367        2013-02-10             635.0  \n",
       "23       2014-03-12     489  140540        2013-02-10             395.0  \n",
       "24       2015-08-08     489  140695        2013-02-10             909.0  \n",
       "25       2014-06-27     489  140727        2013-02-10             502.0  \n",
       "26       2014-04-22     489  140816        2013-02-10             436.0  \n",
       "27       2015-06-16     489  140383        2013-02-10             856.0  \n",
       "28       2014-03-28     489  140977        2013-02-10             411.0  \n",
       "29       2015-08-31     489  140873        2013-02-10             932.0  \n",
       "...             ...     ...     ...               ...               ...  \n",
       "1168925  2013-08-22  139882  168140        2013-08-22               0.0  \n",
       "1168926  2014-05-06  140181  173842        2014-05-06               0.0  \n",
       "1168927  2014-02-16  140018  165684        2014-02-16               0.0  \n",
       "1168928  2015-04-16  140210  177146        2015-04-16               0.0  \n",
       "1168929  2014-08-19  140173  168392        2014-08-19               0.0  \n",
       "1168930  2015-08-10  139865  175028        2015-08-10               0.0  \n",
       "1168931  2012-09-06  139991  172195        2012-09-06               0.0  \n",
       "1168932  2014-03-17  139945  175524        2014-03-17               0.0  \n",
       "1168933  2012-05-11  138965  174140        2012-05-11               0.0  \n",
       "1168934  2012-05-11  138965  173655        2012-05-11               0.0  \n",
       "1168935  2015-07-16  139658  177066        2015-07-16               0.0  \n",
       "1168936  2015-07-16  139658  172994        2015-07-16               0.0  \n",
       "1168937  2011-11-05  139926  175449        2011-11-05               0.0  \n",
       "1168938  2012-06-21  140277  174479        2012-06-21               0.0  \n",
       "1168939  2013-03-19  140213  176942        2013-03-19               0.0  \n",
       "1168940  2015-03-04  139899  165527        2015-03-04               0.0  \n",
       "1168941  2011-02-05  139995  172983        2011-02-05               0.0  \n",
       "1168942  2012-09-18  140056  177387        2012-09-18               0.0  \n",
       "1168943  2011-12-03  138854  176901        2011-12-03               0.0  \n",
       "1168944  2012-03-17  138854  172248        2011-12-03             105.0  \n",
       "1168945  2012-10-17  139869  172248        2012-10-17               0.0  \n",
       "1168946  2013-10-23  140052  175559        2013-10-23               0.0  \n",
       "1168947  2014-06-26  140024  173341        2014-06-26               0.0  \n",
       "1168948  2012-04-08  139888  172479        2012-04-08               0.0  \n",
       "1168949  2015-07-12  140053  178448        2015-07-12               0.0  \n",
       "1168950  2015-07-10  139894  178481        2015-07-10               0.0  \n",
       "1168951  2014-12-09  140096  178602        2014-12-09               0.0  \n",
       "1168952  2014-11-15  139931  177301        2014-11-15               0.0  \n",
       "1168953  2014-11-14  140207  177301        2014-11-14               0.0  \n",
       "1168954  2015-05-30  140293  178153        2015-05-30               0.0  \n",
       "\n",
       "[1168955 rows x 9 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = reduced_df.groupby('customer_id').last().reset_index()\n",
    "\n",
    "train_df = reduced_df.merge(test_df[['customer_id', 'product_id']], \n",
    "                            on=['customer_id', 'product_id'], \n",
    "                            how='outer', \n",
    "                            indicator=True)\n",
    "train_df = train_df[(train_df['_merge'] == 'left_only')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Factorization machines expects data to look something like:\n",
    "  - Sparse matrix\n",
    "  - Target variable is that user's rating for a movie\n",
    "  - One-hot encoding for users ($N$ features)\n",
    "  - One-hot encoding for movies ($M$ features)\n",
    "\n",
    "|Rating|User1|User2|...|UserN|Movie1|Movie2|Movie3|...|MovieM|Feature1|Feature2|...|\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "|4|1|0|...|0|1|0|0|...|0|20|2.2|...|\n",
    "|5|1|0|...|0|0|1|0|...|0|17|9.1|...|\n",
    "|3|0|1|...|0|1|0|0|...|0|3|11.0|...|\n",
    "|4|0|1|...|0|0|0|1|...|0|15|6.4|...|\n",
    "\n",
    "\n",
    "- Wouldn't want to hold this full matrix in memory\n",
    "  - Create a sparse matrix\n",
    "  - Designed to work efficiently with CPUs. Some parts of training for more dense matrices can be parallelized with GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csr_matrix(df, num_users, num_items):\n",
    "    feature_dim = num_users + num_items + 1\n",
    "    data = np.concatenate([np.array([1] * df.shape[0]),\n",
    "                           np.array([1] * df.shape[0]),\n",
    "                           df['days_since_first'].values])\n",
    "    row = np.concatenate([np.arange(df.shape[0])] * 3)\n",
    "    col = np.concatenate([df['user'].values,\n",
    "                          df['item'].values,\n",
    "                          np.array([feature_dim - 1] * df.shape[0])])\n",
    "    return csr_matrix((data, (row, col)), \n",
    "                      shape=(df.shape[0], feature_dim), \n",
    "                      dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csr = to_csr_matrix(train_df, customer_index.shape[0], product_index.shape[0])\n",
    "test_csr = to_csr_matrix(test_df, customer_index.shape[0], product_index.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1028606, 178730)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to sparse recordIO-wrapped protobuf that SageMaker factorization machines expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_s3_protobuf(csr, label, bucket, prefix, channel='train', splits=10):\n",
    "    indices = np.array_split(np.arange(csr.shape[0]), splits)\n",
    "    for i in range(len(indices)):\n",
    "        index = indices[i]\n",
    "        buf = io.BytesIO()\n",
    "        smac.write_spmatrix_to_sparse_tensor(buf, csr[index, ], label[index])\n",
    "        buf.seek(0)\n",
    "        boto3.client('s3').upload_fileobj(buf, bucket, '{}/{}/data-{}'.format(prefix, channel, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_s3_protobuf(train_csr, train_df['star_rating'].values.astype(np.float32), bucket, prefix)\n",
    "to_s3_protobuf(test_csr, test_df['star_rating'].values.astype(np.float32), bucket, prefix, channel='test', splits=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train\n",
    "\n",
    "- Create a [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk) estimator to run a training jobs and specify:\n",
    "  - Algorithm container image\n",
    "  - IAM role\n",
    "  - Hardware setup\n",
    "  - S3 output location\n",
    "  - Algorithm hyperparameters\n",
    "    - `feature_dim`: $N + M + 1$ (additional feature is `days_since_first` to capture trend). 𝑁=140344 (reduced number of customers). M=38385 (reduced number of movies).\n",
    "    - `num_factors`: reduced dimension of factorized interactions\n",
    "    - `epochs`: number of full passes through the dataset\n",
    "- `.fit()` points to training and test data in S3 and begins the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-31 20:31:08 Starting - Starting the training job...\n",
      "2019-10-31 20:31:13 Starting - Launching requested ML instances......\n",
      "2019-10-31 20:32:12 Starting - Preparing the instances for training......\n",
      "2019-10-31 20:33:29 Downloading - Downloading input data\n",
      "2019-10-31 20:33:29 Training - Downloading the training image.\u001b[32mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[31mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[32m/opt/amazon/lib/python2.7/site-packages/pandas/util/nosetester.py:13: DeprecationWarning: Importing from numpy.testing.nosetester is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing import nosetester\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:33:43 INFO 140279785944896] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-conf.json: {u'factors_lr': u'0.0001', u'linear_init_sigma': u'0.01', u'epochs': 1, u'_wd': u'1.0', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'factors_init_sigma': u'0.001', u'_log_level': u'info', u'bias_init_method': u'normal', u'linear_init_method': u'normal', u'linear_lr': u'0.001', u'factors_init_method': u'normal', u'_tuning_objective_metric': u'', u'bias_wd': u'0.01', u'use_linear': u'true', u'bias_lr': u'0.1', u'mini_batch_size': u'1000', u'_use_full_symbolic': u'true', u'batch_metrics_publish_interval': u'500', u'bias_init_sigma': u'0.01', u'_num_gpus': u'auto', u'_data_format': u'record', u'factors_wd': u'0.00001', u'linear_wd': u'0.001', u'_kvstore': u'auto', u'_learning_rate': u'1.0', u'_optimizer': u'adam'}\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:33:43 INFO 140279785944896] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'epochs': u'20', u'feature_dim': u'178730', u'mini_batch_size': u'1000', u'predictor_type': u'regressor', u'num_factors': u'256'}\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:33:43 INFO 140279785944896] Final configuration: {u'factors_lr': u'0.0001', u'linear_init_sigma': u'0.01', u'epochs': u'20', u'feature_dim': u'178730', u'num_factors': u'256', u'_wd': u'1.0', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'factors_init_sigma': u'0.001', u'_log_level': u'info', u'bias_init_method': u'normal', u'linear_init_method': u'normal', u'linear_lr': u'0.001', u'factors_init_method': u'normal', u'_tuning_objective_metric': u'', u'bias_wd': u'0.01', u'use_linear': u'true', u'bias_lr': u'0.1', u'mini_batch_size': u'1000', u'_use_full_symbolic': u'true', u'batch_metrics_publish_interval': u'500', u'predictor_type': u'regressor', u'bias_init_sigma': u'0.01', u'_num_gpus': u'auto', u'_data_format': u'record', u'factors_wd': u'0.00001', u'linear_wd': u'0.001', u'_kvstore': u'auto', u'_learning_rate': u'1.0', u'_optimizer': u'adam'}\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:33:43 WARNING 140279785944896] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31m/opt/amazon/lib/python2.7/site-packages/pandas/util/nosetester.py:13: DeprecationWarning: Importing from numpy.testing.nosetester is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing import nosetester\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:44 INFO 140034480109376] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-conf.json: {u'factors_lr': u'0.0001', u'linear_init_sigma': u'0.01', u'epochs': 1, u'_wd': u'1.0', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'factors_init_sigma': u'0.001', u'_log_level': u'info', u'bias_init_method': u'normal', u'linear_init_method': u'normal', u'linear_lr': u'0.001', u'factors_init_method': u'normal', u'_tuning_objective_metric': u'', u'bias_wd': u'0.01', u'use_linear': u'true', u'bias_lr': u'0.1', u'mini_batch_size': u'1000', u'_use_full_symbolic': u'true', u'batch_metrics_publish_interval': u'500', u'bias_init_sigma': u'0.01', u'_num_gpus': u'auto', u'_data_format': u'record', u'factors_wd': u'0.00001', u'linear_wd': u'0.001', u'_kvstore': u'auto', u'_learning_rate': u'1.0', u'_optimizer': u'adam'}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:44 INFO 140034480109376] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'epochs': u'20', u'feature_dim': u'178730', u'mini_batch_size': u'1000', u'predictor_type': u'regressor', u'num_factors': u'256'}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:44 INFO 140034480109376] Final configuration: {u'factors_lr': u'0.0001', u'linear_init_sigma': u'0.01', u'epochs': u'20', u'feature_dim': u'178730', u'num_factors': u'256', u'_wd': u'1.0', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'factors_init_sigma': u'0.001', u'_log_level': u'info', u'bias_init_method': u'normal', u'linear_init_method': u'normal', u'linear_lr': u'0.001', u'factors_init_method': u'normal', u'_tuning_objective_metric': u'', u'bias_wd': u'0.01', u'use_linear': u'true', u'bias_lr': u'0.1', u'mini_batch_size': u'1000', u'_use_full_symbolic': u'true', u'batch_metrics_publish_interval': u'500', u'predictor_type': u'regressor', u'bias_init_sigma': u'0.01', u'_num_gpus': u'auto', u'_data_format': u'record', u'factors_wd': u'0.00001', u'linear_wd': u'0.001', u'_kvstore': u'auto', u'_learning_rate': u'1.0', u'_optimizer': u'adam'}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:44 WARNING 140034480109376] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:46 INFO 140034480109376] Launching parameter server for role server\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:46 INFO 140034480109376] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/df455f14-626b-4624-a1c1-384c6b4accb8', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'AWS_REGION': 'us-west-2', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'DEMO-loft-recommender-2019-10-31-20-31-08-794', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '4', 'HOSTNAME': 'ip-10-0-236-141.us-west-2.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/84555f63-0d4e-424d-ab3a-73c67785eb01', 'PWD': '/', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:601091450883:training-job/demo-loft-recommender-2019-10-31-20-31-08-794', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:46 INFO 140034480109376] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/df455f14-626b-4624-a1c1-384c6b4accb8', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '4', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.235.151', 'AWS_REGION': 'us-west-2', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'DEMO-loft-recommender-2019-10-31-20-31-08-794', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '4', 'HOSTNAME': 'ip-10-0-236-141.us-west-2.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/84555f63-0d4e-424d-ab3a-73c67785eb01', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '4', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:601091450883:training-job/demo-loft-recommender-2019-10-31-20-31-08-794', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:46 INFO 140034480109376] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/df455f14-626b-4624-a1c1-384c6b4accb8', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '4', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.235.151', 'AWS_REGION': 'us-west-2', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'DEMO-loft-recommender-2019-10-31-20-31-08-794', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '4', 'HOSTNAME': 'ip-10-0-236-141.us-west-2.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/84555f63-0d4e-424d-ab3a-73c67785eb01', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '4', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:601091450883:training-job/demo-loft-recommender-2019-10-31-20-31-08-794', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31mProcess 39 is a shell:server.\u001b[0m\n",
      "\u001b[31mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:46 INFO 140034480109376] Using default worker.\u001b[0m\n",
      "\u001b[31m[2019-10-31 20:33:46.166] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[31m[2019-10-31 20:33:46.172] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 10, \"num_examples\": 1, \"num_bytes\": 71356}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:46 INFO 140034480109376] nvidia-smi took: 0.0251610279083 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:46 INFO 140034480109376] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:46 INFO 140034480109376] [Sparse network] Building a sparse network.\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:46 INFO 140034480109376] Create Store: dist_async\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 499.33409690856934, \"sum\": 499.33409690856934, \"min\": 499.33409690856934}}, \"EndTime\": 1572554026.669033, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554026.161139}\n",
      "\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Total Records Seen\": {\"count\": 1, \"max\": 1000, \"sum\": 1000.0, \"min\": 1000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1000, \"sum\": 1000.0, \"min\": 1000}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1572554026.669218, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554026.669187}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python2.7/site-packages/pandas/util/nosetester.py:13: DeprecationWarning: Importing from numpy.testing.nosetester is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing import nosetester\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:45 INFO 139711786800960] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-conf.json: {u'factors_lr': u'0.0001', u'linear_init_sigma': u'0.01', u'epochs': 1, u'_wd': u'1.0', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'factors_init_sigma': u'0.001', u'_log_level': u'info', u'bias_init_method': u'normal', u'linear_init_method': u'normal', u'linear_lr': u'0.001', u'factors_init_method': u'normal', u'_tuning_objective_metric': u'', u'bias_wd': u'0.01', u'use_linear': u'true', u'bias_lr': u'0.1', u'mini_batch_size': u'1000', u'_use_full_symbolic': u'true', u'batch_metrics_publish_interval': u'500', u'bias_init_sigma': u'0.01', u'_num_gpus': u'auto', u'_data_format': u'record', u'factors_wd': u'0.00001', u'linear_wd': u'0.001', u'_kvstore': u'auto', u'_learning_rate': u'1.0', u'_optimizer': u'adam'}\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:45 INFO 139711786800960] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'epochs': u'20', u'feature_dim': u'178730', u'mini_batch_size': u'1000', u'predictor_type': u'regressor', u'num_factors': u'256'}\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:45 INFO 139711786800960] Final configuration: {u'factors_lr': u'0.0001', u'linear_init_sigma': u'0.01', u'epochs': u'20', u'feature_dim': u'178730', u'num_factors': u'256', u'_wd': u'1.0', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'factors_init_sigma': u'0.001', u'_log_level': u'info', u'bias_init_method': u'normal', u'linear_init_method': u'normal', u'linear_lr': u'0.001', u'factors_init_method': u'normal', u'_tuning_objective_metric': u'', u'bias_wd': u'0.01', u'use_linear': u'true', u'bias_lr': u'0.1', u'mini_batch_size': u'1000', u'_use_full_symbolic': u'true', u'batch_metrics_publish_interval': u'500', u'predictor_type': u'regressor', u'bias_init_sigma': u'0.01', u'_num_gpus': u'auto', u'_data_format': u'record', u'factors_wd': u'0.00001', u'linear_wd': u'0.001', u'_kvstore': u'auto', u'_learning_rate': u'1.0', u'_optimizer': u'adam'}\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:45 WARNING 139711786800960] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:45 INFO 139711786800960] Launching parameter server for role server\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:45 INFO 139711786800960] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/8c7c1d50-1cca-4fc7-ae46-d96c77616625', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'AWS_REGION': 'us-west-2', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'DEMO-loft-recommender-2019-10-31-20-31-08-794', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '4', 'HOSTNAME': 'ip-10-0-241-59.us-west-2.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/77b2273e-fb1c-4f1c-bf8b-b1266a2a02ea', 'PWD': '/', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:601091450883:training-job/demo-loft-recommender-2019-10-31-20-31-08-794', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:45 INFO 139711786800960] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/8c7c1d50-1cca-4fc7-ae46-d96c77616625', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '4', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.235.151', 'AWS_REGION': 'us-west-2', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'DEMO-loft-recommender-2019-10-31-20-31-08-794', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '4', 'HOSTNAME': 'ip-10-0-241-59.us-west-2.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/77b2273e-fb1c-4f1c-bf8b-b1266a2a02ea', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '4', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:601091450883:training-job/demo-loft-recommender-2019-10-31-20-31-08-794', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:45 INFO 139711786800960] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/8c7c1d50-1cca-4fc7-ae46-d96c77616625', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '4', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.235.151', 'AWS_REGION': 'us-west-2', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'DEMO-loft-recommender-2019-10-31-20-31-08-794', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '4', 'HOSTNAME': 'ip-10-0-241-59.us-west-2.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/77b2273e-fb1c-4f1c-bf8b-b1266a2a02ea', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '4', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:601091450883:training-job/demo-loft-recommender-2019-10-31-20-31-08-794', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34mProcess 39 is a shell:server.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:45 INFO 139711786800960] Using default worker.\u001b[0m\n",
      "\u001b[34m[2019-10-31 20:33:45.934] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[2019-10-31 20:33:45.940] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 11, \"num_examples\": 1, \"num_bytes\": 71496}\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:45 INFO 139711786800960] nvidia-smi took: 0.0251491069794 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:45 INFO 139711786800960] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:45 INFO 139711786800960] [Sparse network] Building a sparse network.\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:45 INFO 139711786800960] Create Store: dist_async\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:46 INFO 140279785944896] Launching parameter server for role server\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:46 INFO 140279785944896] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/32d3fa61-e645-4937-a2c6-864c289dd1cf', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'AWS_REGION': 'us-west-2', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'DEMO-loft-recommender-2019-10-31-20-31-08-794', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '4', 'HOSTNAME': 'ip-10-0-209-219.us-west-2.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/1e388b5d-a18e-4464-b41f-76ad0f0544f2', 'PWD': '/', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:601091450883:training-job/demo-loft-recommender-2019-10-31-20-31-08-794', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:46 INFO 140279785944896] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/32d3fa61-e645-4937-a2c6-864c289dd1cf', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '4', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.235.151', 'AWS_REGION': 'us-west-2', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'DEMO-loft-recommender-2019-10-31-20-31-08-794', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '4', 'HOSTNAME': 'ip-10-0-209-219.us-west-2.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/1e388b5d-a18e-4464-b41f-76ad0f0544f2', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '4', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:601091450883:training-job/demo-loft-recommender-2019-10-31-20-31-08-794', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:46 INFO 140279785944896] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/32d3fa61-e645-4937-a2c6-864c289dd1cf', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '4', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.235.151', 'AWS_REGION': 'us-west-2', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'DEMO-loft-recommender-2019-10-31-20-31-08-794', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '4', 'HOSTNAME': 'ip-10-0-209-219.us-west-2.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/1e388b5d-a18e-4464-b41f-76ad0f0544f2', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '4', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:601091450883:training-job/demo-loft-recommender-2019-10-31-20-31-08-794', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[33mProcess 39 is a shell:server.\u001b[0m\n",
      "\u001b[33mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:46 INFO 140279785944896] Using default worker.\u001b[0m\n",
      "\u001b[33m[2019-10-31 20:33:46.165] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[33m[2019-10-31 20:33:46.172] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 11, \"num_examples\": 1, \"num_bytes\": 71212}\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:46 INFO 140279785944896] nvidia-smi took: 0.025120973587 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:46 INFO 140279785944896] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:46 INFO 140279785944896] [Sparse network] Building a sparse network.\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:46 INFO 140279785944896] Create Store: dist_async\u001b[0m\n",
      "\u001b[31mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[31m/opt/amazon/lib/python2.7/site-packages/pandas/util/nosetester.py:13: DeprecationWarning: Importing from numpy.testing.nosetester is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing import nosetester\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:44 INFO 140221098768192] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-conf.json: {u'factors_lr': u'0.0001', u'linear_init_sigma': u'0.01', u'epochs': 1, u'_wd': u'1.0', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'factors_init_sigma': u'0.001', u'_log_level': u'info', u'bias_init_method': u'normal', u'linear_init_method': u'normal', u'linear_lr': u'0.001', u'factors_init_method': u'normal', u'_tuning_objective_metric': u'', u'bias_wd': u'0.01', u'use_linear': u'true', u'bias_lr': u'0.1', u'mini_batch_size': u'1000', u'_use_full_symbolic': u'true', u'batch_metrics_publish_interval': u'500', u'bias_init_sigma': u'0.01', u'_num_gpus': u'auto', u'_data_format': u'record', u'factors_wd': u'0.00001', u'linear_wd': u'0.001', u'_kvstore': u'auto', u'_learning_rate': u'1.0', u'_optimizer': u'adam'}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:44 INFO 140221098768192] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'epochs': u'20', u'feature_dim': u'178730', u'mini_batch_size': u'1000', u'predictor_type': u'regressor', u'num_factors': u'256'}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:44 INFO 140221098768192] Final configuration: {u'factors_lr': u'0.0001', u'linear_init_sigma': u'0.01', u'epochs': u'20', u'feature_dim': u'178730', u'num_factors': u'256', u'_wd': u'1.0', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'factors_init_sigma': u'0.001', u'_log_level': u'info', u'bias_init_method': u'normal', u'linear_init_method': u'normal', u'linear_lr': u'0.001', u'factors_init_method': u'normal', u'_tuning_objective_metric': u'', u'bias_wd': u'0.01', u'use_linear': u'true', u'bias_lr': u'0.1', u'mini_batch_size': u'1000', u'_use_full_symbolic': u'true', u'batch_metrics_publish_interval': u'500', u'predictor_type': u'regressor', u'bias_init_sigma': u'0.01', u'_num_gpus': u'auto', u'_data_format': u'record', u'factors_wd': u'0.00001', u'linear_wd': u'0.001', u'_kvstore': u'auto', u'_learning_rate': u'1.0', u'_optimizer': u'adam'}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:44 WARNING 140221098768192] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:46 INFO 140221098768192] Launching parameter server for role scheduler\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:46 INFO 140221098768192] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/786bd95c-cbbb-44c7-85b2-73589e5ebab6', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'AWS_REGION': 'us-west-2', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'DEMO-loft-recommender-2019-10-31-20-31-08-794', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '4', 'HOSTNAME': 'ip-10-0-235-151.us-west-2.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/361f00c9-676d-4418-be76-def407ad3320', 'PWD': '/', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:601091450883:training-job/demo-loft-recommender-2019-10-31-20-31-08-794', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:46 INFO 140221098768192] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/786bd95c-cbbb-44c7-85b2-73589e5ebab6', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '4', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.235.151', 'AWS_REGION': 'us-west-2', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'DEMO-loft-recommender-2019-10-31-20-31-08-794', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '4', 'HOSTNAME': 'ip-10-0-235-151.us-west-2.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/361f00c9-676d-4418-be76-def407ad3320', 'DMLC_ROLE': 'scheduler', 'PWD': '/', 'DMLC_NUM_SERVER': '4', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:601091450883:training-job/demo-loft-recommender-2019-10-31-20-31-08-794', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:46 INFO 140221098768192] Launching parameter server for role server\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:46 INFO 140221098768192] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/786bd95c-cbbb-44c7-85b2-73589e5ebab6', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'AWS_REGION': 'us-west-2', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'DEMO-loft-recommender-2019-10-31-20-31-08-794', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '4', 'HOSTNAME': 'ip-10-0-235-151.us-west-2.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/361f00c9-676d-4418-be76-def407ad3320', 'PWD': '/', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:601091450883:training-job/demo-loft-recommender-2019-10-31-20-31-08-794', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:46 INFO 140221098768192] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/786bd95c-cbbb-44c7-85b2-73589e5ebab6', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '4', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.235.151', 'AWS_REGION': 'us-west-2', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'DEMO-loft-recommender-2019-10-31-20-31-08-794', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '4', 'HOSTNAME': 'ip-10-0-235-151.us-west-2.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/361f00c9-676d-4418-be76-def407ad3320', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '4', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:601091450883:training-job/demo-loft-recommender-2019-10-31-20-31-08-794', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:46 INFO 140221098768192] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/786bd95c-cbbb-44c7-85b2-73589e5ebab6', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '4', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.235.151', 'AWS_REGION': 'us-west-2', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'DEMO-loft-recommender-2019-10-31-20-31-08-794', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '4', 'HOSTNAME': 'ip-10-0-235-151.us-west-2.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/361f00c9-676d-4418-be76-def407ad3320', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '4', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:601091450883:training-job/demo-loft-recommender-2019-10-31-20-31-08-794', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31mProcess 39 is a shell:scheduler.\u001b[0m\n",
      "\u001b[31mProcess 40 is a shell:server.\u001b[0m\n",
      "\u001b[31mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:46 INFO 140221098768192] Using default worker.\u001b[0m\n",
      "\u001b[31m[2019-10-31 20:33:46.081] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[31m[2019-10-31 20:33:46.091] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 15, \"num_examples\": 1, \"num_bytes\": 71592}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:46 INFO 140221098768192] nvidia-smi took: 0.0251488685608 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:46 INFO 140221098768192] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:46 INFO 140221098768192] [Sparse network] Building a sparse network.\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:46 INFO 140221098768192] Create Store: dist_async\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 731.701135635376, \"sum\": 731.701135635376, \"min\": 731.701135635376}}, \"EndTime\": 1572554026.669839, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554025.927912}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Total Records Seen\": {\"count\": 1, \"max\": 1000, \"sum\": 1000.0, \"min\": 1000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1000, \"sum\": 1000.0, \"min\": 1000}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1572554026.670011, \"Dimensions\": {\"Host\": \"algo-4\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554026.669981}\n",
      "\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 500.852108001709, \"sum\": 500.852108001709, \"min\": 500.852108001709}}, \"EndTime\": 1572554026.670471, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554026.160781}\n",
      "\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Total Records Seen\": {\"count\": 1, \"max\": 1000, \"sum\": 1000.0, \"min\": 1000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1000, \"sum\": 1000.0, \"min\": 1000}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1572554026.670641, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554026.670611}\n",
      "\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 580.6348323822021, \"sum\": 580.6348323822021, \"min\": 580.6348323822021}}, \"EndTime\": 1572554026.669747, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554026.075455}\n",
      "\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Total Records Seen\": {\"count\": 1, \"max\": 1000, \"sum\": 1000.0, \"min\": 1000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1000, \"sum\": 1000.0, \"min\": 1000}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1572554026.669929, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554026.669896}\n",
      "\u001b[0m\n",
      "\u001b[31m[20:33:47] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.202139.0/AL2012/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[32m[20:33:47] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.202139.0/AL2012/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[32m[20:33:47] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.202139.0/AL2012/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[20:33:47] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.202139.0/AL2012/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[20:33:47] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.202139.0/AL2012/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:47 INFO 139711786800960] #quality_metric: host=algo-4, epoch=0, batch=0 train rmse <loss>=4.15243716087\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:47 INFO 139711786800960] #quality_metric: host=algo-4, epoch=0, batch=0 train mse <loss>=17.242734375\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:47 INFO 139711786800960] #quality_metric: host=algo-4, epoch=0, batch=0 train absolute_loss <loss>=3.97287792969\u001b[0m\n",
      "\u001b[33m[20:33:47] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.202139.0/AL2012/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[33m[20:33:47] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.202139.0/AL2012/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:47 INFO 140279785944896] #quality_metric: host=algo-3, epoch=0, batch=0 train rmse <loss>=4.43319046899\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:47 INFO 140279785944896] #quality_metric: host=algo-3, epoch=0, batch=0 train mse <loss>=19.6531777344\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:47 INFO 140279785944896] #quality_metric: host=algo-3, epoch=0, batch=0 train absolute_loss <loss>=4.30187792969\u001b[0m\n",
      "\u001b[31m[20:33:47] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.202139.0/AL2012/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:47 INFO 140221098768192] #quality_metric: host=algo-1, epoch=0, batch=0 train rmse <loss>=4.48608395137\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:47 INFO 140221098768192] #quality_metric: host=algo-1, epoch=0, batch=0 train mse <loss>=20.1249492188\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:47 INFO 140221098768192] #quality_metric: host=algo-1, epoch=0, batch=0 train absolute_loss <loss>=4.37487792969\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:33:47 INFO 140034480109376] #quality_metric: host=algo-2, epoch=0, batch=0 train rmse <loss>=4.24214531517\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:33:47 INFO 140034480109376] #quality_metric: host=algo-2, epoch=0, batch=0 train mse <loss>=17.995796875\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:33:47 INFO 140034480109376] #quality_metric: host=algo-2, epoch=0, batch=0 train absolute_loss <loss>=4.06187792969\u001b[0m\n",
      "\u001b[34m[2019-10-31 20:33:50.476] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 2789, \"num_examples\": 206, \"num_bytes\": 14622296}\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:50 INFO 139711786800960] #quality_metric: host=algo-4, epoch=0, train rmse <loss>=1.32185611246\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:50 INFO 139711786800960] #quality_metric: host=algo-4, epoch=0, train mse <loss>=1.74730358205\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:50 INFO 139711786800960] #quality_metric: host=algo-4, epoch=0, train absolute_loss <loss>=1.02782607054\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}, \"update.time\": {\"count\": 1, \"max\": 3806.7479133605957, \"sum\": 3806.7479133605957, \"min\": 3806.7479133605957}}, \"EndTime\": 1572554030.476915, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554026.669928}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:50 INFO 139711786800960] #progress_metric: host=algo-4, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 207, \"sum\": 207.0, \"min\": 207}, \"Total Records Seen\": {\"count\": 1, \"max\": 206721, \"sum\": 206721.0, \"min\": 206721}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1572554030.477089, \"Dimensions\": {\"Host\": \"algo-4\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 0}, \"StartTime\": 1572554026.670143}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:50 INFO 139711786800960] #throughput_metric: host=algo-4, train throughput=54037.3062989 records/second\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:50 INFO 139711786800960] #quality_metric: host=algo-4, epoch=1, batch=0 train rmse <loss>=1.21250251691\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:50 INFO 139711786800960] #quality_metric: host=algo-4, epoch=1, batch=0 train mse <loss>=1.47016235352\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:50 INFO 139711786800960] #quality_metric: host=algo-4, epoch=1, batch=0 train absolute_loss <loss>=0.934860107422\u001b[0m\n",
      "\u001b[33m[2019-10-31 20:33:50.489] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 2801, \"num_examples\": 206, \"num_bytes\": 14630020}\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:50 INFO 140279785944896] #quality_metric: host=algo-3, epoch=0, train rmse <loss>=1.35754099946\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:50 INFO 140279785944896] #quality_metric: host=algo-3, epoch=0, train mse <loss>=1.84291756521\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:50 INFO 140279785944896] #quality_metric: host=algo-3, epoch=0, train absolute_loss <loss>=1.04555565991\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}, \"update.time\": {\"count\": 1, \"max\": 3819.525957107544, \"sum\": 3819.525957107544, \"min\": 3819.525957107544}}, \"EndTime\": 1572554030.490313, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554026.670554}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:50 INFO 140279785944896] #progress_metric: host=algo-3, completed 5 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 207, \"sum\": 207.0, \"min\": 207}, \"Total Records Seen\": {\"count\": 1, \"max\": 206721, \"sum\": 206721.0, \"min\": 206721}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1572554030.490486, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 0}, \"StartTime\": 1572554026.670764}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:50 INFO 140279785944896] #throughput_metric: host=algo-3, train throughput=53856.6238651 records/second\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:50 INFO 140279785944896] #quality_metric: host=algo-3, epoch=1, batch=0 train rmse <loss>=1.12530887687\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:50 INFO 140279785944896] #quality_metric: host=algo-3, epoch=1, batch=0 train mse <loss>=1.26632006836\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:50 INFO 140279785944896] #quality_metric: host=algo-3, epoch=1, batch=0 train absolute_loss <loss>=0.922461364746\u001b[0m\n",
      "\u001b[31m[2019-10-31 20:33:51.845] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 4157, \"num_examples\": 309, \"num_bytes\": 21961380}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:51 INFO 140221098768192] #quality_metric: host=algo-1, epoch=0, train rmse <loss>=1.31293558649\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:51 INFO 140221098768192] #quality_metric: host=algo-1, epoch=0, train mse <loss>=1.72379985427\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:51 INFO 140221098768192] #quality_metric: host=algo-1, epoch=0, train absolute_loss <loss>=1.01731454655\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}, \"update.time\": {\"count\": 1, \"max\": 5175.307989120483, \"sum\": 5175.307989120483, \"min\": 5175.307989120483}}, \"EndTime\": 1572554031.845406, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554026.669837}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:51 INFO 140221098768192] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 310, \"sum\": 310.0, \"min\": 310}, \"Total Records Seen\": {\"count\": 1, \"max\": 309582, \"sum\": 309582.0, \"min\": 309582}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1572554031.845563, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 0}, \"StartTime\": 1572554026.670076}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:51 INFO 140221098768192] #throughput_metric: host=algo-1, train throughput=59622.9530192 records/second\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:51 INFO 140221098768192] #quality_metric: host=algo-1, epoch=1, batch=0 train rmse <loss>=1.0943594842\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:51 INFO 140221098768192] #quality_metric: host=algo-1, epoch=1, batch=0 train mse <loss>=1.19762268066\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:51 INFO 140221098768192] #quality_metric: host=algo-1, epoch=1, batch=0 train absolute_loss <loss>=0.933962402344\u001b[0m\n",
      "\u001b[32m[2019-10-31 20:33:52.087] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 4400, \"num_examples\": 309, \"num_bytes\": 21931124}\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:33:52 INFO 140034480109376] #quality_metric: host=algo-2, epoch=0, train rmse <loss>=1.31531423965\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:33:52 INFO 140034480109376] #quality_metric: host=algo-2, epoch=0, train mse <loss>=1.73005154903\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:33:52 INFO 140034480109376] #quality_metric: host=algo-2, epoch=0, train absolute_loss <loss>=1.02238927907\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}, \"update.time\": {\"count\": 1, \"max\": 5418.323993682861, \"sum\": 5418.323993682861, \"min\": 5418.323993682861}}, \"EndTime\": 1572554032.087783, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554026.669121}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:33:52 INFO 140034480109376] #progress_metric: host=algo-2, completed 5 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 310, \"sum\": 310.0, \"min\": 310}, \"Total Records Seen\": {\"count\": 1, \"max\": 309582, \"sum\": 309582.0, \"min\": 309582}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1572554032.088, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 0}, \"StartTime\": 1572554026.669388}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:33:52 INFO 140034480109376] #throughput_metric: host=algo-2, train throughput=56947.6802361 records/second\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:33:52 INFO 140034480109376] #quality_metric: host=algo-2, epoch=1, batch=0 train rmse <loss>=1.24256765016\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:33:52 INFO 140034480109376] #quality_metric: host=algo-2, epoch=1, batch=0 train mse <loss>=1.54397436523\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:33:52 INFO 140034480109376] #quality_metric: host=algo-2, epoch=1, batch=0 train absolute_loss <loss>=0.938372314453\u001b[0m\n",
      "\u001b[34m[2019-10-31 20:33:53.111] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 2632, \"num_examples\": 206, \"num_bytes\": 14622296}\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:53 INFO 139711786800960] #quality_metric: host=algo-4, epoch=1, train rmse <loss>=1.20228487105\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:53 INFO 139711786800960] #quality_metric: host=algo-4, epoch=1, train mse <loss>=1.44548891116\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:53 INFO 139711786800960] #quality_metric: host=algo-4, epoch=1, train absolute_loss <loss>=0.959287067302\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2634.44185256958, \"sum\": 2634.44185256958, \"min\": 2634.44185256958}}, \"EndTime\": 1572554033.111721, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554030.476993}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:53 INFO 139711786800960] #progress_metric: host=algo-4, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 413, \"sum\": 413.0, \"min\": 413}, \"Total Records Seen\": {\"count\": 1, \"max\": 412442, \"sum\": 412442.0, \"min\": 412442}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}}, \"EndTime\": 1572554033.111938, \"Dimensions\": {\"Host\": \"algo-4\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 1}, \"StartTime\": 1572554030.477254}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:53 INFO 139711786800960] #throughput_metric: host=algo-4, train throughput=78079.155141 records/second\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:53 INFO 139711786800960] #quality_metric: host=algo-4, epoch=2, batch=0 train rmse <loss>=1.21823246223\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:53 INFO 139711786800960] #quality_metric: host=algo-4, epoch=2, batch=0 train mse <loss>=1.48409033203\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:53 INFO 139711786800960] #quality_metric: host=algo-4, epoch=2, batch=0 train absolute_loss <loss>=0.938971313477\u001b[0m\n",
      "\u001b[33m[2019-10-31 20:33:53.267] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 2774, \"num_examples\": 206, \"num_bytes\": 14630020}\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:53 INFO 140279785944896] #quality_metric: host=algo-3, epoch=1, train rmse <loss>=1.1921942786\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:53 INFO 140279785944896] #quality_metric: host=algo-3, epoch=1, train mse <loss>=1.42132719792\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:53 INFO 140279785944896] #quality_metric: host=algo-3, epoch=1, train absolute_loss <loss>=0.955896200236\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2776.8571376800537, \"sum\": 2776.8571376800537, \"min\": 2776.8571376800537}}, \"EndTime\": 1572554033.267528, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554030.49038}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:53 INFO 140279785944896] #progress_metric: host=algo-3, completed 10 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 413, \"sum\": 413.0, \"min\": 413}, \"Total Records Seen\": {\"count\": 1, \"max\": 412442, \"sum\": 412442.0, \"min\": 412442}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}}, \"EndTime\": 1572554033.267749, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 1}, \"StartTime\": 1572554030.490644}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:53 INFO 140279785944896] #throughput_metric: host=algo-3, train throughput=74074.6437116 records/second\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:53 INFO 140279785944896] #quality_metric: host=algo-3, epoch=2, batch=0 train rmse <loss>=1.20777838931\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:53 INFO 140279785944896] #quality_metric: host=algo-3, epoch=2, batch=0 train mse <loss>=1.4587286377\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:53 INFO 140279785944896] #quality_metric: host=algo-3, epoch=2, batch=0 train absolute_loss <loss>=1.06639025879\u001b[0m\n",
      "\u001b[31m[2019-10-31 20:33:55.693] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 3846, \"num_examples\": 309, \"num_bytes\": 21961380}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:55 INFO 140221098768192] #quality_metric: host=algo-1, epoch=1, train rmse <loss>=1.2055327493\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:55 INFO 140221098768192] #quality_metric: host=algo-1, epoch=1, train mse <loss>=1.45330920963\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:55 INFO 140221098768192] #quality_metric: host=algo-1, epoch=1, train absolute_loss <loss>=0.966023468906\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3848.189115524292, \"sum\": 3848.189115524292, \"min\": 3848.189115524292}}, \"EndTime\": 1572554035.693936, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554031.845468}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:55 INFO 140221098768192] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 619, \"sum\": 619.0, \"min\": 619}, \"Total Records Seen\": {\"count\": 1, \"max\": 618164, \"sum\": 618164.0, \"min\": 618164}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}}, \"EndTime\": 1572554035.694113, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 1}, \"StartTime\": 1572554031.845722}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:55 INFO 140221098768192] #throughput_metric: host=algo-1, train throughput=80183.0343477 records/second\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:55 INFO 140221098768192] #quality_metric: host=algo-1, epoch=2, batch=0 train rmse <loss>=1.16617745161\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:55 INFO 140221098768192] #quality_metric: host=algo-1, epoch=2, batch=0 train mse <loss>=1.35996984863\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:55 INFO 140221098768192] #quality_metric: host=algo-1, epoch=2, batch=0 train absolute_loss <loss>=1.04081018066\u001b[0m\n",
      "\u001b[32m[2019-10-31 20:33:56.419] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 4329, \"num_examples\": 309, \"num_bytes\": 21931124}\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:33:56 INFO 140034480109376] #quality_metric: host=algo-2, epoch=1, train rmse <loss>=1.22532484915\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:33:56 INFO 140034480109376] #quality_metric: host=algo-2, epoch=1, train mse <loss>=1.50142098594\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:33:56 INFO 140034480109376] #quality_metric: host=algo-2, epoch=1, train absolute_loss <loss>=0.973158543263\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4331.9761753082275, \"sum\": 4331.9761753082275, \"min\": 4331.9761753082275}}, \"EndTime\": 1572554036.4202, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554032.087846}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:33:56 INFO 140034480109376] #progress_metric: host=algo-2, completed 10 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 619, \"sum\": 619.0, \"min\": 619}, \"Total Records Seen\": {\"count\": 1, \"max\": 618164, \"sum\": 618164.0, \"min\": 618164}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}}, \"EndTime\": 1572554036.420369, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 1}, \"StartTime\": 1572554032.088199}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:33:56 INFO 140034480109376] #throughput_metric: host=algo-2, train throughput=71228.9224052 records/second\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:33:56 INFO 140034480109376] #quality_metric: host=algo-2, epoch=2, batch=0 train rmse <loss>=1.22981406194\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:33:56 INFO 140034480109376] #quality_metric: host=algo-2, epoch=2, batch=0 train mse <loss>=1.51244262695\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:33:56 INFO 140034480109376] #quality_metric: host=algo-2, epoch=2, batch=0 train absolute_loss <loss>=0.969785522461\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-10-31 20:33:43 Training - Training image download completed. Training in progress.\u001b[34m[2019-10-31 20:33:55.721] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 2608, \"num_examples\": 206, \"num_bytes\": 14622296}\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:55 INFO 139711786800960] #quality_metric: host=algo-4, epoch=2, train rmse <loss>=1.2040238008\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:55 INFO 139711786800960] #quality_metric: host=algo-4, epoch=2, train mse <loss>=1.44967331288\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:55 INFO 139711786800960] #quality_metric: host=algo-4, epoch=2, train absolute_loss <loss>=0.963230873182\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2610.28790473938, \"sum\": 2610.28790473938, \"min\": 2610.28790473938}}, \"EndTime\": 1572554035.722428, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554033.11179}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:55 INFO 139711786800960] #progress_metric: host=algo-4, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 619, \"sum\": 619.0, \"min\": 619}, \"Total Records Seen\": {\"count\": 1, \"max\": 618163, \"sum\": 618163.0, \"min\": 618163}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1572554035.722596, \"Dimensions\": {\"Host\": \"algo-4\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 2}, \"StartTime\": 1572554033.112115}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:55 INFO 139711786800960] #throughput_metric: host=algo-4, train throughput=78803.2491688 records/second\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:55 INFO 139711786800960] #quality_metric: host=algo-4, epoch=3, batch=0 train rmse <loss>=1.21769034462\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:55 INFO 139711786800960] #quality_metric: host=algo-4, epoch=3, batch=0 train mse <loss>=1.48276977539\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:55 INFO 139711786800960] #quality_metric: host=algo-4, epoch=3, batch=0 train absolute_loss <loss>=1.0005760498\u001b[0m\n",
      "\u001b[33m[2019-10-31 20:33:55.804] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 2535, \"num_examples\": 206, \"num_bytes\": 14630020}\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:55 INFO 140279785944896] #quality_metric: host=algo-3, epoch=2, train rmse <loss>=1.19162620075\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:55 INFO 140279785944896] #quality_metric: host=algo-3, epoch=2, train mse <loss>=1.41997300231\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:55 INFO 140279785944896] #quality_metric: host=algo-3, epoch=2, train absolute_loss <loss>=0.95367531282\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2537.2180938720703, \"sum\": 2537.2180938720703, \"min\": 2537.2180938720703}}, \"EndTime\": 1572554035.805424, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554033.267601}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:55 INFO 140279785944896] #progress_metric: host=algo-3, completed 15 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 619, \"sum\": 619.0, \"min\": 619}, \"Total Records Seen\": {\"count\": 1, \"max\": 618163, \"sum\": 618163.0, \"min\": 618163}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1572554035.805629, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 2}, \"StartTime\": 1572554033.268176}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:55 INFO 140279785944896] #throughput_metric: host=algo-3, train throughput=81070.2007365 records/second\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:55 INFO 140279785944896] #quality_metric: host=algo-3, epoch=3, batch=0 train rmse <loss>=1.09880050456\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:55 INFO 140279785944896] #quality_metric: host=algo-3, epoch=3, batch=0 train mse <loss>=1.20736254883\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:55 INFO 140279785944896] #quality_metric: host=algo-3, epoch=3, batch=0 train absolute_loss <loss>=0.889820251465\u001b[0m\n",
      "\u001b[34m[2019-10-31 20:33:58.232] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 2507, \"num_examples\": 206, \"num_bytes\": 14622296}\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:58 INFO 139711786800960] #quality_metric: host=algo-4, epoch=3, train rmse <loss>=1.22046190651\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:58 INFO 139711786800960] #quality_metric: host=algo-4, epoch=3, train mse <loss>=1.48952726523\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:58 INFO 139711786800960] #quality_metric: host=algo-4, epoch=3, train absolute_loss <loss>=0.974583261138\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2509.90891456604, \"sum\": 2509.90891456604, \"min\": 2509.90891456604}}, \"EndTime\": 1572554038.232695, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554035.722485}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:58 INFO 139711786800960] #progress_metric: host=algo-4, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 825, \"sum\": 825.0, \"min\": 825}, \"Total Records Seen\": {\"count\": 1, \"max\": 823884, \"sum\": 823884.0, \"min\": 823884}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 5, \"sum\": 5.0, \"min\": 5}}, \"EndTime\": 1572554038.232911, \"Dimensions\": {\"Host\": \"algo-4\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 3}, \"StartTime\": 1572554035.722757}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:58 INFO 139711786800960] #throughput_metric: host=algo-4, train throughput=81952.6257018 records/second\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:58 INFO 139711786800960] #quality_metric: host=algo-4, epoch=4, batch=0 train rmse <loss>=1.21234801923\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:58 INFO 139711786800960] #quality_metric: host=algo-4, epoch=4, batch=0 train mse <loss>=1.46978771973\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:33:58 INFO 139711786800960] #quality_metric: host=algo-4, epoch=4, batch=0 train absolute_loss <loss>=0.93472076416\u001b[0m\n",
      "\u001b[33m[2019-10-31 20:33:58.303] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 2496, \"num_examples\": 206, \"num_bytes\": 14630020}\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:58 INFO 140279785944896] #quality_metric: host=algo-3, epoch=3, train rmse <loss>=1.20715323587\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:58 INFO 140279785944896] #quality_metric: host=algo-3, epoch=3, train mse <loss>=1.45721893488\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:58 INFO 140279785944896] #quality_metric: host=algo-3, epoch=3, train absolute_loss <loss>=0.967208143808\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2498.3789920806885, \"sum\": 2498.3789920806885, \"min\": 2498.3789920806885}}, \"EndTime\": 1572554038.304414, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554035.805493}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:58 INFO 140279785944896] #progress_metric: host=algo-3, completed 20 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 825, \"sum\": 825.0, \"min\": 825}, \"Total Records Seen\": {\"count\": 1, \"max\": 823884, \"sum\": 823884.0, \"min\": 823884}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 5, \"sum\": 5.0, \"min\": 5}}, \"EndTime\": 1572554038.304615, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 3}, \"StartTime\": 1572554035.806007}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:58 INFO 140279785944896] #throughput_metric: host=algo-3, train throughput=82330.9875928 records/second\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:58 INFO 140279785944896] #quality_metric: host=algo-3, epoch=4, batch=0 train rmse <loss>=1.28163466025\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:58 INFO 140279785944896] #quality_metric: host=algo-3, epoch=4, batch=0 train mse <loss>=1.64258740234\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:33:58 INFO 140279785944896] #quality_metric: host=algo-3, epoch=4, batch=0 train absolute_loss <loss>=1.16485473633\u001b[0m\n",
      "\u001b[31m[2019-10-31 20:33:59.528] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 3832, \"num_examples\": 309, \"num_bytes\": 21961380}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:59 INFO 140221098768192] #quality_metric: host=algo-1, epoch=2, train rmse <loss>=1.21670378371\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:59 INFO 140221098768192] #quality_metric: host=algo-1, epoch=2, train mse <loss>=1.4803680973\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:59 INFO 140221098768192] #quality_metric: host=algo-1, epoch=2, train absolute_loss <loss>=0.973322312043\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3833.9359760284424, \"sum\": 3833.9359760284424, \"min\": 3833.9359760284424}}, \"EndTime\": 1572554039.528635, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554035.694017}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:59 INFO 140221098768192] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 928, \"sum\": 928.0, \"min\": 928}, \"Total Records Seen\": {\"count\": 1, \"max\": 926746, \"sum\": 926746.0, \"min\": 926746}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1572554039.528823, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 2}, \"StartTime\": 1572554035.69467}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:59 INFO 140221098768192] #throughput_metric: host=algo-1, train throughput=80480.1572018 records/second\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:59 INFO 140221098768192] #quality_metric: host=algo-1, epoch=3, batch=0 train rmse <loss>=1.13132849995\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:59 INFO 140221098768192] #quality_metric: host=algo-1, epoch=3, batch=0 train mse <loss>=1.2799041748\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:33:59 INFO 140221098768192] #quality_metric: host=algo-1, epoch=3, batch=0 train absolute_loss <loss>=0.99185760498\u001b[0m\n",
      "\u001b[32m[2019-10-31 20:34:00.721] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 4299, \"num_examples\": 309, \"num_bytes\": 21931124}\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:00 INFO 140034480109376] #quality_metric: host=algo-2, epoch=2, train rmse <loss>=1.24325539752\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:00 INFO 140034480109376] #quality_metric: host=algo-2, epoch=2, train mse <loss>=1.54568398347\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:00 INFO 140034480109376] #quality_metric: host=algo-2, epoch=2, train absolute_loss <loss>=0.992002648215\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4300.915956497192, \"sum\": 4300.915956497192, \"min\": 4300.915956497192}}, \"EndTime\": 1572554040.721897, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554036.420271}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:00 INFO 140034480109376] #progress_metric: host=algo-2, completed 15 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 928, \"sum\": 928.0, \"min\": 928}, \"Total Records Seen\": {\"count\": 1, \"max\": 926746, \"sum\": 926746.0, \"min\": 926746}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1572554040.722069, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 2}, \"StartTime\": 1572554036.420954}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:00 INFO 140034480109376] #throughput_metric: host=algo-2, train throughput=71743.0012678 records/second\u001b[0m\n",
      "\u001b[34m[2019-10-31 20:34:00.861] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 2626, \"num_examples\": 206, \"num_bytes\": 14622296}\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:00 INFO 139711786800960] #quality_metric: host=algo-4, epoch=4, train rmse <loss>=1.21099195004\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:00 INFO 139711786800960] #quality_metric: host=algo-4, epoch=4, train mse <loss>=1.46650150306\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:00 INFO 139711786800960] #quality_metric: host=algo-4, epoch=4, train absolute_loss <loss>=0.971979762401\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2628.8249492645264, \"sum\": 2628.8249492645264, \"min\": 2628.8249492645264}}, \"EndTime\": 1572554040.862021, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554038.232766}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:00 INFO 139711786800960] #progress_metric: host=algo-4, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1031, \"sum\": 1031.0, \"min\": 1031}, \"Total Records Seen\": {\"count\": 1, \"max\": 1029605, \"sum\": 1029605.0, \"min\": 1029605}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1572554040.862228, \"Dimensions\": {\"Host\": \"algo-4\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 4}, \"StartTime\": 1572554038.233167}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:00 INFO 139711786800960] #throughput_metric: host=algo-4, train throughput=78245.3665398 records/second\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:00 INFO 139711786800960] #quality_metric: host=algo-4, epoch=5, batch=0 train rmse <loss>=1.21009143831\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:00 INFO 139711786800960] #quality_metric: host=algo-4, epoch=5, batch=0 train mse <loss>=1.46432128906\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:00 INFO 139711786800960] #quality_metric: host=algo-4, epoch=5, batch=0 train absolute_loss <loss>=0.963090637207\u001b[0m\n",
      "\u001b[33m[2019-10-31 20:34:00.922] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 2616, \"num_examples\": 206, \"num_bytes\": 14630020}\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:00 INFO 140279785944896] #quality_metric: host=algo-3, epoch=4, train rmse <loss>=1.20683390215\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:00 INFO 140279785944896] #quality_metric: host=algo-3, epoch=4, train mse <loss>=1.45644806738\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:00 INFO 140279785944896] #quality_metric: host=algo-3, epoch=4, train absolute_loss <loss>=0.971634274085\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2618.4141635894775, \"sum\": 2618.4141635894775, \"min\": 2618.4141635894775}}, \"EndTime\": 1572554040.923457, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554038.304485}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:00 INFO 140279785944896] #progress_metric: host=algo-3, completed 25 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1031, \"sum\": 1031.0, \"min\": 1031}, \"Total Records Seen\": {\"count\": 1, \"max\": 1029605, \"sum\": 1029605.0, \"min\": 1029605}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1572554040.923606, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 4}, \"StartTime\": 1572554038.305018}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:00 INFO 140279785944896] #throughput_metric: host=algo-3, train throughput=78559.2309696 records/second\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:00 INFO 140279785944896] #quality_metric: host=algo-3, epoch=5, batch=0 train rmse <loss>=1.09861507277\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:00 INFO 140279785944896] #quality_metric: host=algo-3, epoch=5, batch=0 train mse <loss>=1.20695507813\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:00 INFO 140279785944896] #quality_metric: host=algo-3, epoch=5, batch=0 train absolute_loss <loss>=0.889627502441\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:00 INFO 140034480109376] #quality_metric: host=algo-2, epoch=3, batch=0 train rmse <loss>=1.24658968383\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:00 INFO 140034480109376] #quality_metric: host=algo-2, epoch=3, batch=0 train mse <loss>=1.55398583984\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:00 INFO 140034480109376] #quality_metric: host=algo-2, epoch=3, batch=0 train absolute_loss <loss>=1.03130078125\u001b[0m\n",
      "\u001b[31m[2019-10-31 20:34:03.455] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 3924, \"num_examples\": 309, \"num_bytes\": 21961380}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:03 INFO 140221098768192] #quality_metric: host=algo-1, epoch=3, train rmse <loss>=1.21679493728\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:03 INFO 140221098768192] #quality_metric: host=algo-1, epoch=3, train mse <loss>=1.4805899194\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:03 INFO 140221098768192] #quality_metric: host=algo-1, epoch=3, train absolute_loss <loss>=0.979084251231\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3926.532030105591, \"sum\": 3926.532030105591, \"min\": 3926.532030105591}}, \"EndTime\": 1572554043.455606, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554039.5287}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:03 INFO 140221098768192] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1237, \"sum\": 1237.0, \"min\": 1237}, \"Total Records Seen\": {\"count\": 1, \"max\": 1235328, \"sum\": 1235328.0, \"min\": 1235328}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 5, \"sum\": 5.0, \"min\": 5}}, \"EndTime\": 1572554043.45576, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 3}, \"StartTime\": 1572554039.529046}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:03 INFO 140221098768192] #throughput_metric: host=algo-1, train throughput=78583.5219979 records/second\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:03 INFO 140221098768192] #quality_metric: host=algo-1, epoch=4, batch=0 train rmse <loss>=1.21982734834\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:03 INFO 140221098768192] #quality_metric: host=algo-1, epoch=4, batch=0 train mse <loss>=1.48797875977\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:03 INFO 140221098768192] #quality_metric: host=algo-1, epoch=4, batch=0 train absolute_loss <loss>=1.10922558594\u001b[0m\n",
      "\u001b[34m[2019-10-31 20:34:03.525] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 12, \"duration\": 2660, \"num_examples\": 206, \"num_bytes\": 14622296}\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:03 INFO 139711786800960] #quality_metric: host=algo-4, epoch=5, train rmse <loss>=1.21515714611\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:03 INFO 139711786800960] #quality_metric: host=algo-4, epoch=5, train mse <loss>=1.47660688974\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:03 INFO 139711786800960] #quality_metric: host=algo-4, epoch=5, train absolute_loss <loss>=0.968956829182\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2662.7140045166016, \"sum\": 2662.7140045166016, \"min\": 2662.7140045166016}}, \"EndTime\": 1572554043.525517, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554040.862091}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:03 INFO 139711786800960] #progress_metric: host=algo-4, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1237, \"sum\": 1237.0, \"min\": 1237}, \"Total Records Seen\": {\"count\": 1, \"max\": 1235326, \"sum\": 1235326.0, \"min\": 1235326}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 7, \"sum\": 7.0, \"min\": 7}}, \"EndTime\": 1572554043.525747, \"Dimensions\": {\"Host\": \"algo-4\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 5}, \"StartTime\": 1572554040.862772}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:03 INFO 139711786800960] #throughput_metric: host=algo-4, train throughput=77249.6217392 records/second\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:03 INFO 139711786800960] #quality_metric: host=algo-4, epoch=6, batch=0 train rmse <loss>=1.32087497264\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:03 INFO 139711786800960] #quality_metric: host=algo-4, epoch=6, batch=0 train mse <loss>=1.74471069336\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:03 INFO 139711786800960] #quality_metric: host=algo-4, epoch=6, batch=0 train absolute_loss <loss>=0.968304321289\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2019-10-31 20:34:03.546] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 12, \"duration\": 2620, \"num_examples\": 206, \"num_bytes\": 14630020}\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:03 INFO 140279785944896] #quality_metric: host=algo-3, epoch=5, train rmse <loss>=1.20656613229\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:03 INFO 140279785944896] #quality_metric: host=algo-3, epoch=5, train mse <loss>=1.45580183159\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:03 INFO 140279785944896] #quality_metric: host=algo-3, epoch=5, train absolute_loss <loss>=0.966801044946\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2622.497081756592, \"sum\": 2622.497081756592, \"min\": 2622.497081756592}}, \"EndTime\": 1572554043.546596, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554040.923516}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:03 INFO 140279785944896] #progress_metric: host=algo-3, completed 30 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1237, \"sum\": 1237.0, \"min\": 1237}, \"Total Records Seen\": {\"count\": 1, \"max\": 1235326, \"sum\": 1235326.0, \"min\": 1235326}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 7, \"sum\": 7.0, \"min\": 7}}, \"EndTime\": 1572554043.546749, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 5}, \"StartTime\": 1572554040.924074}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:03 INFO 140279785944896] #throughput_metric: host=algo-3, train throughput=78436.8072911 records/second\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:03 INFO 140279785944896] #quality_metric: host=algo-3, epoch=6, batch=0 train rmse <loss>=1.13071880873\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:03 INFO 140279785944896] #quality_metric: host=algo-3, epoch=6, batch=0 train mse <loss>=1.27852502441\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:03 INFO 140279785944896] #quality_metric: host=algo-3, epoch=6, batch=0 train absolute_loss <loss>=0.934114379883\u001b[0m\n",
      "\u001b[32m[2019-10-31 20:34:05.179] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 4456, \"num_examples\": 309, \"num_bytes\": 21931124}\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:05 INFO 140034480109376] #quality_metric: host=algo-2, epoch=3, train rmse <loss>=1.23460894238\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:05 INFO 140034480109376] #quality_metric: host=algo-2, epoch=3, train mse <loss>=1.5242592406\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:05 INFO 140034480109376] #quality_metric: host=algo-2, epoch=3, train absolute_loss <loss>=0.98098104651\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4457.948923110962, \"sum\": 4457.948923110962, \"min\": 4457.948923110962}}, \"EndTime\": 1572554045.180233, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554040.721954}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:05 INFO 140034480109376] #progress_metric: host=algo-2, completed 20 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1237, \"sum\": 1237.0, \"min\": 1237}, \"Total Records Seen\": {\"count\": 1, \"max\": 1235328, \"sum\": 1235328.0, \"min\": 1235328}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 5, \"sum\": 5.0, \"min\": 5}}, \"EndTime\": 1572554045.180389, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 3}, \"StartTime\": 1572554040.722258}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:05 INFO 140034480109376] #throughput_metric: host=algo-2, train throughput=69216.3840348 records/second\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:05 INFO 140034480109376] #quality_metric: host=algo-2, epoch=4, batch=0 train rmse <loss>=1.22607499269\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:05 INFO 140034480109376] #quality_metric: host=algo-2, epoch=4, batch=0 train mse <loss>=1.5032598877\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:05 INFO 140034480109376] #quality_metric: host=algo-2, epoch=4, batch=0 train absolute_loss <loss>=0.947855102539\u001b[0m\n",
      "\u001b[34m[2019-10-31 20:34:06.195] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 2667, \"num_examples\": 206, \"num_bytes\": 14622296}\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:06 INFO 139711786800960] #quality_metric: host=algo-4, epoch=6, train rmse <loss>=1.20410380566\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:06 INFO 139711786800960] #quality_metric: host=algo-4, epoch=6, train mse <loss>=1.4498659748\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:06 INFO 139711786800960] #quality_metric: host=algo-4, epoch=6, train absolute_loss <loss>=0.961381232993\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2669.451951980591, \"sum\": 2669.451951980591, \"min\": 2669.451951980591}}, \"EndTime\": 1572554046.195707, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554043.525591}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:06 INFO 139711786800960] #progress_metric: host=algo-4, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1443, \"sum\": 1443.0, \"min\": 1443}, \"Total Records Seen\": {\"count\": 1, \"max\": 1441047, \"sum\": 1441047.0, \"min\": 1441047}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1572554046.195858, \"Dimensions\": {\"Host\": \"algo-4\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 6}, \"StartTime\": 1572554043.526228}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:06 INFO 139711786800960] #throughput_metric: host=algo-4, train throughput=77057.2909742 records/second\u001b[0m\n",
      "\u001b[33m[2019-10-31 20:34:06.177] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 2628, \"num_examples\": 206, \"num_bytes\": 14630020}\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:06 INFO 140279785944896] #quality_metric: host=algo-3, epoch=6, train rmse <loss>=1.19666110048\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:06 INFO 140279785944896] #quality_metric: host=algo-3, epoch=6, train mse <loss>=1.4319977894\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:06 INFO 140279785944896] #quality_metric: host=algo-3, epoch=6, train absolute_loss <loss>=0.956309055662\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2630.579948425293, \"sum\": 2630.579948425293, \"min\": 2630.579948425293}}, \"EndTime\": 1572554046.177512, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554043.546659}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:06 INFO 140279785944896] #progress_metric: host=algo-3, completed 35 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1443, \"sum\": 1443.0, \"min\": 1443}, \"Total Records Seen\": {\"count\": 1, \"max\": 1441047, \"sum\": 1441047.0, \"min\": 1441047}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1572554046.177692, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 6}, \"StartTime\": 1572554043.546904}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:06 INFO 140279785944896] #throughput_metric: host=algo-3, train throughput=78193.6183375 records/second\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:06 INFO 140279785944896] #quality_metric: host=algo-3, epoch=7, batch=0 train rmse <loss>=1.18155502014\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:06 INFO 140279785944896] #quality_metric: host=algo-3, epoch=7, batch=0 train mse <loss>=1.39607226562\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:06 INFO 140279785944896] #quality_metric: host=algo-3, epoch=7, batch=0 train absolute_loss <loss>=1.02633203125\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:06 INFO 139711786800960] #quality_metric: host=algo-4, epoch=7, batch=0 train rmse <loss>=1.26299480751\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:06 INFO 139711786800960] #quality_metric: host=algo-4, epoch=7, batch=0 train mse <loss>=1.59515588379\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:06 INFO 139711786800960] #quality_metric: host=algo-4, epoch=7, batch=0 train absolute_loss <loss>=1.09980053711\u001b[0m\n",
      "\u001b[31m[2019-10-31 20:34:07.482] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 4025, \"num_examples\": 309, \"num_bytes\": 21961380}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:07 INFO 140221098768192] #quality_metric: host=algo-1, epoch=4, train rmse <loss>=1.21648817957\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:07 INFO 140221098768192] #quality_metric: host=algo-1, epoch=4, train mse <loss>=1.47984349105\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:07 INFO 140221098768192] #quality_metric: host=algo-1, epoch=4, train absolute_loss <loss>=0.969889364397\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4027.0071029663086, \"sum\": 4027.0071029663086, \"min\": 4027.0071029663086}}, \"EndTime\": 1572554047.482948, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554043.455669}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:07 INFO 140221098768192] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1546, \"sum\": 1546.0, \"min\": 1546}, \"Total Records Seen\": {\"count\": 1, \"max\": 1543910, \"sum\": 1543910.0, \"min\": 1543910}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1572554047.483111, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 4}, \"StartTime\": 1572554043.455916}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:07 INFO 140221098768192] #throughput_metric: host=algo-1, train throughput=76622.9343876 records/second\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:07 INFO 140221098768192] #quality_metric: host=algo-1, epoch=5, batch=0 train rmse <loss>=1.0283979857\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:07 INFO 140221098768192] #quality_metric: host=algo-1, epoch=5, batch=0 train mse <loss>=1.05760241699\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:07 INFO 140221098768192] #quality_metric: host=algo-1, epoch=5, batch=0 train absolute_loss <loss>=0.852312438965\u001b[0m\n",
      "\u001b[34m[2019-10-31 20:34:08.830] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 2632, \"num_examples\": 206, \"num_bytes\": 14622296}\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:08 INFO 139711786800960] #quality_metric: host=algo-4, epoch=7, train rmse <loss>=1.22350232722\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:08 INFO 139711786800960] #quality_metric: host=algo-4, epoch=7, train mse <loss>=1.4969579447\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:08 INFO 139711786800960] #quality_metric: host=algo-4, epoch=7, train absolute_loss <loss>=0.983214985079\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2634.104013442993, \"sum\": 2634.104013442993, \"min\": 2634.104013442993}}, \"EndTime\": 1572554048.830482, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554046.195768}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:08 INFO 139711786800960] #progress_metric: host=algo-4, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1649, \"sum\": 1649.0, \"min\": 1649}, \"Total Records Seen\": {\"count\": 1, \"max\": 1646768, \"sum\": 1646768.0, \"min\": 1646768}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 9, \"sum\": 9.0, \"min\": 9}}, \"EndTime\": 1572554048.830636, \"Dimensions\": {\"Host\": \"algo-4\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 7}, \"StartTime\": 1572554046.196352}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:08 INFO 139711786800960] #throughput_metric: host=algo-4, train throughput=78091.1256237 records/second\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:08 INFO 139711786800960] #quality_metric: host=algo-4, epoch=8, batch=0 train rmse <loss>=1.2239206222\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:08 INFO 139711786800960] #quality_metric: host=algo-4, epoch=8, batch=0 train mse <loss>=1.49798168945\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:08 INFO 139711786800960] #quality_metric: host=algo-4, epoch=8, batch=0 train absolute_loss <loss>=0.942015075684\u001b[0m\n",
      "\u001b[33m[2019-10-31 20:34:08.844] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 2664, \"num_examples\": 206, \"num_bytes\": 14630020}\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:08 INFO 140279785944896] #quality_metric: host=algo-3, epoch=7, train rmse <loss>=1.21243957681\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:08 INFO 140279785944896] #quality_metric: host=algo-3, epoch=7, train mse <loss>=1.4700097274\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:08 INFO 140279785944896] #quality_metric: host=algo-3, epoch=7, train absolute_loss <loss>=0.976618540347\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2666.6641235351562, \"sum\": 2666.6641235351562, \"min\": 2666.6641235351562}}, \"EndTime\": 1572554048.844624, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554046.177582}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:08 INFO 140279785944896] #progress_metric: host=algo-3, completed 40 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1649, \"sum\": 1649.0, \"min\": 1649}, \"Total Records Seen\": {\"count\": 1, \"max\": 1646768, \"sum\": 1646768.0, \"min\": 1646768}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 9, \"sum\": 9.0, \"min\": 9}}, \"EndTime\": 1572554048.844778, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 7}, \"StartTime\": 1572554046.177933}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:08 INFO 140279785944896] #throughput_metric: host=algo-3, train throughput=77137.5588135 records/second\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:08 INFO 140279785944896] #quality_metric: host=algo-3, epoch=8, batch=0 train rmse <loss>=1.1432395121\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:08 INFO 140279785944896] #quality_metric: host=algo-3, epoch=8, batch=0 train mse <loss>=1.30699658203\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:08 INFO 140279785944896] #quality_metric: host=algo-3, epoch=8, batch=0 train absolute_loss <loss>=0.959352783203\u001b[0m\n",
      "\u001b[32m[2019-10-31 20:34:09.569] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 4387, \"num_examples\": 309, \"num_bytes\": 21931124}\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:09 INFO 140034480109376] #quality_metric: host=algo-2, epoch=4, train rmse <loss>=1.23727856241\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:09 INFO 140034480109376] #quality_metric: host=algo-2, epoch=4, train mse <loss>=1.53085824101\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:09 INFO 140034480109376] #quality_metric: host=algo-2, epoch=4, train absolute_loss <loss>=0.986541181546\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4389.331817626953, \"sum\": 4389.331817626953, \"min\": 4389.331817626953}}, \"EndTime\": 1572554049.569911, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554045.180295}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:09 INFO 140034480109376] #progress_metric: host=algo-2, completed 25 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1546, \"sum\": 1546.0, \"min\": 1546}, \"Total Records Seen\": {\"count\": 1, \"max\": 1543910, \"sum\": 1543910.0, \"min\": 1543910}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1572554049.570065, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 4}, \"StartTime\": 1572554045.180554}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:09 INFO 140034480109376] #throughput_metric: host=algo-2, train throughput=70298.5122247 records/second\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:09 INFO 140034480109376] #quality_metric: host=algo-2, epoch=5, batch=0 train rmse <loss>=1.22455862385\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:09 INFO 140034480109376] #quality_metric: host=algo-2, epoch=5, batch=0 train mse <loss>=1.49954382324\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:09 INFO 140034480109376] #quality_metric: host=algo-2, epoch=5, batch=0 train absolute_loss <loss>=0.946943603516\u001b[0m\n",
      "\u001b[34m[2019-10-31 20:34:11.580] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 18, \"duration\": 2748, \"num_examples\": 206, \"num_bytes\": 14622296}\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:11 INFO 139711786800960] #quality_metric: host=algo-4, epoch=8, train rmse <loss>=1.21552869862\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:11 INFO 139711786800960] #quality_metric: host=algo-4, epoch=8, train mse <loss>=1.47751001717\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:11 INFO 139711786800960] #quality_metric: host=algo-4, epoch=8, train absolute_loss <loss>=0.974843190314\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2749.809980392456, \"sum\": 2749.809980392456, \"min\": 2749.809980392456}}, \"EndTime\": 1572554051.581047, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554048.830543}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:11 INFO 139711786800960] #progress_metric: host=algo-4, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1855, \"sum\": 1855.0, \"min\": 1855}, \"Total Records Seen\": {\"count\": 1, \"max\": 1852489, \"sum\": 1852489.0, \"min\": 1852489}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1572554051.581222, \"Dimensions\": {\"Host\": \"algo-4\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 8}, \"StartTime\": 1572554048.831208}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:11 INFO 139711786800960] #throughput_metric: host=algo-4, train throughput=74804.8606095 records/second\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:11 INFO 139711786800960] #quality_metric: host=algo-4, epoch=9, batch=0 train rmse <loss>=1.20812905174\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:11 INFO 139711786800960] #quality_metric: host=algo-4, epoch=9, batch=0 train mse <loss>=1.45957580566\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:11 INFO 139711786800960] #quality_metric: host=algo-4, epoch=9, batch=0 train absolute_loss <loss>=0.94063482666\u001b[0m\n",
      "\u001b[33m[2019-10-31 20:34:11.533] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 18, \"duration\": 2686, \"num_examples\": 206, \"num_bytes\": 14630020}\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:11 INFO 140279785944896] #quality_metric: host=algo-3, epoch=8, train rmse <loss>=1.20282027502\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:11 INFO 140279785944896] #quality_metric: host=algo-3, epoch=8, train mse <loss>=1.44677661399\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:11 INFO 140279785944896] #quality_metric: host=algo-3, epoch=8, train absolute_loss <loss>=0.964262271918\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2688.662052154541, \"sum\": 2688.662052154541, \"min\": 2688.662052154541}}, \"EndTime\": 1572554051.534011, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554048.844687}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:11 INFO 140279785944896] #progress_metric: host=algo-3, completed 45 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1855, \"sum\": 1855.0, \"min\": 1855}, \"Total Records Seen\": {\"count\": 1, \"max\": 1852489, \"sum\": 1852489.0, \"min\": 1852489}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1572554051.534159, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 8}, \"StartTime\": 1572554048.845325}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:11 INFO 140279785944896] #throughput_metric: host=algo-3, train throughput=76506.985167 records/second\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:11 INFO 140279785944896] #quality_metric: host=algo-3, epoch=9, batch=0 train rmse <loss>=1.23891702921\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:11 INFO 140279785944896] #quality_metric: host=algo-3, epoch=9, batch=0 train mse <loss>=1.53491540527\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:11 INFO 140279785944896] #quality_metric: host=algo-3, epoch=9, batch=0 train absolute_loss <loss>=1.10999511719\u001b[0m\n",
      "\u001b[31m[2019-10-31 20:34:11.544] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 12, \"duration\": 4059, \"num_examples\": 309, \"num_bytes\": 21961380}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:11 INFO 140221098768192] #quality_metric: host=algo-1, epoch=5, train rmse <loss>=1.21578004282\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:11 INFO 140221098768192] #quality_metric: host=algo-1, epoch=5, train mse <loss>=1.47812111251\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:11 INFO 140221098768192] #quality_metric: host=algo-1, epoch=5, train absolute_loss <loss>=0.979133153625\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4060.9230995178223, \"sum\": 4060.9230995178223, \"min\": 4060.9230995178223}}, \"EndTime\": 1572554051.544696, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554047.48301}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:11 INFO 140221098768192] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1855, \"sum\": 1855.0, \"min\": 1855}, \"Total Records Seen\": {\"count\": 1, \"max\": 1852492, \"sum\": 1852492.0, \"min\": 1852492}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 7, \"sum\": 7.0, \"min\": 7}}, \"EndTime\": 1572554051.544851, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 5}, \"StartTime\": 1572554047.483744}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:11 INFO 140221098768192] #throughput_metric: host=algo-1, train throughput=75982.7528552 records/second\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:11 INFO 140221098768192] #quality_metric: host=algo-1, epoch=6, batch=0 train rmse <loss>=1.24053476923\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:11 INFO 140221098768192] #quality_metric: host=algo-1, epoch=6, batch=0 train mse <loss>=1.53892651367\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:11 INFO 140221098768192] #quality_metric: host=algo-1, epoch=6, batch=0 train absolute_loss <loss>=1.13398168945\u001b[0m\n",
      "\u001b[32m[2019-10-31 20:34:14.338] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 12, \"duration\": 4765, \"num_examples\": 309, \"num_bytes\": 21931124}\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:14 INFO 140034480109376] #quality_metric: host=algo-2, epoch=5, train rmse <loss>=1.23351113859\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:14 INFO 140034480109376] #quality_metric: host=algo-2, epoch=5, train mse <loss>=1.52154972904\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:14 INFO 140034480109376] #quality_metric: host=algo-2, epoch=5, train absolute_loss <loss>=0.984681144048\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4767.784118652344, \"sum\": 4767.784118652344, \"min\": 4767.784118652344}}, \"EndTime\": 1572554054.338453, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554049.569974}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:14 INFO 140034480109376] #progress_metric: host=algo-2, completed 30 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1855, \"sum\": 1855.0, \"min\": 1855}, \"Total Records Seen\": {\"count\": 1, \"max\": 1852492, \"sum\": 1852492.0, \"min\": 1852492}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 7, \"sum\": 7.0, \"min\": 7}}, \"EndTime\": 1572554054.338606, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 5}, \"StartTime\": 1572554049.570642}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:14 INFO 140034480109376] #throughput_metric: host=algo-2, train throughput=64718.6590528 records/second\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:14 INFO 140034480109376] #quality_metric: host=algo-2, epoch=6, batch=0 train rmse <loss>=1.30449090959\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:14 INFO 140034480109376] #quality_metric: host=algo-2, epoch=6, batch=0 train mse <loss>=1.7016965332\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:14 INFO 140034480109376] #quality_metric: host=algo-2, epoch=6, batch=0 train absolute_loss <loss>=1.14625085449\u001b[0m\n",
      "\u001b[34m[2019-10-31 20:34:14.386] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 2803, \"num_examples\": 206, \"num_bytes\": 14622296}\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:14 INFO 139711786800960] #quality_metric: host=algo-4, epoch=9, train rmse <loss>=1.21302888627\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:14 INFO 139711786800960] #quality_metric: host=algo-4, epoch=9, train mse <loss>=1.47143907891\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:14 INFO 139711786800960] #quality_metric: host=algo-4, epoch=9, train absolute_loss <loss>=0.971045568077\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2805.2680492401123, \"sum\": 2805.2680492401123, \"min\": 2805.2680492401123}}, \"EndTime\": 1572554054.387145, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554051.581107}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:14 INFO 139711786800960] #progress_metric: host=algo-4, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2061, \"sum\": 2061.0, \"min\": 2061}, \"Total Records Seen\": {\"count\": 1, \"max\": 2058210, \"sum\": 2058210.0, \"min\": 2058210}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 11, \"sum\": 11.0, \"min\": 11}}, \"EndTime\": 1572554054.387318, \"Dimensions\": {\"Host\": \"algo-4\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 9}, \"StartTime\": 1572554051.581848}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:14 INFO 139711786800960] #throughput_metric: host=algo-4, train throughput=73326.2589494 records/second\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:14 INFO 139711786800960] #quality_metric: host=algo-4, epoch=10, batch=0 train rmse <loss>=1.23589611577\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:14 INFO 139711786800960] #quality_metric: host=algo-4, epoch=10, batch=0 train mse <loss>=1.52743920898\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:14 INFO 139711786800960] #quality_metric: host=algo-4, epoch=10, batch=0 train absolute_loss <loss>=0.947019226074\u001b[0m\n",
      "\u001b[33m[2019-10-31 20:34:14.482] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 2946, \"num_examples\": 206, \"num_bytes\": 14630020}\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:14 INFO 140279785944896] #quality_metric: host=algo-3, epoch=9, train rmse <loss>=1.1985396355\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:14 INFO 140279785944896] #quality_metric: host=algo-3, epoch=9, train mse <loss>=1.43649725786\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:14 INFO 140279785944896] #quality_metric: host=algo-3, epoch=9, train absolute_loss <loss>=0.963257701096\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2948.348045349121, \"sum\": 2948.348045349121, \"min\": 2948.348045349121}}, \"EndTime\": 1572554054.483109, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554051.534072}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:14 INFO 140279785944896] #progress_metric: host=algo-3, completed 50 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2061, \"sum\": 2061.0, \"min\": 2061}, \"Total Records Seen\": {\"count\": 1, \"max\": 2058210, \"sum\": 2058210.0, \"min\": 2058210}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 11, \"sum\": 11.0, \"min\": 11}}, \"EndTime\": 1572554054.483257, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 9}, \"StartTime\": 1572554051.534735}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:14 INFO 140279785944896] #throughput_metric: host=algo-3, train throughput=69768.8780627 records/second\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:14 INFO 140279785944896] #quality_metric: host=algo-3, epoch=10, batch=0 train rmse <loss>=1.12578423187\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:14 INFO 140279785944896] #quality_metric: host=algo-3, epoch=10, batch=0 train mse <loss>=1.26739013672\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:14 INFO 140279785944896] #quality_metric: host=algo-3, epoch=10, batch=0 train absolute_loss <loss>=0.923506286621\u001b[0m\n",
      "\u001b[31m[2019-10-31 20:34:15.822] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 4275, \"num_examples\": 309, \"num_bytes\": 21961380}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:15 INFO 140221098768192] #quality_metric: host=algo-1, epoch=6, train rmse <loss>=1.21372160123\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:15 INFO 140221098768192] #quality_metric: host=algo-1, epoch=6, train mse <loss>=1.47312012529\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:15 INFO 140221098768192] #quality_metric: host=algo-1, epoch=6, train absolute_loss <loss>=0.972103012529\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4277.550935745239, \"sum\": 4277.550935745239, \"min\": 4277.550935745239}}, \"EndTime\": 1572554055.822592, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554051.544759}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:15 INFO 140221098768192] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2164, \"sum\": 2164.0, \"min\": 2164}, \"Total Records Seen\": {\"count\": 1, \"max\": 2161074, \"sum\": 2161074.0, \"min\": 2161074}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1572554055.822779, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 6}, \"StartTime\": 1572554051.545009}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:15 INFO 140221098768192] #throughput_metric: host=algo-1, train throughput=72134.6506264 records/second\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:15 INFO 140221098768192] #quality_metric: host=algo-1, epoch=7, batch=0 train rmse <loss>=1.07010616168\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:15 INFO 140221098768192] #quality_metric: host=algo-1, epoch=7, batch=0 train mse <loss>=1.14512719727\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:15 INFO 140221098768192] #quality_metric: host=algo-1, epoch=7, batch=0 train absolute_loss <loss>=0.890885070801\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2019-10-31 20:34:17.263] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 22, \"duration\": 2874, \"num_examples\": 206, \"num_bytes\": 14622296}\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:17 INFO 139711786800960] #quality_metric: host=algo-4, epoch=10, train rmse <loss>=1.20658453054\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:17 INFO 139711786800960] #quality_metric: host=algo-4, epoch=10, train mse <loss>=1.45584622933\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:17 INFO 139711786800960] #quality_metric: host=algo-4, epoch=10, train absolute_loss <loss>=0.962628084942\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2876.37996673584, \"sum\": 2876.37996673584, \"min\": 2876.37996673584}}, \"EndTime\": 1572554057.264358, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554054.387205}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:17 INFO 139711786800960] #progress_metric: host=algo-4, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2267, \"sum\": 2267.0, \"min\": 2267}, \"Total Records Seen\": {\"count\": 1, \"max\": 2263931, \"sum\": 2263931.0, \"min\": 2263931}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1572554057.264537, \"Dimensions\": {\"Host\": \"algo-4\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 10}, \"StartTime\": 1572554054.387954}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:17 INFO 139711786800960] #throughput_metric: host=algo-4, train throughput=71512.9175797 records/second\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:17 INFO 139711786800960] #quality_metric: host=algo-4, epoch=11, batch=0 train rmse <loss>=1.22444333254\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:17 INFO 139711786800960] #quality_metric: host=algo-4, epoch=11, batch=0 train mse <loss>=1.49926147461\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:17 INFO 139711786800960] #quality_metric: host=algo-4, epoch=11, batch=0 train absolute_loss <loss>=0.94226574707\u001b[0m\n",
      "\u001b[33m[2019-10-31 20:34:17.380] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 22, \"duration\": 2895, \"num_examples\": 206, \"num_bytes\": 14630020}\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:17 INFO 140279785944896] #quality_metric: host=algo-3, epoch=10, train rmse <loss>=1.1995324117\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:17 INFO 140279785944896] #quality_metric: host=algo-3, epoch=10, train mse <loss>=1.43887800672\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:17 INFO 140279785944896] #quality_metric: host=algo-3, epoch=10, train absolute_loss <loss>=0.960242831036\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2897.351026535034, \"sum\": 2897.351026535034, \"min\": 2897.351026535034}}, \"EndTime\": 1572554057.381267, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554054.483169}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:17 INFO 140279785944896] #progress_metric: host=algo-3, completed 55 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2267, \"sum\": 2267.0, \"min\": 2267}, \"Total Records Seen\": {\"count\": 1, \"max\": 2263931, \"sum\": 2263931.0, \"min\": 2263931}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1572554057.381408, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 10}, \"StartTime\": 1572554054.483888}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:17 INFO 140279785944896] #throughput_metric: host=algo-3, train throughput=70996.8349877 records/second\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:17 INFO 140279785944896] #quality_metric: host=algo-3, epoch=11, batch=0 train rmse <loss>=1.07142164246\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:17 INFO 140279785944896] #quality_metric: host=algo-3, epoch=11, batch=0 train mse <loss>=1.14794433594\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:17 INFO 140279785944896] #quality_metric: host=algo-3, epoch=11, batch=0 train absolute_loss <loss>=0.839870056152\u001b[0m\n",
      "\u001b[32m[2019-10-31 20:34:19.014] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 4674, \"num_examples\": 309, \"num_bytes\": 21931124}\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:19 INFO 140034480109376] #quality_metric: host=algo-2, epoch=6, train rmse <loss>=1.24040666657\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:19 INFO 140034480109376] #quality_metric: host=algo-2, epoch=6, train mse <loss>=1.53860869848\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:19 INFO 140034480109376] #quality_metric: host=algo-2, epoch=6, train absolute_loss <loss>=0.985604140198\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4676.207065582275, \"sum\": 4676.207065582275, \"min\": 4676.207065582275}}, \"EndTime\": 1572554059.014993, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554054.338514}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:19 INFO 140034480109376] #progress_metric: host=algo-2, completed 35 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2164, \"sum\": 2164.0, \"min\": 2164}, \"Total Records Seen\": {\"count\": 1, \"max\": 2161074, \"sum\": 2161074.0, \"min\": 2161074}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1572554059.015159, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 6}, \"StartTime\": 1572554054.338763}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:19 INFO 140034480109376] #throughput_metric: host=algo-2, train throughput=65985.9578385 records/second\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:19 INFO 140034480109376] #quality_metric: host=algo-2, epoch=7, batch=0 train rmse <loss>=1.23573743047\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:19 INFO 140034480109376] #quality_metric: host=algo-2, epoch=7, batch=0 train mse <loss>=1.52704699707\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:19 INFO 140034480109376] #quality_metric: host=algo-2, epoch=7, batch=0 train absolute_loss <loss>=0.93975994873\u001b[0m\n",
      "\u001b[34m[2019-10-31 20:34:20.134] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 24, \"duration\": 2867, \"num_examples\": 206, \"num_bytes\": 14622296}\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:20 INFO 139711786800960] #quality_metric: host=algo-4, epoch=11, train rmse <loss>=1.21832187415\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:20 INFO 139711786800960] #quality_metric: host=algo-4, epoch=11, train mse <loss>=1.48430818902\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:20 INFO 139711786800960] #quality_metric: host=algo-4, epoch=11, train absolute_loss <loss>=0.97388917823\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2869.8179721832275, \"sum\": 2869.8179721832275, \"min\": 2869.8179721832275}}, \"EndTime\": 1572554060.135094, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554057.26442}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:20 INFO 139711786800960] #progress_metric: host=algo-4, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2473, \"sum\": 2473.0, \"min\": 2473}, \"Total Records Seen\": {\"count\": 1, \"max\": 2469652, \"sum\": 2469652.0, \"min\": 2469652}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 13, \"sum\": 13.0, \"min\": 13}}, \"EndTime\": 1572554060.135249, \"Dimensions\": {\"Host\": \"algo-4\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 11}, \"StartTime\": 1572554057.265249}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:20 INFO 139711786800960] #throughput_metric: host=algo-4, train throughput=71677.4894303 records/second\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:20 INFO 139711786800960] #quality_metric: host=algo-4, epoch=12, batch=0 train rmse <loss>=1.23488685985\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:20 INFO 139711786800960] #quality_metric: host=algo-4, epoch=12, batch=0 train mse <loss>=1.52494555664\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:20 INFO 139711786800960] #quality_metric: host=algo-4, epoch=12, batch=0 train absolute_loss <loss>=0.946644104004\u001b[0m\n",
      "\u001b[33m[2019-10-31 20:34:20.283] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 24, \"duration\": 2899, \"num_examples\": 206, \"num_bytes\": 14630020}\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:20 INFO 140279785944896] #quality_metric: host=algo-3, epoch=11, train rmse <loss>=1.20652467698\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:20 INFO 140279785944896] #quality_metric: host=algo-3, epoch=11, train mse <loss>=1.45570179615\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:20 INFO 140279785944896] #quality_metric: host=algo-3, epoch=11, train absolute_loss <loss>=0.968964177993\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2901.9830226898193, \"sum\": 2901.9830226898193, \"min\": 2901.9830226898193}}, \"EndTime\": 1572554060.284125, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554057.381322}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:20 INFO 140279785944896] #progress_metric: host=algo-3, completed 60 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2473, \"sum\": 2473.0, \"min\": 2473}, \"Total Records Seen\": {\"count\": 1, \"max\": 2469652, \"sum\": 2469652.0, \"min\": 2469652}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 13, \"sum\": 13.0, \"min\": 13}}, \"EndTime\": 1572554060.284305, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 11}, \"StartTime\": 1572554057.382113}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:20 INFO 140279785944896] #throughput_metric: host=algo-3, train throughput=70882.2774088 records/second\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:20 INFO 140279785944896] #quality_metric: host=algo-3, epoch=12, batch=0 train rmse <loss>=1.10082915527\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:20 INFO 140279785944896] #quality_metric: host=algo-3, epoch=12, batch=0 train mse <loss>=1.2118248291\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:20 INFO 140279785944896] #quality_metric: host=algo-3, epoch=12, batch=0 train absolute_loss <loss>=0.891889953613\u001b[0m\n",
      "\u001b[31m[2019-10-31 20:34:20.213] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 4389, \"num_examples\": 309, \"num_bytes\": 21961380}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:20 INFO 140221098768192] #quality_metric: host=algo-1, epoch=7, train rmse <loss>=1.21192063985\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:20 INFO 140221098768192] #quality_metric: host=algo-1, epoch=7, train mse <loss>=1.46875163728\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:20 INFO 140221098768192] #quality_metric: host=algo-1, epoch=7, train absolute_loss <loss>=0.972405444889\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4391.146183013916, \"sum\": 4391.146183013916, \"min\": 4391.146183013916}}, \"EndTime\": 1572554060.21415, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554055.822654}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:20 INFO 140221098768192] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2473, \"sum\": 2473.0, \"min\": 2473}, \"Total Records Seen\": {\"count\": 1, \"max\": 2469656, \"sum\": 2469656.0, \"min\": 2469656}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 9, \"sum\": 9.0, \"min\": 9}}, \"EndTime\": 1572554060.214302, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 7}, \"StartTime\": 1572554055.822977}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:20 INFO 140221098768192] #throughput_metric: host=algo-1, train throughput=70269.3454383 records/second\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:20 INFO 140221098768192] #quality_metric: host=algo-1, epoch=8, batch=0 train rmse <loss>=1.07578947664\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:20 INFO 140221098768192] #quality_metric: host=algo-1, epoch=8, batch=0 train mse <loss>=1.15732299805\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:20 INFO 140221098768192] #quality_metric: host=algo-1, epoch=8, batch=0 train absolute_loss <loss>=0.901458618164\u001b[0m\n",
      "\u001b[34m[2019-10-31 20:34:23.091] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 2954, \"num_examples\": 206, \"num_bytes\": 14622296}\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:23 INFO 139711786800960] #quality_metric: host=algo-4, epoch=12, train rmse <loss>=1.22214380216\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:23 INFO 139711786800960] #quality_metric: host=algo-4, epoch=12, train mse <loss>=1.49363547316\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:23 INFO 139711786800960] #quality_metric: host=algo-4, epoch=12, train absolute_loss <loss>=0.978190000367\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2956.8419456481934, \"sum\": 2956.8419456481934, \"min\": 2956.8419456481934}}, \"EndTime\": 1572554063.092343, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554060.135156}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:23 INFO 139711786800960] #progress_metric: host=algo-4, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2679, \"sum\": 2679.0, \"min\": 2679}, \"Total Records Seen\": {\"count\": 1, \"max\": 2675373, \"sum\": 2675373.0, \"min\": 2675373}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1572554063.092489, \"Dimensions\": {\"Host\": \"algo-4\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 12}, \"StartTime\": 1572554060.135476}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:23 INFO 139711786800960] #throughput_metric: host=algo-4, train throughput=69567.931666 records/second\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:23 INFO 139711786800960] #quality_metric: host=algo-4, epoch=13, batch=0 train rmse <loss>=1.21409789202\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:23 INFO 139711786800960] #quality_metric: host=algo-4, epoch=13, batch=0 train mse <loss>=1.47403369141\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:23 INFO 139711786800960] #quality_metric: host=algo-4, epoch=13, batch=0 train absolute_loss <loss>=0.93618170166\u001b[0m\n",
      "\u001b[33m[2019-10-31 20:34:23.244] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 2957, \"num_examples\": 206, \"num_bytes\": 14630020}\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:23 INFO 140279785944896] #quality_metric: host=algo-3, epoch=12, train rmse <loss>=1.20393728318\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:23 INFO 140279785944896] #quality_metric: host=algo-3, epoch=12, train mse <loss>=1.44946498182\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:23 INFO 140279785944896] #quality_metric: host=algo-3, epoch=12, train absolute_loss <loss>=0.964293534066\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2959.6121311187744, \"sum\": 2959.6121311187744, \"min\": 2959.6121311187744}}, \"EndTime\": 1572554063.2448, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554060.284189}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:23 INFO 140279785944896] #progress_metric: host=algo-3, completed 65 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2679, \"sum\": 2679.0, \"min\": 2679}, \"Total Records Seen\": {\"count\": 1, \"max\": 2675373, \"sum\": 2675373.0, \"min\": 2675373}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1572554063.24497, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 12}, \"StartTime\": 1572554060.28516}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:23 INFO 140279785944896] #throughput_metric: host=algo-3, train throughput=69502.5930218 records/second\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:23 INFO 140279785944896] #quality_metric: host=algo-3, epoch=13, batch=0 train rmse <loss>=1.25057452617\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:23 INFO 140279785944896] #quality_metric: host=algo-3, epoch=13, batch=0 train mse <loss>=1.56393664551\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:23 INFO 140279785944896] #quality_metric: host=algo-3, epoch=13, batch=0 train absolute_loss <loss>=1.1254609375\u001b[0m\n",
      "\u001b[32m[2019-10-31 20:34:23.565] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 4549, \"num_examples\": 309, \"num_bytes\": 21931124}\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:23 INFO 140034480109376] #quality_metric: host=algo-2, epoch=7, train rmse <loss>=1.24006099283\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:23 INFO 140034480109376] #quality_metric: host=algo-2, epoch=7, train mse <loss>=1.53775126594\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:23 INFO 140034480109376] #quality_metric: host=algo-2, epoch=7, train absolute_loss <loss>=0.989517911349\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4551.151990890503, \"sum\": 4551.151990890503, \"min\": 4551.151990890503}}, \"EndTime\": 1572554063.566518, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554059.015052}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:23 INFO 140034480109376] #progress_metric: host=algo-2, completed 40 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2473, \"sum\": 2473.0, \"min\": 2473}, \"Total Records Seen\": {\"count\": 1, \"max\": 2469656, \"sum\": 2469656.0, \"min\": 2469656}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 9, \"sum\": 9.0, \"min\": 9}}, \"EndTime\": 1572554063.566732, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 7}, \"StartTime\": 1572554059.015337}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:23 INFO 140034480109376] #throughput_metric: host=algo-2, train throughput=67797.8676128 records/second\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:23 INFO 140034480109376] #quality_metric: host=algo-2, epoch=8, batch=0 train rmse <loss>=1.32034411944\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:23 INFO 140034480109376] #quality_metric: host=algo-2, epoch=8, batch=0 train mse <loss>=1.74330859375\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:23 INFO 140034480109376] #quality_metric: host=algo-2, epoch=8, batch=0 train absolute_loss <loss>=0.929428771973\u001b[0m\n",
      "\u001b[34m[2019-10-31 20:34:26.056] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 28, \"duration\": 2961, \"num_examples\": 206, \"num_bytes\": 14622296}\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:26 INFO 139711786800960] #quality_metric: host=algo-4, epoch=13, train rmse <loss>=1.23006340134\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:26 INFO 139711786800960] #quality_metric: host=algo-4, epoch=13, train mse <loss>=1.51305597131\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:26 INFO 139711786800960] #quality_metric: host=algo-4, epoch=13, train absolute_loss <loss>=0.988318122049\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2963.801860809326, \"sum\": 2963.801860809326, \"min\": 2963.801860809326}}, \"EndTime\": 1572554066.056504, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554063.092399}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:26 INFO 139711786800960] #progress_metric: host=algo-4, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2885, \"sum\": 2885.0, \"min\": 2885}, \"Total Records Seen\": {\"count\": 1, \"max\": 2881094, \"sum\": 2881094.0, \"min\": 2881094}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 15, \"sum\": 15.0, \"min\": 15}}, \"EndTime\": 1572554066.056632, \"Dimensions\": {\"Host\": \"algo-4\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 13}, \"StartTime\": 1572554063.092678}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:26 INFO 139711786800960] #throughput_metric: host=algo-4, train throughput=69405.8420795 records/second\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:26 INFO 139711786800960] #quality_metric: host=algo-4, epoch=14, batch=0 train rmse <loss>=1.22563748958\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:26 INFO 139711786800960] #quality_metric: host=algo-4, epoch=14, batch=0 train mse <loss>=1.50218725586\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:26 INFO 139711786800960] #quality_metric: host=algo-4, epoch=14, batch=0 train absolute_loss <loss>=1.02534851074\u001b[0m\n",
      "\u001b[33m[2019-10-31 20:34:26.270] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 28, \"duration\": 3023, \"num_examples\": 206, \"num_bytes\": 14630020}\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:26 INFO 140279785944896] #quality_metric: host=algo-3, epoch=13, train rmse <loss>=1.22328836452\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:26 INFO 140279785944896] #quality_metric: host=algo-3, epoch=13, train mse <loss>=1.49643442276\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:26 INFO 140279785944896] #quality_metric: host=algo-3, epoch=13, train absolute_loss <loss>=0.984389695843\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3025.7351398468018, \"sum\": 3025.7351398468018, \"min\": 3025.7351398468018}}, \"EndTime\": 1572554066.271452, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554063.244864}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:26 INFO 140279785944896] #progress_metric: host=algo-3, completed 70 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2885, \"sum\": 2885.0, \"min\": 2885}, \"Total Records Seen\": {\"count\": 1, \"max\": 2881094, \"sum\": 2881094.0, \"min\": 2881094}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 15, \"sum\": 15.0, \"min\": 15}}, \"EndTime\": 1572554066.271663, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 13}, \"StartTime\": 1572554063.245676}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:26 INFO 140279785944896] #throughput_metric: host=algo-3, train throughput=67982.6516263 records/second\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:26 INFO 140279785944896] #quality_metric: host=algo-3, epoch=14, batch=0 train rmse <loss>=1.08422025868\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:26 INFO 140279785944896] #quality_metric: host=algo-3, epoch=14, batch=0 train mse <loss>=1.17553356934\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:26 INFO 140279785944896] #quality_metric: host=algo-3, epoch=14, batch=0 train absolute_loss <loss>=0.872004638672\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[2019-10-31 20:34:24.621] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 18, \"duration\": 4405, \"num_examples\": 309, \"num_bytes\": 21961380}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:24 INFO 140221098768192] #quality_metric: host=algo-1, epoch=8, train rmse <loss>=1.22128654056\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:24 INFO 140221098768192] #quality_metric: host=algo-1, epoch=8, train mse <loss>=1.49154081415\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:24 INFO 140221098768192] #quality_metric: host=algo-1, epoch=8, train absolute_loss <loss>=0.975459591961\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4407.644033432007, \"sum\": 4407.644033432007, \"min\": 4407.644033432007}}, \"EndTime\": 1572554064.622528, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554060.214212}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:24 INFO 140221098768192] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2782, \"sum\": 2782.0, \"min\": 2782}, \"Total Records Seen\": {\"count\": 1, \"max\": 2778238, \"sum\": 2778238.0, \"min\": 2778238}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1572554064.622681, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 8}, \"StartTime\": 1572554060.214857}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:24 INFO 140221098768192] #throughput_metric: host=algo-1, train throughput=70006.3087419 records/second\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:24 INFO 140221098768192] #quality_metric: host=algo-1, epoch=9, batch=0 train rmse <loss>=0.992706833819\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:24 INFO 140221098768192] #quality_metric: host=algo-1, epoch=9, batch=0 train mse <loss>=0.98546685791\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:24 INFO 140221098768192] #quality_metric: host=algo-1, epoch=9, batch=0 train absolute_loss <loss>=0.77794921875\u001b[0m\n",
      "\u001b[32m[2019-10-31 20:34:28.155] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 18, \"duration\": 4586, \"num_examples\": 309, \"num_bytes\": 21931124}\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:28 INFO 140034480109376] #quality_metric: host=algo-2, epoch=8, train rmse <loss>=1.24245467162\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:28 INFO 140034480109376] #quality_metric: host=algo-2, epoch=8, train mse <loss>=1.54369361102\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:28 INFO 140034480109376] #quality_metric: host=algo-2, epoch=8, train absolute_loss <loss>=0.988900998014\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4588.377952575684, \"sum\": 4588.377952575684, \"min\": 4588.377952575684}}, \"EndTime\": 1572554068.155687, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554063.566578}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:28 INFO 140034480109376] #progress_metric: host=algo-2, completed 45 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2782, \"sum\": 2782.0, \"min\": 2782}, \"Total Records Seen\": {\"count\": 1, \"max\": 2778238, \"sum\": 2778238.0, \"min\": 2778238}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1572554068.155861, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 8}, \"StartTime\": 1572554063.567282}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:28 INFO 140034480109376] #throughput_metric: host=algo-2, train throughput=67248.679676 records/second\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:28 INFO 140034480109376] #quality_metric: host=algo-2, epoch=9, batch=0 train rmse <loss>=1.26026013792\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:28 INFO 140034480109376] #quality_metric: host=algo-2, epoch=9, batch=0 train mse <loss>=1.58825561523\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:28 INFO 140034480109376] #quality_metric: host=algo-2, epoch=9, batch=0 train absolute_loss <loss>=1.06537976074\u001b[0m\n",
      "\u001b[34m[2019-10-31 20:34:28.988] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 30, \"duration\": 2929, \"num_examples\": 206, \"num_bytes\": 14622296}\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:28 INFO 139711786800960] #quality_metric: host=algo-4, epoch=14, train rmse <loss>=1.20411387519\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:28 INFO 139711786800960] #quality_metric: host=algo-4, epoch=14, train mse <loss>=1.44989022442\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:28 INFO 139711786800960] #quality_metric: host=algo-4, epoch=14, train absolute_loss <loss>=0.962365716731\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2931.7848682403564, \"sum\": 2931.7848682403564, \"min\": 2931.7848682403564}}, \"EndTime\": 1572554068.988582, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554066.056552}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:28 INFO 139711786800960] #progress_metric: host=algo-4, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3091, \"sum\": 3091.0, \"min\": 3091}, \"Total Records Seen\": {\"count\": 1, \"max\": 3086815, \"sum\": 3086815.0, \"min\": 3086815}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1572554068.988749, \"Dimensions\": {\"Host\": \"algo-4\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 14}, \"StartTime\": 1572554066.05677}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:28 INFO 139711786800960] #throughput_metric: host=algo-4, train throughput=70162.450515 records/second\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:29 INFO 139711786800960] #quality_metric: host=algo-4, epoch=15, batch=0 train rmse <loss>=1.20790168857\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:29 INFO 139711786800960] #quality_metric: host=algo-4, epoch=15, batch=0 train mse <loss>=1.45902648926\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:29 INFO 139711786800960] #quality_metric: host=algo-4, epoch=15, batch=0 train absolute_loss <loss>=0.927169494629\u001b[0m\n",
      "\u001b[33m[2019-10-31 20:34:29.220] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 30, \"duration\": 2946, \"num_examples\": 206, \"num_bytes\": 14630020}\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:29 INFO 140279785944896] #quality_metric: host=algo-3, epoch=14, train rmse <loss>=1.19208880282\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:29 INFO 140279785944896] #quality_metric: host=algo-3, epoch=14, train mse <loss>=1.42107571382\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:29 INFO 140279785944896] #quality_metric: host=algo-3, epoch=14, train absolute_loss <loss>=0.957469613973\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2948.6398696899414, \"sum\": 2948.6398696899414, \"min\": 2948.6398696899414}}, \"EndTime\": 1572554069.221179, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554066.271516}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:29 INFO 140279785944896] #progress_metric: host=algo-3, completed 75 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3091, \"sum\": 3091.0, \"min\": 3091}, \"Total Records Seen\": {\"count\": 1, \"max\": 3086815, \"sum\": 3086815.0, \"min\": 3086815}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1572554069.221345, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 14}, \"StartTime\": 1572554066.27251}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:29 INFO 140279785944896] #throughput_metric: host=algo-3, train throughput=69761.2855986 records/second\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:29 INFO 140279785944896] #quality_metric: host=algo-3, epoch=15, batch=0 train rmse <loss>=1.24490945877\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:29 INFO 140279785944896] #quality_metric: host=algo-3, epoch=15, batch=0 train mse <loss>=1.54979956055\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:29 INFO 140279785944896] #quality_metric: host=algo-3, epoch=15, batch=0 train absolute_loss <loss>=1.1179967041\u001b[0m\n",
      "\u001b[31m[2019-10-31 20:34:29.036] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 4412, \"num_examples\": 309, \"num_bytes\": 21961380}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:29 INFO 140221098768192] #quality_metric: host=algo-1, epoch=9, train rmse <loss>=1.21401654737\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:29 INFO 140221098768192] #quality_metric: host=algo-1, epoch=9, train mse <loss>=1.4738361773\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:29 INFO 140221098768192] #quality_metric: host=algo-1, epoch=9, train absolute_loss <loss>=0.976104310267\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4413.97500038147, \"sum\": 4413.97500038147, \"min\": 4413.97500038147}}, \"EndTime\": 1572554069.036845, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554064.62259}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:29 INFO 140221098768192] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3091, \"sum\": 3091.0, \"min\": 3091}, \"Total Records Seen\": {\"count\": 1, \"max\": 3086820, \"sum\": 3086820.0, \"min\": 3086820}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 11, \"sum\": 11.0, \"min\": 11}}, \"EndTime\": 1572554069.036989, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 9}, \"StartTime\": 1572554064.622845}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:29 INFO 140221098768192] #throughput_metric: host=algo-1, train throughput=69906.1578342 records/second\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:29 INFO 140221098768192] #quality_metric: host=algo-1, epoch=10, batch=0 train rmse <loss>=1.08171108756\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:29 INFO 140221098768192] #quality_metric: host=algo-1, epoch=10, batch=0 train mse <loss>=1.17009887695\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:29 INFO 140221098768192] #quality_metric: host=algo-1, epoch=10, batch=0 train absolute_loss <loss>=0.912140258789\u001b[0m\n",
      "\u001b[34m[2019-10-31 20:34:31.967] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 2976, \"num_examples\": 206, \"num_bytes\": 14622296}\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:31 INFO 139711786800960] #quality_metric: host=algo-4, epoch=15, train rmse <loss>=1.22009741562\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:31 INFO 139711786800960] #quality_metric: host=algo-4, epoch=15, train mse <loss>=1.48863770361\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:31 INFO 139711786800960] #quality_metric: host=algo-4, epoch=15, train absolute_loss <loss>=0.979010644413\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2978.835105895996, \"sum\": 2978.835105895996, \"min\": 2978.835105895996}}, \"EndTime\": 1572554071.967811, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554068.988632}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:31 INFO 139711786800960] #progress_metric: host=algo-4, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3297, \"sum\": 3297.0, \"min\": 3297}, \"Total Records Seen\": {\"count\": 1, \"max\": 3292536, \"sum\": 3292536.0, \"min\": 3292536}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 17, \"sum\": 17.0, \"min\": 17}}, \"EndTime\": 1572554071.967957, \"Dimensions\": {\"Host\": \"algo-4\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 15}, \"StartTime\": 1572554068.988952}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:31 INFO 139711786800960] #throughput_metric: host=algo-4, train throughput=69055.0743985 records/second\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:31 INFO 139711786800960] #quality_metric: host=algo-4, epoch=16, batch=0 train rmse <loss>=1.2103765838\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:31 INFO 139711786800960] #quality_metric: host=algo-4, epoch=16, batch=0 train mse <loss>=1.46501147461\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:31 INFO 139711786800960] #quality_metric: host=algo-4, epoch=16, batch=0 train absolute_loss <loss>=0.96520489502\u001b[0m\n",
      "\u001b[33m[2019-10-31 20:34:32.243] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 3019, \"num_examples\": 206, \"num_bytes\": 14630020}\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:32 INFO 140279785944896] #quality_metric: host=algo-3, epoch=15, train rmse <loss>=1.20593661016\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:32 INFO 140279785944896] #quality_metric: host=algo-3, epoch=15, train mse <loss>=1.45428310772\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:32 INFO 140279785944896] #quality_metric: host=algo-3, epoch=15, train absolute_loss <loss>=0.971681049532\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3021.0111141204834, \"sum\": 3021.0111141204834, \"min\": 3021.0111141204834}}, \"EndTime\": 1572554072.243649, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554069.221239}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:32 INFO 140279785944896] #progress_metric: host=algo-3, completed 80 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3297, \"sum\": 3297.0, \"min\": 3297}, \"Total Records Seen\": {\"count\": 1, \"max\": 3292536, \"sum\": 3292536.0, \"min\": 3292536}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 17, \"sum\": 17.0, \"min\": 17}}, \"EndTime\": 1572554072.243789, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 15}, \"StartTime\": 1572554069.2226}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:32 INFO 140279785944896] #throughput_metric: host=algo-3, train throughput=68089.4774533 records/second\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:32 INFO 140279785944896] #quality_metric: host=algo-3, epoch=16, batch=0 train rmse <loss>=1.25074601762\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:32 INFO 140279785944896] #quality_metric: host=algo-3, epoch=16, batch=0 train mse <loss>=1.56436560059\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:32 INFO 140279785944896] #quality_metric: host=algo-3, epoch=16, batch=0 train absolute_loss <loss>=1.1256854248\u001b[0m\n",
      "\u001b[31m[2019-10-31 20:34:33.459] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 22, \"duration\": 4421, \"num_examples\": 309, \"num_bytes\": 21961380}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:33 INFO 140221098768192] #quality_metric: host=algo-1, epoch=10, train rmse <loss>=1.21749375233\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:33 INFO 140221098768192] #quality_metric: host=algo-1, epoch=10, train mse <loss>=1.48229103696\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:33 INFO 140221098768192] #quality_metric: host=algo-1, epoch=10, train absolute_loss <loss>=0.974707321019\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4422.891855239868, \"sum\": 4422.891855239868, \"min\": 4422.891855239868}}, \"EndTime\": 1572554073.460069, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554069.036899}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:33 INFO 140221098768192] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3400, \"sum\": 3400.0, \"min\": 3400}, \"Total Records Seen\": {\"count\": 1, \"max\": 3395402, \"sum\": 3395402.0, \"min\": 3395402}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1572554073.460233, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 10}, \"StartTime\": 1572554069.03715}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:33 INFO 140221098768192] #throughput_metric: host=algo-1, train throughput=69764.9445924 records/second\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:33 INFO 140221098768192] #quality_metric: host=algo-1, epoch=11, batch=0 train rmse <loss>=0.99962334215\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:33 INFO 140221098768192] #quality_metric: host=algo-1, epoch=11, batch=0 train mse <loss>=0.999246826172\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:33 INFO 140221098768192] #quality_metric: host=algo-1, epoch=11, batch=0 train absolute_loss <loss>=0.809123291016\u001b[0m\n",
      "\u001b[32m[2019-10-31 20:34:32.759] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 4602, \"num_examples\": 309, \"num_bytes\": 21931124}\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:32 INFO 140034480109376] #quality_metric: host=algo-2, epoch=9, train rmse <loss>=1.23536384441\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:32 INFO 140034480109376] #quality_metric: host=algo-2, epoch=9, train mse <loss>=1.52612382809\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:32 INFO 140034480109376] #quality_metric: host=algo-2, epoch=9, train absolute_loss <loss>=0.986851326655\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4604.25591468811, \"sum\": 4604.25591468811, \"min\": 4604.25591468811}}, \"EndTime\": 1572554072.760308, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554068.155748}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:32 INFO 140034480109376] #progress_metric: host=algo-2, completed 50 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3091, \"sum\": 3091.0, \"min\": 3091}, \"Total Records Seen\": {\"count\": 1, \"max\": 3086820, \"sum\": 3086820.0, \"min\": 3086820}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 11, \"sum\": 11.0, \"min\": 11}}, \"EndTime\": 1572554072.760509, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 9}, \"StartTime\": 1572554068.156027}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:32 INFO 140034480109376] #throughput_metric: host=algo-2, train throughput=67016.2467487 records/second\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:32 INFO 140034480109376] #quality_metric: host=algo-2, epoch=10, batch=0 train rmse <loss>=1.2358183808\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:32 INFO 140034480109376] #quality_metric: host=algo-2, epoch=10, batch=0 train mse <loss>=1.52724707031\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:32 INFO 140034480109376] #quality_metric: host=algo-2, epoch=10, batch=0 train absolute_loss <loss>=0.996461791992\u001b[0m\n",
      "\u001b[32m[2019-10-31 20:34:37.466] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 22, \"duration\": 4704, \"num_examples\": 309, \"num_bytes\": 21931124}\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:37 INFO 140034480109376] #quality_metric: host=algo-2, epoch=10, train rmse <loss>=1.23560788695\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:37 INFO 140034480109376] #quality_metric: host=algo-2, epoch=10, train mse <loss>=1.52672685029\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:37 INFO 140034480109376] #quality_metric: host=algo-2, epoch=10, train absolute_loss <loss>=0.984999767316\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4706.291913986206, \"sum\": 4706.291913986206, \"min\": 4706.291913986206}}, \"EndTime\": 1572554077.467026, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554072.760378}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:37 INFO 140034480109376] #progress_metric: host=algo-2, completed 55 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3400, \"sum\": 3400.0, \"min\": 3400}, \"Total Records Seen\": {\"count\": 1, \"max\": 3395402, \"sum\": 3395402.0, \"min\": 3395402}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1572554077.467208, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 10}, \"StartTime\": 1572554072.760703}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:37 INFO 140034480109376] #throughput_metric: host=algo-2, train throughput=65563.7469703 records/second\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:37 INFO 140034480109376] #quality_metric: host=algo-2, epoch=11, batch=0 train rmse <loss>=1.22413618499\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:37 INFO 140034480109376] #quality_metric: host=algo-2, epoch=11, batch=0 train mse <loss>=1.49850939941\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:37 INFO 140034480109376] #quality_metric: host=algo-2, epoch=11, batch=0 train absolute_loss <loss>=0.946578125\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2019-10-31 20:34:34.946] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 34, \"duration\": 2976, \"num_examples\": 206, \"num_bytes\": 14622296}\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:34 INFO 139711786800960] #quality_metric: host=algo-4, epoch=16, train rmse <loss>=1.2135177607\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:34 INFO 139711786800960] #quality_metric: host=algo-4, epoch=16, train mse <loss>=1.47262535554\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:34 INFO 139711786800960] #quality_metric: host=algo-4, epoch=16, train absolute_loss <loss>=0.971689663193\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2978.6159992218018, \"sum\": 2978.6159992218018, \"min\": 2978.6159992218018}}, \"EndTime\": 1572554074.946753, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554071.967868}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:34 INFO 139711786800960] #progress_metric: host=algo-4, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3503, \"sum\": 3503.0, \"min\": 3503}, \"Total Records Seen\": {\"count\": 1, \"max\": 3498257, \"sum\": 3498257.0, \"min\": 3498257}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1572554074.9469, \"Dimensions\": {\"Host\": \"algo-4\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 16}, \"StartTime\": 1572554071.968113}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:34 INFO 139711786800960] #throughput_metric: host=algo-4, train throughput=69059.8220156 records/second\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:34 INFO 139711786800960] #quality_metric: host=algo-4, epoch=17, batch=0 train rmse <loss>=1.36093332914\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:34 INFO 139711786800960] #quality_metric: host=algo-4, epoch=17, batch=0 train mse <loss>=1.85213952637\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:34 INFO 139711786800960] #quality_metric: host=algo-4, epoch=17, batch=0 train absolute_loss <loss>=0.975518188477\u001b[0m\n",
      "\u001b[33m[2019-10-31 20:34:35.262] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 34, \"duration\": 3015, \"num_examples\": 206, \"num_bytes\": 14630020}\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:35 INFO 140279785944896] #quality_metric: host=algo-3, epoch=16, train rmse <loss>=1.2012562569\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:35 INFO 140279785944896] #quality_metric: host=algo-3, epoch=16, train mse <loss>=1.44301659475\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:35 INFO 140279785944896] #quality_metric: host=algo-3, epoch=16, train absolute_loss <loss>=0.964955049681\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3017.751932144165, \"sum\": 3017.751932144165, \"min\": 3017.751932144165}}, \"EndTime\": 1572554075.26242, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554072.243703}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:35 INFO 140279785944896] #progress_metric: host=algo-3, completed 85 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3503, \"sum\": 3503.0, \"min\": 3503}, \"Total Records Seen\": {\"count\": 1, \"max\": 3498257, \"sum\": 3498257.0, \"min\": 3498257}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1572554075.262586, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 16}, \"StartTime\": 1572554072.24464}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:35 INFO 140279785944896] #throughput_metric: host=algo-3, train throughput=68163.5018569 records/second\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:35 INFO 140279785944896] #quality_metric: host=algo-3, epoch=17, batch=0 train rmse <loss>=1.28909995211\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:35 INFO 140279785944896] #quality_metric: host=algo-3, epoch=17, batch=0 train mse <loss>=1.66177868652\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:35 INFO 140279785944896] #quality_metric: host=algo-3, epoch=17, batch=0 train absolute_loss <loss>=1.17398168945\u001b[0m\n",
      "\u001b[34m[2019-10-31 20:34:37.958] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 36, \"duration\": 3009, \"num_examples\": 206, \"num_bytes\": 14622296}\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:37 INFO 139711786800960] #quality_metric: host=algo-4, epoch=17, train rmse <loss>=1.21375565677\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:37 INFO 139711786800960] #quality_metric: host=algo-4, epoch=17, train mse <loss>=1.47320279434\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:37 INFO 139711786800960] #quality_metric: host=algo-4, epoch=17, train absolute_loss <loss>=0.974182755257\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3011.7220878601074, \"sum\": 3011.7220878601074, \"min\": 3011.7220878601074}}, \"EndTime\": 1572554077.959061, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554074.946802}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:37 INFO 139711786800960] #progress_metric: host=algo-4, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3709, \"sum\": 3709.0, \"min\": 3709}, \"Total Records Seen\": {\"count\": 1, \"max\": 3703978, \"sum\": 3703978.0, \"min\": 3703978}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 19, \"sum\": 19.0, \"min\": 19}}, \"EndTime\": 1572554077.959245, \"Dimensions\": {\"Host\": \"algo-4\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 17}, \"StartTime\": 1572554074.94731}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:37 INFO 139711786800960] #throughput_metric: host=algo-4, train throughput=68300.1277088 records/second\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:37 INFO 139711786800960] #quality_metric: host=algo-4, epoch=18, batch=0 train rmse <loss>=1.31914653584\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:37 INFO 139711786800960] #quality_metric: host=algo-4, epoch=18, batch=0 train mse <loss>=1.74014758301\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:37 INFO 139711786800960] #quality_metric: host=algo-4, epoch=18, batch=0 train absolute_loss <loss>=0.967970031738\u001b[0m\n",
      "\u001b[33m[2019-10-31 20:34:38.252] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 36, \"duration\": 2987, \"num_examples\": 206, \"num_bytes\": 14630020}\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:38 INFO 140279785944896] #quality_metric: host=algo-3, epoch=17, train rmse <loss>=1.20936120418\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:38 INFO 140279785944896] #quality_metric: host=algo-3, epoch=17, train mse <loss>=1.46255452217\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:38 INFO 140279785944896] #quality_metric: host=algo-3, epoch=17, train absolute_loss <loss>=0.97240726271\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2989.2330169677734, \"sum\": 2989.2330169677734, \"min\": 2989.2330169677734}}, \"EndTime\": 1572554078.25272, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554075.262482}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:38 INFO 140279785944896] #progress_metric: host=algo-3, completed 90 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3709, \"sum\": 3709.0, \"min\": 3709}, \"Total Records Seen\": {\"count\": 1, \"max\": 3703978, \"sum\": 3703978.0, \"min\": 3703978}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 19, \"sum\": 19.0, \"min\": 19}}, \"EndTime\": 1572554078.252868, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 17}, \"StartTime\": 1572554075.26346}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:38 INFO 140279785944896] #throughput_metric: host=algo-3, train throughput=68814.5603289 records/second\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:38 INFO 140279785944896] #quality_metric: host=algo-3, epoch=18, batch=0 train rmse <loss>=1.26890888591\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:38 INFO 140279785944896] #quality_metric: host=algo-3, epoch=18, batch=0 train mse <loss>=1.61012976074\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:38 INFO 140279785944896] #quality_metric: host=algo-3, epoch=18, batch=0 train absolute_loss <loss>=1.14900671387\u001b[0m\n",
      "\u001b[31m[2019-10-31 20:34:37.960] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 24, \"duration\": 4498, \"num_examples\": 309, \"num_bytes\": 21961380}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:37 INFO 140221098768192] #quality_metric: host=algo-1, epoch=11, train rmse <loss>=1.2160011813\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:37 INFO 140221098768192] #quality_metric: host=algo-1, epoch=11, train mse <loss>=1.47865887293\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:37 INFO 140221098768192] #quality_metric: host=algo-1, epoch=11, train absolute_loss <loss>=0.977907819927\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4500.056982040405, \"sum\": 4500.056982040405, \"min\": 4500.056982040405}}, \"EndTime\": 1572554077.960859, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554073.460128}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:37 INFO 140221098768192] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3709, \"sum\": 3709.0, \"min\": 3709}, \"Total Records Seen\": {\"count\": 1, \"max\": 3703984, \"sum\": 3703984.0, \"min\": 3703984}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 13, \"sum\": 13.0, \"min\": 13}}, \"EndTime\": 1572554077.961029, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 11}, \"StartTime\": 1572554073.460774}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:37 INFO 140221098768192] #throughput_metric: host=algo-1, train throughput=68568.5500455 records/second\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:37 INFO 140221098768192] #quality_metric: host=algo-1, epoch=12, batch=0 train rmse <loss>=0.993747328269\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:37 INFO 140221098768192] #quality_metric: host=algo-1, epoch=12, batch=0 train mse <loss>=0.987533752441\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:37 INFO 140221098768192] #quality_metric: host=algo-1, epoch=12, batch=0 train absolute_loss <loss>=0.762219116211\u001b[0m\n",
      "\u001b[34m[2019-10-31 20:34:41.055] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 3094, \"num_examples\": 206, \"num_bytes\": 14622296}\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:41 INFO 139711786800960] #quality_metric: host=algo-4, epoch=18, train rmse <loss>=1.222471379\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:41 INFO 139711786800960] #quality_metric: host=algo-4, epoch=18, train mse <loss>=1.49443627248\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:41 INFO 139711786800960] #quality_metric: host=algo-4, epoch=18, train absolute_loss <loss>=0.977018568613\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3096.338987350464, \"sum\": 3096.338987350464, \"min\": 3096.338987350464}}, \"EndTime\": 1572554081.056054, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554077.959122}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:41 INFO 139711786800960] #progress_metric: host=algo-4, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3915, \"sum\": 3915.0, \"min\": 3915}, \"Total Records Seen\": {\"count\": 1, \"max\": 3909699, \"sum\": 3909699.0, \"min\": 3909699}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1572554081.056201, \"Dimensions\": {\"Host\": \"algo-4\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 18}, \"StartTime\": 1572554077.95969}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:41 INFO 139711786800960] #throughput_metric: host=algo-4, train throughput=66434.6083189 records/second\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:41 INFO 139711786800960] #quality_metric: host=algo-4, epoch=19, batch=0 train rmse <loss>=1.21439043994\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:41 INFO 139711786800960] #quality_metric: host=algo-4, epoch=19, batch=0 train mse <loss>=1.47474414062\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:41 INFO 139711786800960] #quality_metric: host=algo-4, epoch=19, batch=0 train absolute_loss <loss>=0.936404846191\u001b[0m\n",
      "\u001b[33m[2019-10-31 20:34:41.334] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 3079, \"num_examples\": 206, \"num_bytes\": 14630020}\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:41 INFO 140279785944896] #quality_metric: host=algo-3, epoch=18, train rmse <loss>=1.2012587436\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:41 INFO 140279785944896] #quality_metric: host=algo-3, epoch=18, train mse <loss>=1.44302256908\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:41 INFO 140279785944896] #quality_metric: host=algo-3, epoch=18, train absolute_loss <loss>=0.963968148241\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3081.686019897461, \"sum\": 3081.686019897461, \"min\": 3081.686019897461}}, \"EndTime\": 1572554081.33538, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554078.252775}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:41 INFO 140279785944896] #progress_metric: host=algo-3, completed 95 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3915, \"sum\": 3915.0, \"min\": 3915}, \"Total Records Seen\": {\"count\": 1, \"max\": 3909699, \"sum\": 3909699.0, \"min\": 3909699}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1572554081.335577, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 18}, \"StartTime\": 1572554078.253666}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:41 INFO 140279785944896] #throughput_metric: host=algo-3, train throughput=66748.7547262 records/second\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:41 INFO 140279785944896] #quality_metric: host=algo-3, epoch=19, batch=0 train rmse <loss>=1.13958316075\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:41 INFO 140279785944896] #quality_metric: host=algo-3, epoch=19, batch=0 train mse <loss>=1.29864978027\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:41 INFO 140279785944896] #quality_metric: host=algo-3, epoch=19, batch=0 train absolute_loss <loss>=0.952204711914\u001b[0m\n",
      "\u001b[32m[2019-10-31 20:34:42.671] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 24, \"duration\": 5202, \"num_examples\": 309, \"num_bytes\": 21931124}\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:42 INFO 140034480109376] #quality_metric: host=algo-2, epoch=11, train rmse <loss>=1.23586484558\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:42 INFO 140034480109376] #quality_metric: host=algo-2, epoch=11, train mse <loss>=1.52736191655\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:42 INFO 140034480109376] #quality_metric: host=algo-2, epoch=11, train absolute_loss <loss>=0.983585105328\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5204.675912857056, \"sum\": 5204.675912857056, \"min\": 5204.675912857056}}, \"EndTime\": 1572554082.672505, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554077.467086}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:42 INFO 140034480109376] #progress_metric: host=algo-2, completed 60 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3709, \"sum\": 3709.0, \"min\": 3709}, \"Total Records Seen\": {\"count\": 1, \"max\": 3703984, \"sum\": 3703984.0, \"min\": 3703984}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 13, \"sum\": 13.0, \"min\": 13}}, \"EndTime\": 1572554082.672681, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 11}, \"StartTime\": 1572554077.467799}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:42 INFO 140034480109376] #throughput_metric: host=algo-2, train throughput=59285.82349 records/second\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:42 INFO 140034480109376] #quality_metric: host=algo-2, epoch=12, batch=0 train rmse <loss>=1.22671147583\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:42 INFO 140034480109376] #quality_metric: host=algo-2, epoch=12, batch=0 train mse <loss>=1.50482104492\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:42 INFO 140034480109376] #quality_metric: host=algo-2, epoch=12, batch=0 train absolute_loss <loss>=0.950557067871\u001b[0m\n",
      "\u001b[31m[2019-10-31 20:34:42.613] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 4650, \"num_examples\": 309, \"num_bytes\": 21961380}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:42 INFO 140221098768192] #quality_metric: host=algo-1, epoch=12, train rmse <loss>=1.21513849993\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:42 INFO 140221098768192] #quality_metric: host=algo-1, epoch=12, train mse <loss>=1.476561574\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:42 INFO 140221098768192] #quality_metric: host=algo-1, epoch=12, train absolute_loss <loss>=0.972936733011\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4652.183055877686, \"sum\": 4652.183055877686, \"min\": 4652.183055877686}}, \"EndTime\": 1572554082.613869, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554077.960933}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:42 INFO 140221098768192] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 4018, \"sum\": 4018.0, \"min\": 4018}, \"Total Records Seen\": {\"count\": 1, \"max\": 4012566, \"sum\": 4012566.0, \"min\": 4012566}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1572554082.614042, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 12}, \"StartTime\": 1572554077.96166}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:42 INFO 140221098768192] #throughput_metric: host=algo-1, train throughput=66326.3887767 records/second\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:42 INFO 140221098768192] #quality_metric: host=algo-1, epoch=13, batch=0 train rmse <loss>=1.00990613445\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:42 INFO 140221098768192] #quality_metric: host=algo-1, epoch=13, batch=0 train mse <loss>=1.01991040039\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:42 INFO 140221098768192] #quality_metric: host=algo-1, epoch=13, batch=0 train absolute_loss <loss>=0.828595092773\u001b[0m\n",
      "\u001b[34m[2019-10-31 20:34:44.147] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 40, \"duration\": 3089, \"num_examples\": 206, \"num_bytes\": 14622296}\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:44 INFO 139711786800960] #quality_metric: host=algo-4, epoch=19, train rmse <loss>=1.22290756874\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:44 INFO 139711786800960] #quality_metric: host=algo-4, epoch=19, train mse <loss>=1.49550292169\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:44 INFO 139711786800960] #quality_metric: host=algo-4, epoch=19, train absolute_loss <loss>=0.981689191503\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:44 INFO 139711786800960] #quality_metric: host=algo-4, train rmse <loss>=1.22290756874\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:44 INFO 139711786800960] #quality_metric: host=algo-4, train mse <loss>=1.49550292169\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:44 INFO 139711786800960] #quality_metric: host=algo-4, train absolute_loss <loss>=0.981689191503\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3091.038942337036, \"sum\": 3091.038942337036, \"min\": 3091.038942337036}}, \"EndTime\": 1572554084.147769, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554081.056111}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:44 INFO 139711786800960] #progress_metric: host=algo-4, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 4121, \"sum\": 4121.0, \"min\": 4121}, \"Total Records Seen\": {\"count\": 1, \"max\": 4115420, \"sum\": 4115420.0, \"min\": 4115420}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 21, \"sum\": 21.0, \"min\": 21}}, \"EndTime\": 1572554084.147912, \"Dimensions\": {\"Host\": \"algo-4\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 19}, \"StartTime\": 1572554081.056706}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:34:44 INFO 139711786800960] #throughput_metric: host=algo-4, train throughput=66548.5930446 records/second\u001b[0m\n",
      "\u001b[33m[2019-10-31 20:34:44.459] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 40, \"duration\": 3121, \"num_examples\": 206, \"num_bytes\": 14630020}\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:44 INFO 140279785944896] #quality_metric: host=algo-3, epoch=19, train rmse <loss>=1.21103351397\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:44 INFO 140279785944896] #quality_metric: host=algo-3, epoch=19, train mse <loss>=1.46660217196\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:44 INFO 140279785944896] #quality_metric: host=algo-3, epoch=19, train absolute_loss <loss>=0.97256624981\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:44 INFO 140279785944896] #quality_metric: host=algo-3, train rmse <loss>=1.21103351397\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:44 INFO 140279785944896] #quality_metric: host=algo-3, train mse <loss>=1.46660217196\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:44 INFO 140279785944896] #quality_metric: host=algo-3, train absolute_loss <loss>=0.97256624981\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3123.6019134521484, \"sum\": 3123.6019134521484, \"min\": 3123.6019134521484}}, \"EndTime\": 1572554084.460246, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554081.335445}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:44 INFO 140279785944896] #progress_metric: host=algo-3, completed 100 % of epochs\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 206, \"sum\": 206.0, \"min\": 206}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Total Batches Seen\": {\"count\": 1, \"max\": 4121, \"sum\": 4121.0, \"min\": 4121}, \"Total Records Seen\": {\"count\": 1, \"max\": 4115420, \"sum\": 4115420.0, \"min\": 4115420}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 205721, \"sum\": 205721.0, \"min\": 205721}, \"Reset Count\": {\"count\": 1, \"max\": 21, \"sum\": 21.0, \"min\": 21}}, \"EndTime\": 1572554084.460397, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 19}, \"StartTime\": 1572554081.336619}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:34:44 INFO 140279785944896] #throughput_metric: host=algo-3, train throughput=65854.7462906 records/second\u001b[0m\n",
      "\u001b[31m[2019-10-31 20:34:46.779] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 28, \"duration\": 4163, \"num_examples\": 309, \"num_bytes\": 21961380}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:46 INFO 140221098768192] #quality_metric: host=algo-1, epoch=13, train rmse <loss>=1.20995853561\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:46 INFO 140221098768192] #quality_metric: host=algo-1, epoch=13, train mse <loss>=1.46399965789\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:46 INFO 140221098768192] #quality_metric: host=algo-1, epoch=13, train absolute_loss <loss>=0.97291264763\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4165.730953216553, \"sum\": 4165.730953216553, \"min\": 4165.730953216553}}, \"EndTime\": 1572554086.780448, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554082.61393}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:46 INFO 140221098768192] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 4327, \"sum\": 4327.0, \"min\": 4327}, \"Total Records Seen\": {\"count\": 1, \"max\": 4321148, \"sum\": 4321148.0, \"min\": 4321148}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 15, \"sum\": 15.0, \"min\": 15}}, \"EndTime\": 1572554086.780603, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 13}, \"StartTime\": 1572554082.61469}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:46 INFO 140221098768192] #throughput_metric: host=algo-1, train throughput=74071.4783039 records/second\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:46 INFO 140221098768192] #quality_metric: host=algo-1, epoch=14, batch=0 train rmse <loss>=1.03003740261\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:46 INFO 140221098768192] #quality_metric: host=algo-1, epoch=14, batch=0 train mse <loss>=1.06097705078\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:46 INFO 140221098768192] #quality_metric: host=algo-1, epoch=14, batch=0 train absolute_loss <loss>=0.854087768555\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2019-10-31 20:34:46.966] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 4292, \"num_examples\": 309, \"num_bytes\": 21931124}\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:46 INFO 140034480109376] #quality_metric: host=algo-2, epoch=12, train rmse <loss>=1.2347373394\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:46 INFO 140034480109376] #quality_metric: host=algo-2, epoch=12, train mse <loss>=1.5245762973\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:46 INFO 140034480109376] #quality_metric: host=algo-2, epoch=12, train absolute_loss <loss>=0.987198285643\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4294.019937515259, \"sum\": 4294.019937515259, \"min\": 4294.019937515259}}, \"EndTime\": 1572554086.967362, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554082.672567}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:46 INFO 140034480109376] #progress_metric: host=algo-2, completed 65 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 4018, \"sum\": 4018.0, \"min\": 4018}, \"Total Records Seen\": {\"count\": 1, \"max\": 4012566, \"sum\": 4012566.0, \"min\": 4012566}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1572554086.967515, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 12}, \"StartTime\": 1572554082.673315}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:46 INFO 140034480109376] #throughput_metric: host=algo-2, train throughput=71858.7242277 records/second\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:46 INFO 140034480109376] #quality_metric: host=algo-2, epoch=13, batch=0 train rmse <loss>=1.24200956862\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:46 INFO 140034480109376] #quality_metric: host=algo-2, epoch=13, batch=0 train mse <loss>=1.54258776855\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:46 INFO 140034480109376] #quality_metric: host=algo-2, epoch=13, batch=0 train absolute_loss <loss>=1.01774841309\u001b[0m\n",
      "\u001b[31m[2019-10-31 20:34:50.714] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 30, \"duration\": 3932, \"num_examples\": 309, \"num_bytes\": 21961380}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:50 INFO 140221098768192] #quality_metric: host=algo-1, epoch=14, train rmse <loss>=1.2041311502\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:50 INFO 140221098768192] #quality_metric: host=algo-1, epoch=14, train mse <loss>=1.44993182689\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:50 INFO 140221098768192] #quality_metric: host=algo-1, epoch=14, train absolute_loss <loss>=0.962010978032\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3934.0782165527344, \"sum\": 3934.0782165527344, \"min\": 3934.0782165527344}}, \"EndTime\": 1572554090.715376, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554086.780509}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:50 INFO 140221098768192] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 4636, \"sum\": 4636.0, \"min\": 4636}, \"Total Records Seen\": {\"count\": 1, \"max\": 4629730, \"sum\": 4629730.0, \"min\": 4629730}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1572554090.715528, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 14}, \"StartTime\": 1572554086.78127}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:50 INFO 140221098768192] #throughput_metric: host=algo-1, train throughput=78432.2406132 records/second\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:50 INFO 140221098768192] #quality_metric: host=algo-1, epoch=15, batch=0 train rmse <loss>=1.07530246472\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:50 INFO 140221098768192] #quality_metric: host=algo-1, epoch=15, batch=0 train mse <loss>=1.15627539063\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:50 INFO 140221098768192] #quality_metric: host=algo-1, epoch=15, batch=0 train absolute_loss <loss>=0.900565490723\u001b[0m\n",
      "\u001b[32m[2019-10-31 20:34:51.100] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 28, \"duration\": 4131, \"num_examples\": 309, \"num_bytes\": 21931124}\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:51 INFO 140034480109376] #quality_metric: host=algo-2, epoch=13, train rmse <loss>=1.22441915073\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:51 INFO 140034480109376] #quality_metric: host=algo-2, epoch=13, train mse <loss>=1.49920225668\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:51 INFO 140034480109376] #quality_metric: host=algo-2, epoch=13, train absolute_loss <loss>=0.969960199152\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4133.03804397583, \"sum\": 4133.03804397583, \"min\": 4133.03804397583}}, \"EndTime\": 1572554091.101218, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554086.967423}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:51 INFO 140034480109376] #progress_metric: host=algo-2, completed 70 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 4327, \"sum\": 4327.0, \"min\": 4327}, \"Total Records Seen\": {\"count\": 1, \"max\": 4321148, \"sum\": 4321148.0, \"min\": 4321148}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 15, \"sum\": 15.0, \"min\": 15}}, \"EndTime\": 1572554091.101382, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 13}, \"StartTime\": 1572554086.968152}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:51 INFO 140034480109376] #throughput_metric: host=algo-2, train throughput=74657.1401798 records/second\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:51 INFO 140034480109376] #quality_metric: host=algo-2, epoch=14, batch=0 train rmse <loss>=1.22435794119\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:51 INFO 140034480109376] #quality_metric: host=algo-2, epoch=14, batch=0 train mse <loss>=1.49905236816\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:51 INFO 140034480109376] #quality_metric: host=algo-2, epoch=14, batch=0 train absolute_loss <loss>=0.946781311035\u001b[0m\n",
      "\u001b[31m[2019-10-31 20:34:54.702] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 3985, \"num_examples\": 309, \"num_bytes\": 21961380}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:54 INFO 140221098768192] #quality_metric: host=algo-1, epoch=15, train rmse <loss>=1.20563100158\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:54 INFO 140221098768192] #quality_metric: host=algo-1, epoch=15, train mse <loss>=1.45354611196\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:54 INFO 140221098768192] #quality_metric: host=algo-1, epoch=15, train absolute_loss <loss>=0.965810378979\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3986.7799282073975, \"sum\": 3986.7799282073975, \"min\": 3986.7799282073975}}, \"EndTime\": 1572554094.702562, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554090.715437}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:54 INFO 140221098768192] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 4945, \"sum\": 4945.0, \"min\": 4945}, \"Total Records Seen\": {\"count\": 1, \"max\": 4938312, \"sum\": 4938312.0, \"min\": 4938312}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 17, \"sum\": 17.0, \"min\": 17}}, \"EndTime\": 1572554094.702708, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 15}, \"StartTime\": 1572554090.715756}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:54 INFO 140221098768192] #throughput_metric: host=algo-1, train throughput=77395.9761171 records/second\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:54 INFO 140221098768192] #quality_metric: host=algo-1, epoch=16, batch=0 train rmse <loss>=1.04179791491\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:54 INFO 140221098768192] #quality_metric: host=algo-1, epoch=16, batch=0 train mse <loss>=1.08534289551\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:54 INFO 140221098768192] #quality_metric: host=algo-1, epoch=16, batch=0 train absolute_loss <loss>=0.865877380371\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2019-10-31 20:34:55.193] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 30, \"duration\": 4090, \"num_examples\": 309, \"num_bytes\": 21931124}\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:55 INFO 140034480109376] #quality_metric: host=algo-2, epoch=14, train rmse <loss>=1.22541849924\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:55 INFO 140034480109376] #quality_metric: host=algo-2, epoch=14, train mse <loss>=1.50165049828\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:55 INFO 140034480109376] #quality_metric: host=algo-2, epoch=14, train absolute_loss <loss>=0.975165844569\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4091.8970108032227, \"sum\": 4091.8970108032227, \"min\": 4091.8970108032227}}, \"EndTime\": 1572554095.193993, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554091.101279}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:55 INFO 140034480109376] #progress_metric: host=algo-2, completed 75 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 4636, \"sum\": 4636.0, \"min\": 4636}, \"Total Records Seen\": {\"count\": 1, \"max\": 4629730, \"sum\": 4629730.0, \"min\": 4629730}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1572554095.19414, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 14}, \"StartTime\": 1572554091.102069}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:55 INFO 140034480109376] #throughput_metric: host=algo-2, train throughput=75408.0229954 records/second\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:55 INFO 140034480109376] #quality_metric: host=algo-2, epoch=15, batch=0 train rmse <loss>=1.22417392826\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:55 INFO 140034480109376] #quality_metric: host=algo-2, epoch=15, batch=0 train mse <loss>=1.49860180664\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:55 INFO 140034480109376] #quality_metric: host=algo-2, epoch=15, batch=0 train absolute_loss <loss>=0.944009094238\u001b[0m\n",
      "\u001b[31m[2019-10-31 20:34:58.681] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 34, \"duration\": 3977, \"num_examples\": 309, \"num_bytes\": 21961380}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:58 INFO 140221098768192] #quality_metric: host=algo-1, epoch=16, train rmse <loss>=1.20350441521\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:58 INFO 140221098768192] #quality_metric: host=algo-1, epoch=16, train mse <loss>=1.44842287744\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:58 INFO 140221098768192] #quality_metric: host=algo-1, epoch=16, train absolute_loss <loss>=0.964242534156\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3979.3128967285156, \"sum\": 3979.3128967285156, \"min\": 3979.3128967285156}}, \"EndTime\": 1572554098.682204, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554094.702617}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:58 INFO 140221098768192] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 5254, \"sum\": 5254.0, \"min\": 5254}, \"Total Records Seen\": {\"count\": 1, \"max\": 5246894, \"sum\": 5246894.0, \"min\": 5246894}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1572554098.682355, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 16}, \"StartTime\": 1572554094.702866}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:58 INFO 140221098768192] #throughput_metric: host=algo-1, train throughput=77541.5219208 records/second\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:58 INFO 140221098768192] #quality_metric: host=algo-1, epoch=17, batch=0 train rmse <loss>=1.08004759507\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:58 INFO 140221098768192] #quality_metric: host=algo-1, epoch=17, batch=0 train mse <loss>=1.16650280762\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:34:58 INFO 140221098768192] #quality_metric: host=algo-1, epoch=17, batch=0 train absolute_loss <loss>=0.909172058105\u001b[0m\n",
      "\u001b[32m[2019-10-31 20:34:59.340] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 4145, \"num_examples\": 309, \"num_bytes\": 21931124}\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:59 INFO 140034480109376] #quality_metric: host=algo-2, epoch=15, train rmse <loss>=1.2229669663\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:59 INFO 140034480109376] #quality_metric: host=algo-2, epoch=15, train mse <loss>=1.49564820067\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:59 INFO 140034480109376] #quality_metric: host=algo-2, epoch=15, train absolute_loss <loss>=0.972721811832\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4147.098064422607, \"sum\": 4147.098064422607, \"min\": 4147.098064422607}}, \"EndTime\": 1572554099.341424, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554095.194049}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:59 INFO 140034480109376] #progress_metric: host=algo-2, completed 80 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 4945, \"sum\": 4945.0, \"min\": 4945}, \"Total Records Seen\": {\"count\": 1, \"max\": 4938312, \"sum\": 4938312.0, \"min\": 4938312}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 17, \"sum\": 17.0, \"min\": 17}}, \"EndTime\": 1572554099.341572, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 15}, \"StartTime\": 1572554095.1943}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:59 INFO 140034480109376] #throughput_metric: host=algo-2, train throughput=74404.5329645 records/second\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:59 INFO 140034480109376] #quality_metric: host=algo-2, epoch=16, batch=0 train rmse <loss>=1.22363718637\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:59 INFO 140034480109376] #quality_metric: host=algo-2, epoch=16, batch=0 train mse <loss>=1.49728796387\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:34:59 INFO 140034480109376] #quality_metric: host=algo-2, epoch=16, batch=0 train absolute_loss <loss>=0.945905517578\u001b[0m\n",
      "\u001b[31m[2019-10-31 20:35:02.716] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 36, \"duration\": 4032, \"num_examples\": 309, \"num_bytes\": 21961380}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:02 INFO 140221098768192] #quality_metric: host=algo-1, epoch=17, train rmse <loss>=1.20341699702\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:02 INFO 140221098768192] #quality_metric: host=algo-1, epoch=17, train mse <loss>=1.44821246871\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:02 INFO 140221098768192] #quality_metric: host=algo-1, epoch=17, train absolute_loss <loss>=0.962960142858\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4034.260034561157, \"sum\": 4034.260034561157, \"min\": 4034.260034561157}}, \"EndTime\": 1572554102.717222, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554098.682263}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:02 INFO 140221098768192] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 5563, \"sum\": 5563.0, \"min\": 5563}, \"Total Records Seen\": {\"count\": 1, \"max\": 5555476, \"sum\": 5555476.0, \"min\": 5555476}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 19, \"sum\": 19.0, \"min\": 19}}, \"EndTime\": 1572554102.717367, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 17}, \"StartTime\": 1572554098.682933}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:02 INFO 140221098768192] #throughput_metric: host=algo-1, train throughput=76485.3917447 records/second\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:02 INFO 140221098768192] #quality_metric: host=algo-1, epoch=18, batch=0 train rmse <loss>=1.06677426113\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:02 INFO 140221098768192] #quality_metric: host=algo-1, epoch=18, batch=0 train mse <loss>=1.13800732422\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:02 INFO 140221098768192] #quality_metric: host=algo-1, epoch=18, batch=0 train absolute_loss <loss>=0.887187133789\u001b[0m\n",
      "\u001b[32m[2019-10-31 20:35:04.203] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 34, \"duration\": 4860, \"num_examples\": 309, \"num_bytes\": 21931124}\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:04 INFO 140034480109376] #quality_metric: host=algo-2, epoch=16, train rmse <loss>=1.22458833241\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:04 INFO 140034480109376] #quality_metric: host=algo-2, epoch=16, train mse <loss>=1.49961658386\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:04 INFO 140034480109376] #quality_metric: host=algo-2, epoch=16, train absolute_loss <loss>=0.976406444562\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4862.66303062439, \"sum\": 4862.66303062439, \"min\": 4862.66303062439}}, \"EndTime\": 1572554104.20443, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554099.34148}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:04 INFO 140034480109376] #progress_metric: host=algo-2, completed 85 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 5254, \"sum\": 5254.0, \"min\": 5254}, \"Total Records Seen\": {\"count\": 1, \"max\": 5246894, \"sum\": 5246894.0, \"min\": 5246894}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1572554104.204596, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 16}, \"StartTime\": 1572554099.341732}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:04 INFO 140034480109376] #throughput_metric: host=algo-2, train throughput=63455.7855242 records/second\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:04 INFO 140034480109376] #quality_metric: host=algo-2, epoch=17, batch=0 train rmse <loss>=1.22351133238\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:04 INFO 140034480109376] #quality_metric: host=algo-2, epoch=17, batch=0 train mse <loss>=1.49697998047\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:04 INFO 140034480109376] #quality_metric: host=algo-2, epoch=17, batch=0 train absolute_loss <loss>=0.945119995117\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[2019-10-31 20:35:06.692] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 3973, \"num_examples\": 309, \"num_bytes\": 21961380}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:06 INFO 140221098768192] #quality_metric: host=algo-1, epoch=18, train rmse <loss>=1.20740140744\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:06 INFO 140221098768192] #quality_metric: host=algo-1, epoch=18, train mse <loss>=1.45781815869\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:06 INFO 140221098768192] #quality_metric: host=algo-1, epoch=18, train absolute_loss <loss>=0.962839274286\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3975.3808975219727, \"sum\": 3975.3808975219727, \"min\": 3975.3808975219727}}, \"EndTime\": 1572554106.692934, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554102.717277}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:06 INFO 140221098768192] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 5872, \"sum\": 5872.0, \"min\": 5872}, \"Total Records Seen\": {\"count\": 1, \"max\": 5864058, \"sum\": 5864058.0, \"min\": 5864058}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1572554106.693138, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 18}, \"StartTime\": 1572554102.717524}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:06 INFO 140221098768192] #throughput_metric: host=algo-1, train throughput=77616.9648288 records/second\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:06 INFO 140221098768192] #quality_metric: host=algo-1, epoch=19, batch=0 train rmse <loss>=1.07309429785\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:06 INFO 140221098768192] #quality_metric: host=algo-1, epoch=19, batch=0 train mse <loss>=1.15153137207\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:06 INFO 140221098768192] #quality_metric: host=algo-1, epoch=19, batch=0 train absolute_loss <loss>=0.896485839844\u001b[0m\n",
      "\u001b[32m[2019-10-31 20:35:08.939] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 36, \"duration\": 4733, \"num_examples\": 309, \"num_bytes\": 21931124}\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:08 INFO 140034480109376] #quality_metric: host=algo-2, epoch=17, train rmse <loss>=1.22324787661\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:08 INFO 140034480109376] #quality_metric: host=algo-2, epoch=17, train mse <loss>=1.49633536764\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:08 INFO 140034480109376] #quality_metric: host=algo-2, epoch=17, train absolute_loss <loss>=0.971530321041\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4735.075950622559, \"sum\": 4735.075950622559, \"min\": 4735.075950622559}}, \"EndTime\": 1572554108.940262, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554104.204502}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:08 INFO 140034480109376] #progress_metric: host=algo-2, completed 90 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 5563, \"sum\": 5563.0, \"min\": 5563}, \"Total Records Seen\": {\"count\": 1, \"max\": 5555476, \"sum\": 5555476.0, \"min\": 5555476}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 19, \"sum\": 19.0, \"min\": 19}}, \"EndTime\": 1572554108.940443, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 17}, \"StartTime\": 1572554104.205156}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:08 INFO 140034480109376] #throughput_metric: host=algo-2, train throughput=65165.2303666 records/second\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:08 INFO 140034480109376] #quality_metric: host=algo-2, epoch=18, batch=0 train rmse <loss>=1.26095410266\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:08 INFO 140034480109376] #quality_metric: host=algo-2, epoch=18, batch=0 train mse <loss>=1.59000524902\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:08 INFO 140034480109376] #quality_metric: host=algo-2, epoch=18, batch=0 train absolute_loss <loss>=1.06692993164\u001b[0m\n",
      "\u001b[31m[2019-10-31 20:35:10.784] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 40, \"duration\": 4089, \"num_examples\": 309, \"num_bytes\": 21961380}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:10 INFO 140221098768192] #quality_metric: host=algo-1, epoch=19, train rmse <loss>=1.20223505242\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:10 INFO 140221098768192] #quality_metric: host=algo-1, epoch=19, train mse <loss>=1.44536912127\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:10 INFO 140221098768192] #quality_metric: host=algo-1, epoch=19, train absolute_loss <loss>=0.962499417697\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:10 INFO 140221098768192] #quality_metric: host=algo-1, train rmse <loss>=1.20223505242\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:10 INFO 140221098768192] #quality_metric: host=algo-1, train mse <loss>=1.44536912127\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:10 INFO 140221098768192] #quality_metric: host=algo-1, train absolute_loss <loss>=0.962499417697\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4091.3920402526855, \"sum\": 4091.3920402526855, \"min\": 4091.3920402526855}}, \"EndTime\": 1572554110.784759, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554106.693}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:10 INFO 140221098768192] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 6181, \"sum\": 6181.0, \"min\": 6181}, \"Total Records Seen\": {\"count\": 1, \"max\": 6172640, \"sum\": 6172640.0, \"min\": 6172640}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 21, \"sum\": 21.0, \"min\": 21}}, \"EndTime\": 1572554110.784902, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 19}, \"StartTime\": 1572554106.693343}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:10 INFO 140221098768192] #throughput_metric: host=algo-1, train throughput=75417.4964402 records/second\u001b[0m\n",
      "\u001b[32m[2019-10-31 20:35:13.069] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 4127, \"num_examples\": 309, \"num_bytes\": 21931124}\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:13 INFO 140034480109376] #quality_metric: host=algo-2, epoch=18, train rmse <loss>=1.22236129099\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:13 INFO 140034480109376] #quality_metric: host=algo-2, epoch=18, train mse <loss>=1.49416712571\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:13 INFO 140034480109376] #quality_metric: host=algo-2, epoch=18, train absolute_loss <loss>=0.974877598438\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4129.69708442688, \"sum\": 4129.69708442688, \"min\": 4129.69708442688}}, \"EndTime\": 1572554113.070353, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554108.940314}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:13 INFO 140034480109376] #progress_metric: host=algo-2, completed 95 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 5872, \"sum\": 5872.0, \"min\": 5872}, \"Total Records Seen\": {\"count\": 1, \"max\": 5864058, \"sum\": 5864058.0, \"min\": 5864058}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1572554113.070528, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 18}, \"StartTime\": 1572554108.940626}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:13 INFO 140034480109376] #throughput_metric: host=algo-2, train throughput=74717.2797925 records/second\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:13 INFO 140034480109376] #quality_metric: host=algo-2, epoch=19, batch=0 train rmse <loss>=1.23284032367\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:13 INFO 140034480109376] #quality_metric: host=algo-2, epoch=19, batch=0 train mse <loss>=1.51989526367\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:13 INFO 140034480109376] #quality_metric: host=algo-2, epoch=19, batch=0 train absolute_loss <loss>=0.984313293457\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/31/2019 20:35:17 INFO 140221098768192] Pulling entire model from kvstore to finalize\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 25.786876678466797, \"sum\": 25.786876678466797, \"min\": 25.786876678466797}}, \"EndTime\": 1572554117.407578, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554110.784816}\n",
      "\u001b[0m\n",
      "\u001b[32m[2019-10-31 20:35:17.379] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 40, \"duration\": 4306, \"num_examples\": 309, \"num_bytes\": 21931124}\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:17 INFO 140034480109376] #quality_metric: host=algo-2, epoch=19, train rmse <loss>=1.22600470515\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:17 INFO 140034480109376] #quality_metric: host=algo-2, epoch=19, train mse <loss>=1.50308753706\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:17 INFO 140034480109376] #quality_metric: host=algo-2, epoch=19, train absolute_loss <loss>=0.980596392289\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:17 INFO 140034480109376] #quality_metric: host=algo-2, train rmse <loss>=1.22600470515\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:17 INFO 140034480109376] #quality_metric: host=algo-2, train mse <loss>=1.50308753706\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:17 INFO 140034480109376] #quality_metric: host=algo-2, train absolute_loss <loss>=0.980596392289\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4309.010028839111, \"sum\": 4309.010028839111, \"min\": 4309.010028839111}}, \"EndTime\": 1572554117.379739, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554113.07041}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:17 INFO 140034480109376] #progress_metric: host=algo-2, completed 100 % of epochs\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 309, \"sum\": 309.0, \"min\": 309}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Total Batches Seen\": {\"count\": 1, \"max\": 6181, \"sum\": 6181.0, \"min\": 6181}, \"Total Records Seen\": {\"count\": 1, \"max\": 6172640, \"sum\": 6172640.0, \"min\": 6172640}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 308582, \"sum\": 308582.0, \"min\": 308582}, \"Reset Count\": {\"count\": 1, \"max\": 21, \"sum\": 21.0, \"min\": 21}}, \"EndTime\": 1572554117.37989, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 19}, \"StartTime\": 1572554113.070707}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:17 INFO 140034480109376] #throughput_metric: host=algo-2, train throughput=71608.9612056 records/second\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:17 INFO 140034480109376] Pulling entire model from kvstore to finalize\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 22.881031036376953, \"sum\": 22.881031036376953, \"min\": 22.881031036376953}}, \"EndTime\": 1572554117.403255, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554117.379804}\n",
      "\u001b[0m\n",
      "\u001b[32m[2019-10-31 20:35:17.404] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 91238, \"num_examples\": 1, \"num_bytes\": 70668}\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:35:17 INFO 139711786800960] Pulling entire model from kvstore to finalize\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 27.5881290435791, \"sum\": 27.5881290435791, \"min\": 27.5881290435791}}, \"EndTime\": 1572554117.409656, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554084.147825}\n",
      "\u001b[0m\n",
      "\u001b[34m[2019-10-31 20:35:17.410] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 91476, \"num_examples\": 1, \"num_bytes\": 70668}\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:35:17 INFO 140279785944896] Pulling entire model from kvstore to finalize\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 25.56610107421875, \"sum\": 25.56610107421875, \"min\": 25.56610107421875}}, \"EndTime\": 1572554117.408836, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554084.460302}\n",
      "\u001b[0m\n",
      "\u001b[33m[2019-10-31 20:35:17.409] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 91243, \"num_examples\": 1, \"num_bytes\": 70668}\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:18 INFO 140221098768192] Saved checkpoint to \"/tmp/tmpj8uDJX/state-0001.params\"\u001b[0m\n",
      "\u001b[31m[2019-10-31 20:35:18.625] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 92544, \"num_examples\": 1, \"num_bytes\": 70668}\u001b[0m\n",
      "\n",
      "2019-10-31 20:35:32 Uploading - Uploading generated training model\u001b[32m[2019-10-31 20:35:28.450] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 11042, \"num_examples\": 141, \"num_bytes\": 9937804}\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 141, \"sum\": 141.0, \"min\": 141}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 141, \"sum\": 141.0, \"min\": 141}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 140344, \"sum\": 140344.0, \"min\": 140344}, \"Total Batches Seen\": {\"count\": 1, \"max\": 141, \"sum\": 141.0, \"min\": 141}, \"Total Records Seen\": {\"count\": 1, \"max\": 140344, \"sum\": 140344.0, \"min\": 140344}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 140344, \"sum\": 140344.0, \"min\": 140344}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1572554128.450116, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"test_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554117.403534}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:28 INFO 140034480109376] #test_score (algo-2) : ('rmse', 551.1751287686211)\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:28 INFO 140034480109376] #test_score (algo-2) : ('mse', 303794.0225731061)\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:28 INFO 140034480109376] #test_score (algo-2) : ('absolute_loss', 336.23358848392166)\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:28 INFO 140034480109376] #quality_metric: host=algo-2, test rmse <loss>=551.175128769\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:28 INFO 140034480109376] #quality_metric: host=algo-2, test mse <loss>=303794.022573\u001b[0m\n",
      "\u001b[32m[10/31/2019 20:35:28 INFO 140034480109376] #quality_metric: host=algo-2, test absolute_loss <loss>=336.233588484\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 103852.7238368988, \"sum\": 103852.7238368988, \"min\": 103852.7238368988}, \"setuptime\": {\"count\": 1, \"max\": 1558.3198070526123, \"sum\": 1558.3198070526123, \"min\": 1558.3198070526123}}, \"EndTime\": 1572554128.451167, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554117.403321}\n",
      "\u001b[0m\n",
      "\u001b[34m[2019-10-31 20:35:28.509] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 11095, \"num_examples\": 141, \"num_bytes\": 9937804}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 141, \"sum\": 141.0, \"min\": 141}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 141, \"sum\": 141.0, \"min\": 141}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 140344, \"sum\": 140344.0, \"min\": 140344}, \"Total Batches Seen\": {\"count\": 1, \"max\": 141, \"sum\": 141.0, \"min\": 141}, \"Total Records Seen\": {\"count\": 1, \"max\": 140344, \"sum\": 140344.0, \"min\": 140344}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 140344, \"sum\": 140344.0, \"min\": 140344}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1572554128.5094, \"Dimensions\": {\"Host\": \"algo-4\", \"Meta\": \"test_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554117.409976}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:35:28 INFO 139711786800960] #test_score (algo-4) : ('rmse', 551.1751287686211)\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:35:28 INFO 139711786800960] #test_score (algo-4) : ('mse', 303794.0225731061)\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:35:28 INFO 139711786800960] #test_score (algo-4) : ('absolute_loss', 336.23358848392166)\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:35:28 INFO 139711786800960] #quality_metric: host=algo-4, test rmse <loss>=551.175128769\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:35:28 INFO 139711786800960] #quality_metric: host=algo-4, test mse <loss>=303794.022573\u001b[0m\n",
      "\u001b[34m[10/31/2019 20:35:28 INFO 139711786800960] #quality_metric: host=algo-4, test absolute_loss <loss>=336.233588484\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 102631.01005554199, \"sum\": 102631.01005554199, \"min\": 102631.01005554199}, \"setuptime\": {\"count\": 1, \"max\": 44.016122817993164, \"sum\": 44.016122817993164, \"min\": 44.016122817993164}}, \"EndTime\": 1572554128.510424, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554117.409719}\n",
      "\u001b[0m\n",
      "\u001b[33m[2019-10-31 20:35:28.489] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 11076, \"num_examples\": 141, \"num_bytes\": 9937804}\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 141, \"sum\": 141.0, \"min\": 141}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 141, \"sum\": 141.0, \"min\": 141}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 140344, \"sum\": 140344.0, \"min\": 140344}, \"Total Batches Seen\": {\"count\": 1, \"max\": 141, \"sum\": 141.0, \"min\": 141}, \"Total Records Seen\": {\"count\": 1, \"max\": 140344, \"sum\": 140344.0, \"min\": 140344}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 140344, \"sum\": 140344.0, \"min\": 140344}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1572554128.489542, \"Dimensions\": {\"Host\": \"algo-3\", \"Meta\": \"test_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554117.409102}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:35:28 INFO 140279785944896] #test_score (algo-3) : ('rmse', 551.1751287686211)\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:35:28 INFO 140279785944896] #test_score (algo-3) : ('mse', 303794.0225731061)\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:35:28 INFO 140279785944896] #test_score (algo-3) : ('absolute_loss', 336.23358848392166)\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:35:28 INFO 140279785944896] #quality_metric: host=algo-3, test rmse <loss>=551.175128769\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:35:28 INFO 140279785944896] #quality_metric: host=algo-3, test mse <loss>=303794.022573\u001b[0m\n",
      "\u001b[33m[10/31/2019 20:35:28 INFO 140279785944896] #quality_metric: host=algo-3, test absolute_loss <loss>=336.233588484\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 104927.76298522949, \"sum\": 104927.76298522949, \"min\": 104927.76298522949}, \"setuptime\": {\"count\": 1, \"max\": 2591.4969444274902, \"sum\": 2591.4969444274902, \"min\": 2591.4969444274902}}, \"EndTime\": 1572554128.490596, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554117.408896}\n",
      "\u001b[0m\n",
      "\u001b[31m[2019-10-31 20:35:29.330] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 10704, \"num_examples\": 141, \"num_bytes\": 9937804}\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 141, \"sum\": 141.0, \"min\": 141}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 141, \"sum\": 141.0, \"min\": 141}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 140344, \"sum\": 140344.0, \"min\": 140344}, \"Total Batches Seen\": {\"count\": 1, \"max\": 141, \"sum\": 141.0, \"min\": 141}, \"Total Records Seen\": {\"count\": 1, \"max\": 140344, \"sum\": 140344.0, \"min\": 140344}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 140344, \"sum\": 140344.0, \"min\": 140344}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1572554129.330405, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"test_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554118.625223}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:29 INFO 140221098768192] #test_score (algo-1) : ('rmse', 551.1751287686211)\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:29 INFO 140221098768192] #test_score (algo-1) : ('mse', 303794.0225731061)\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:29 INFO 140221098768192] #test_score (algo-1) : ('absolute_loss', 336.23358848392166)\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:29 INFO 140221098768192] #quality_metric: host=algo-1, test rmse <loss>=551.175128769\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:29 INFO 140221098768192] #quality_metric: host=algo-1, test mse <loss>=303794.022573\u001b[0m\n",
      "\u001b[31m[10/31/2019 20:35:29 INFO 140221098768192] #quality_metric: host=algo-1, test absolute_loss <loss>=336.233588484\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 104820.14608383179, \"sum\": 104820.14608383179, \"min\": 104820.14608383179}, \"setuptime\": {\"count\": 1, \"max\": 1559.4050884246826, \"sum\": 1559.4050884246826, \"min\": 1559.4050884246826}}, \"EndTime\": 1572554129.331591, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1572554117.407643}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-10-31 20:35:56 Completed - Training job completed\n",
      "Training seconds: 616\n",
      "Billable seconds: 616\n"
     ]
    }
   ],
   "source": [
    "fm = sagemaker.estimator.Estimator(\n",
    "    sagemaker.amazon.amazon_estimator.get_image_uri(boto3.Session().region_name, 'factorization-machines', 'latest'),\n",
    "    role, \n",
    "    train_instance_count=4, \n",
    "    train_instance_type='ml.c5.2xlarge',\n",
    "    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "    base_job_name=base,\n",
    "    sagemaker_session=sess)\n",
    "\n",
    "fm.set_hyperparameters(\n",
    "    feature_dim=customer_index.shape[0] + product_index.shape[0] + 1,\n",
    "    predictor_type='regressor',\n",
    "    mini_batch_size=1000,\n",
    "    num_factors=256,\n",
    "    epochs=20)\n",
    "\n",
    "fm.fit({'train': sagemaker.s3_input('s3://{}/{}/train/'.format(bucket, prefix), distribution='ShardedByS3Key'), \n",
    "        'test': sagemaker.s3_input('s3://{}/{}/test/'.format(bucket, prefix), distribution='FullyReplicated')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Host\n",
    "\n",
    "Deploy trained model to a real-time production endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: DEMO-loft-recommender-2019-10-31-20-31-08-794\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateEndpoint operation: Cannot create already existing endpoint \"arn:aws:sagemaker:us-west-2:601091450883:endpoint/demo-loft-recommender-2019-10-31-20-31-08-794\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-5fbe3412c72d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfm_predictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ml.m4.xlarge'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_instance_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, use_compiled_model, update_endpoint, wait, model_name, kms_key, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m             \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         )\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, update_endpoint, tags, kms_key, wait)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             self.sagemaker_session.endpoint_from_production_variants(\n\u001b[0;32m--> 470\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mproduction_variant\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m             )\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mendpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait)\u001b[0m\n\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand_role\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mcreate_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m         self.sagemaker_client.create_endpoint(\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mEndpointName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEndpointConfigName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m         )\n\u001b[1;32m    981\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateEndpoint operation: Cannot create already existing endpoint \"arn:aws:sagemaker:us-west-2:601091450883:endpoint/demo-loft-recommender-2019-10-31-20-31-08-794\"."
     ]
    }
   ],
   "source": [
    "fm_predictor = fm.deploy(instance_type='ml.m4.xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup predictor to serialize in-memory data for invocation requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fm_serializer(df):\n",
    "    feature_dim = customer_index.shape[0] + product_index.shape[0] + 1\n",
    "    js = {'instances': []}\n",
    "    for index, data in df.iterrows():\n",
    "        js['instances'].append({'data': {'features': {'values': [1, 1, data['days_since_first']],\n",
    "                                                      'keys': [data['user'], data['item'], feature_dim - 1],\n",
    "                                                      'shape': [feature_dim]}}})\n",
    "    return json.dumps(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor.content_type = 'application/json'\n",
    "fm_predictor.serializer = fm_serializer\n",
    "fm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-time prediction for a single user item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_title</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_date</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>first_review_date</th>\n",
       "      <th>days_since_first</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10378</td>\n",
       "      <td>B002PZDM9Y</td>\n",
       "      <td>Ghosts of Girlfriends Past</td>\n",
       "      <td>4</td>\n",
       "      <td>2013-01-14</td>\n",
       "      <td>8644</td>\n",
       "      <td>172646</td>\n",
       "      <td>2012-12-28</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  product_id               product_title  star_rating  \\\n",
       "0        10378  B002PZDM9Y  Ghosts of Girlfriends Past            4   \n",
       "\n",
       "  review_date  user    item first_review_date  days_since_first  \n",
       "0  2013-01-14  8644  172646        2012-12-28              17.0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [{'score': -24.361328125}]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_predictor.predict(test_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean-up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "# Extra credit\n",
    "\n",
    "- What happens when a new movie is added?\n",
    "  - No feature to set to \"1\" in the dataset\n",
    "  - No previous ratings to find similar items\n",
    "  - Cold start problem is hard with factorization machines\n",
    "- Word2vec\n",
    "  - Word embeddings for natural language processing (similar words get similar vectors)\n",
    "  - Use concatenated product titles as words customer review history as sentences\n",
    "  - SageMaker BlazingText is an extremely fast implementation that can work with subwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data\n",
    "\n",
    "Concatenate product titles to treat each one as a single word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df['product_title'] = reduced_df['product_title'].apply(lambda x: x.lower().replace(' ', '-'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write customer purchase histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = True\n",
    "with open('customer_purchases.txt', 'w') as f:\n",
    "    for customer, data in reduced_df.sort_values(['customer_id', 'review_date']).groupby('customer_id'):\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            f.write('\\n')\n",
    "        f.write(' '.join(data['product_title'].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to S3 so SageMaker training can use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sess.upload_data('customer_purchases.txt', bucket, '{}/word2vec/train'.format(prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train\n",
    "\n",
    "Create a SageMaker estimator:\n",
    "- Specify training job arguments\n",
    "- Set hyperparameters\n",
    "  - Remove titles that occur less than 5 times\n",
    "  - Embed in a 100-dimensional subspace\n",
    "  - Use subwords to capture similarity in titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-01 16:46:39 Starting - Starting the training job...\n",
      "2019-11-01 16:46:41 Starting - Launching requested ML instances...\n",
      "2019-11-01 16:47:38 Starting - Preparing the instances for training......\n",
      "2019-11-01 16:48:30 Downloading - Downloading input data...\n",
      "2019-11-01 16:48:54 Training - Training image download completed. Training in progress.\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[31m[11/01/2019 16:48:55 WARNING 140163234461504] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31m[11/01/2019 16:48:55 WARNING 140163234461504] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31m[11/01/2019 16:48:55 INFO 140163234461504] nvidia-smi took: 0.0503108501434 secs to identify 1 gpus\u001b[0m\n",
      "\u001b[31m[11/01/2019 16:48:55 INFO 140163234461504] Running BlazingText on singe GPU using skipgram\u001b[0m\n",
      "\u001b[31m[11/01/2019 16:48:55 INFO 140163234461504] Processing /opt/ml/input/data/train/customer_purchases.txt . File size: 23 MB\u001b[0m\n",
      "\u001b[31mRead 1M words\u001b[0m\n",
      "\u001b[31mNumber of words:  17990\u001b[0m\n",
      "\u001b[31mInitialized GPU 0 successfully! Now starting training....\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0485  Progress: 3.06%  Million Words/sec: 0.67 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0456  Progress: 8.78%  Million Words/sec: 1.15 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0428  Progress: 14.30%  Million Words/sec: 1.34 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0399  Progress: 20.20%  Million Words/sec: 1.47 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0372  Progress: 25.67%  Million Words/sec: 1.53 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0343  Progress: 31.48%  Million Words/sec: 1.58 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0314  Progress: 37.21%  Million Words/sec: 1.62 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0286  Progress: 42.86%  Million Words/sec: 1.65 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0257  Progress: 48.56%  Million Words/sec: 1.67 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0229  Progress: 54.20%  Million Words/sec: 1.69 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0200  Progress: 60.02%  Million Words/sec: 1.71 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0172  Progress: 65.60%  Million Words/sec: 1.71 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0143  Progress: 71.37%  Million Words/sec: 1.73 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0116  Progress: 76.76%  Million Words/sec: 1.73 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0087  Progress: 82.64%  Million Words/sec: 1.74 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0058  Progress: 88.31%  Million Words/sec: 1.75 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0031  Progress: 93.83%  Million Words/sec: 1.75 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0002  Progress: 99.68%  Million Words/sec: 1.76 #####\u001b[0m\n",
      "\u001b[31mExiting thread 3\u001b[0m\n",
      "\u001b[31mExiting thread 7\u001b[0m\n",
      "\u001b[31mExiting thread 0\u001b[0m\n",
      "\u001b[31mExiting thread 1\u001b[0m\n",
      "\u001b[31mExiting thread 5\u001b[0m\n",
      "\u001b[31mExiting thread 6\u001b[0m\n",
      "\u001b[31mExiting thread 2\u001b[0m\n",
      "\u001b[31mExiting thread 4\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0000  Progress: 100.00%  Million Words/sec: 1.74 #####\u001b[0m\n",
      "\u001b[31mTraining finished.\u001b[0m\n",
      "\u001b[31mAverage throughput in Million words/sec: 1.74\u001b[0m\n",
      "\u001b[31mTotal training time in seconds: 7.51\u001b[0m\n",
      "\n",
      "2019-11-01 16:49:12 Uploading - Uploading generated training model\n",
      "2019-11-01 16:50:53 Completed - Training job completed\n",
      "Training seconds: 143\n",
      "Billable seconds: 143\n"
     ]
    }
   ],
   "source": [
    "bt = sagemaker.estimator.Estimator(\n",
    "    sagemaker.amazon.amazon_estimator.get_image_uri(boto3.Session().region_name, 'blazingtext', 'latest'),\n",
    "    role, \n",
    "    train_instance_count=1, \n",
    "    train_instance_type='ml.p3.2xlarge',\n",
    "    train_volume_size = 5,\n",
    "    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "    sagemaker_session=sess)\n",
    "\n",
    "bt.set_hyperparameters(mode=\"skipgram\",\n",
    "    epochs=10,\n",
    "    min_count=5,\n",
    "    sampling_threshold=0.0001,\n",
    "    learning_rate=0.05,\n",
    "    window_size=5,\n",
    "    vector_dim=100,\n",
    "    negative_samples=5,\n",
    "    min_char=5,\n",
    "    max_char=10,\n",
    "    evaluation=False,\n",
    "    subwords=True)\n",
    "\n",
    "bt.fit({'train': sagemaker.s3_input(inputs, distribution='FullyReplicated', content_type='text/plain')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model\n",
    "\n",
    "- Bring in and extract the model from S3\n",
    "- Take a look at the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-west-2-601091450883/sagemaker/DEMO-loft-recommender/output/blazingtext-2019-11-01-16-46-39-008/output/model.tar.gz to ./model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp $bt.model_data ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectors.txt\n",
      "vectors.bin\n"
     ]
    }
   ],
   "source": [
    "!tar -xvzf model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = pd.read_csv('vectors.txt', delimiter=' ', skiprows=2, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the embeddings appear to have meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5128</th>\n",
       "      <td>veggietales:-the-little-drummer-boy</td>\n",
       "      <td>-1.19390</td>\n",
       "      <td>-0.052365</td>\n",
       "      <td>0.220690</td>\n",
       "      <td>0.405330</td>\n",
       "      <td>0.247000</td>\n",
       "      <td>0.155670</td>\n",
       "      <td>0.742000</td>\n",
       "      <td>0.380340</td>\n",
       "      <td>0.182870</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074601</td>\n",
       "      <td>0.100630</td>\n",
       "      <td>-0.584120</td>\n",
       "      <td>0.474680</td>\n",
       "      <td>-0.276830</td>\n",
       "      <td>0.519280</td>\n",
       "      <td>-0.062354</td>\n",
       "      <td>-0.822050</td>\n",
       "      <td>-0.577450</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16906</th>\n",
       "      <td>veggietales:-noah's-ark</td>\n",
       "      <td>-1.11910</td>\n",
       "      <td>-0.294990</td>\n",
       "      <td>0.441100</td>\n",
       "      <td>0.514340</td>\n",
       "      <td>0.403550</td>\n",
       "      <td>0.353930</td>\n",
       "      <td>0.866120</td>\n",
       "      <td>0.596170</td>\n",
       "      <td>0.252490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064793</td>\n",
       "      <td>0.298160</td>\n",
       "      <td>-0.536720</td>\n",
       "      <td>0.797080</td>\n",
       "      <td>0.161680</td>\n",
       "      <td>-0.094865</td>\n",
       "      <td>0.024814</td>\n",
       "      <td>-1.224600</td>\n",
       "      <td>-0.725510</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6037</th>\n",
       "      <td>veggietales:-the-league-of-incredible-vegetables</td>\n",
       "      <td>-1.10510</td>\n",
       "      <td>-0.209130</td>\n",
       "      <td>0.330890</td>\n",
       "      <td>0.189740</td>\n",
       "      <td>0.106640</td>\n",
       "      <td>0.092642</td>\n",
       "      <td>0.649720</td>\n",
       "      <td>0.409550</td>\n",
       "      <td>0.193740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120320</td>\n",
       "      <td>-0.133850</td>\n",
       "      <td>-0.484600</td>\n",
       "      <td>0.476060</td>\n",
       "      <td>-0.020432</td>\n",
       "      <td>0.147540</td>\n",
       "      <td>-0.251300</td>\n",
       "      <td>-0.716380</td>\n",
       "      <td>-0.159310</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9235</th>\n",
       "      <td>caribbean</td>\n",
       "      <td>-1.09580</td>\n",
       "      <td>0.290460</td>\n",
       "      <td>1.020700</td>\n",
       "      <td>0.008382</td>\n",
       "      <td>-0.246890</td>\n",
       "      <td>-0.036772</td>\n",
       "      <td>-0.188250</td>\n",
       "      <td>0.095978</td>\n",
       "      <td>0.702370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302040</td>\n",
       "      <td>-0.366680</td>\n",
       "      <td>-0.067651</td>\n",
       "      <td>0.420240</td>\n",
       "      <td>-0.320750</td>\n",
       "      <td>-0.342030</td>\n",
       "      <td>-0.439280</td>\n",
       "      <td>-0.448760</td>\n",
       "      <td>-0.037929</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6439</th>\n",
       "      <td>veggietales:-pistachio</td>\n",
       "      <td>-1.08910</td>\n",
       "      <td>-0.321500</td>\n",
       "      <td>0.517720</td>\n",
       "      <td>0.668640</td>\n",
       "      <td>0.568390</td>\n",
       "      <td>0.259110</td>\n",
       "      <td>0.543620</td>\n",
       "      <td>0.483110</td>\n",
       "      <td>0.418360</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127140</td>\n",
       "      <td>0.243270</td>\n",
       "      <td>-0.493600</td>\n",
       "      <td>0.749870</td>\n",
       "      <td>0.413340</td>\n",
       "      <td>-0.286370</td>\n",
       "      <td>-0.051256</td>\n",
       "      <td>-1.170100</td>\n",
       "      <td>-0.707940</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14000</th>\n",
       "      <td>veggietales:-the-penniless-princess</td>\n",
       "      <td>-1.07000</td>\n",
       "      <td>-0.146680</td>\n",
       "      <td>0.226450</td>\n",
       "      <td>0.289110</td>\n",
       "      <td>0.219090</td>\n",
       "      <td>0.221150</td>\n",
       "      <td>0.886730</td>\n",
       "      <td>0.535690</td>\n",
       "      <td>0.096234</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>-0.035394</td>\n",
       "      <td>-0.527440</td>\n",
       "      <td>0.517770</td>\n",
       "      <td>-0.144500</td>\n",
       "      <td>-0.009809</td>\n",
       "      <td>-0.052265</td>\n",
       "      <td>-0.915840</td>\n",
       "      <td>-0.416480</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4433</th>\n",
       "      <td>veggietales:-lord-of-the-beans</td>\n",
       "      <td>-1.05330</td>\n",
       "      <td>-0.614600</td>\n",
       "      <td>0.356070</td>\n",
       "      <td>0.477120</td>\n",
       "      <td>0.291580</td>\n",
       "      <td>0.228700</td>\n",
       "      <td>0.750190</td>\n",
       "      <td>0.576410</td>\n",
       "      <td>0.064485</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074266</td>\n",
       "      <td>-0.163830</td>\n",
       "      <td>-0.520610</td>\n",
       "      <td>0.834990</td>\n",
       "      <td>0.340570</td>\n",
       "      <td>-0.211410</td>\n",
       "      <td>-0.251420</td>\n",
       "      <td>-1.416200</td>\n",
       "      <td>-0.444940</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5984</th>\n",
       "      <td>veggietales:-madame-blueberry</td>\n",
       "      <td>-0.96960</td>\n",
       "      <td>-0.263260</td>\n",
       "      <td>0.361300</td>\n",
       "      <td>0.334050</td>\n",
       "      <td>0.517200</td>\n",
       "      <td>0.441120</td>\n",
       "      <td>0.958090</td>\n",
       "      <td>0.653950</td>\n",
       "      <td>0.310360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104210</td>\n",
       "      <td>0.051946</td>\n",
       "      <td>-0.315170</td>\n",
       "      <td>0.505240</td>\n",
       "      <td>-0.151040</td>\n",
       "      <td>-0.156950</td>\n",
       "      <td>0.028734</td>\n",
       "      <td>-1.285700</td>\n",
       "      <td>-0.636860</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16421</th>\n",
       "      <td>veggietales:-beauty-and-the-beet</td>\n",
       "      <td>-0.96147</td>\n",
       "      <td>-0.376670</td>\n",
       "      <td>0.241670</td>\n",
       "      <td>0.429800</td>\n",
       "      <td>0.293260</td>\n",
       "      <td>0.032279</td>\n",
       "      <td>1.142100</td>\n",
       "      <td>0.701820</td>\n",
       "      <td>0.175340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320670</td>\n",
       "      <td>0.172290</td>\n",
       "      <td>-0.466030</td>\n",
       "      <td>0.537150</td>\n",
       "      <td>0.077558</td>\n",
       "      <td>0.187760</td>\n",
       "      <td>-0.022171</td>\n",
       "      <td>-1.058200</td>\n",
       "      <td>-0.513350</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7547</th>\n",
       "      <td>history-rediscovered:-submarines-at-war</td>\n",
       "      <td>-0.95485</td>\n",
       "      <td>0.469910</td>\n",
       "      <td>-0.110400</td>\n",
       "      <td>-0.125500</td>\n",
       "      <td>-0.073741</td>\n",
       "      <td>-0.007412</td>\n",
       "      <td>0.049664</td>\n",
       "      <td>-0.483600</td>\n",
       "      <td>-0.141090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438480</td>\n",
       "      <td>-0.919770</td>\n",
       "      <td>0.043590</td>\n",
       "      <td>0.726340</td>\n",
       "      <td>-0.170780</td>\n",
       "      <td>-0.501150</td>\n",
       "      <td>-0.317210</td>\n",
       "      <td>-0.303260</td>\n",
       "      <td>-0.107350</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>taking-chance</td>\n",
       "      <td>-0.94820</td>\n",
       "      <td>-0.390910</td>\n",
       "      <td>-0.065807</td>\n",
       "      <td>0.334980</td>\n",
       "      <td>0.270210</td>\n",
       "      <td>0.233740</td>\n",
       "      <td>0.057580</td>\n",
       "      <td>0.088500</td>\n",
       "      <td>0.074139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195580</td>\n",
       "      <td>-0.612050</td>\n",
       "      <td>-0.258890</td>\n",
       "      <td>0.202120</td>\n",
       "      <td>-0.069473</td>\n",
       "      <td>-0.343810</td>\n",
       "      <td>-0.037812</td>\n",
       "      <td>-0.774500</td>\n",
       "      <td>-0.564460</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3043</th>\n",
       "      <td>soldier</td>\n",
       "      <td>-0.93105</td>\n",
       "      <td>0.693380</td>\n",
       "      <td>-0.176810</td>\n",
       "      <td>-0.009913</td>\n",
       "      <td>0.504000</td>\n",
       "      <td>0.114380</td>\n",
       "      <td>-0.240380</td>\n",
       "      <td>-0.483640</td>\n",
       "      <td>0.071218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.577630</td>\n",
       "      <td>-0.494280</td>\n",
       "      <td>-0.599140</td>\n",
       "      <td>0.524450</td>\n",
       "      <td>-0.330870</td>\n",
       "      <td>-0.628520</td>\n",
       "      <td>0.293440</td>\n",
       "      <td>-0.189690</td>\n",
       "      <td>-0.429660</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6042</th>\n",
       "      <td>pimpernel-smith</td>\n",
       "      <td>-0.93023</td>\n",
       "      <td>0.665780</td>\n",
       "      <td>0.251390</td>\n",
       "      <td>0.082006</td>\n",
       "      <td>0.514270</td>\n",
       "      <td>-0.504470</td>\n",
       "      <td>0.157630</td>\n",
       "      <td>0.049282</td>\n",
       "      <td>0.045563</td>\n",
       "      <td>...</td>\n",
       "      <td>0.611730</td>\n",
       "      <td>-0.281780</td>\n",
       "      <td>-0.228160</td>\n",
       "      <td>0.226770</td>\n",
       "      <td>-0.292150</td>\n",
       "      <td>0.087501</td>\n",
       "      <td>-0.615940</td>\n",
       "      <td>-0.147060</td>\n",
       "      <td>-0.217480</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2918</th>\n",
       "      <td>stargate</td>\n",
       "      <td>-0.91425</td>\n",
       "      <td>-0.284440</td>\n",
       "      <td>0.317040</td>\n",
       "      <td>-0.346990</td>\n",
       "      <td>-0.374920</td>\n",
       "      <td>-0.566580</td>\n",
       "      <td>1.112500</td>\n",
       "      <td>-0.517720</td>\n",
       "      <td>-0.047896</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085940</td>\n",
       "      <td>-0.661050</td>\n",
       "      <td>-0.832220</td>\n",
       "      <td>-0.037020</td>\n",
       "      <td>0.002793</td>\n",
       "      <td>0.172750</td>\n",
       "      <td>-0.095076</td>\n",
       "      <td>0.489050</td>\n",
       "      <td>-0.487500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8212</th>\n",
       "      <td>blackbeard-the-pirate---1952---color</td>\n",
       "      <td>-0.91269</td>\n",
       "      <td>0.427350</td>\n",
       "      <td>0.221810</td>\n",
       "      <td>-0.252050</td>\n",
       "      <td>0.039966</td>\n",
       "      <td>-0.058824</td>\n",
       "      <td>-0.058828</td>\n",
       "      <td>0.309860</td>\n",
       "      <td>0.196010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962390</td>\n",
       "      <td>-0.272760</td>\n",
       "      <td>-0.043752</td>\n",
       "      <td>-0.056918</td>\n",
       "      <td>-0.524680</td>\n",
       "      <td>-0.069396</td>\n",
       "      <td>0.109300</td>\n",
       "      <td>-0.479240</td>\n",
       "      <td>-0.025095</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5757</th>\n",
       "      <td>veggietales:-minnesota-cuke-and-the-search-for...</td>\n",
       "      <td>-0.91106</td>\n",
       "      <td>-0.240680</td>\n",
       "      <td>0.268170</td>\n",
       "      <td>0.431240</td>\n",
       "      <td>0.098384</td>\n",
       "      <td>0.183720</td>\n",
       "      <td>0.911880</td>\n",
       "      <td>0.562160</td>\n",
       "      <td>0.305780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266760</td>\n",
       "      <td>0.051816</td>\n",
       "      <td>-0.149290</td>\n",
       "      <td>0.556910</td>\n",
       "      <td>0.139170</td>\n",
       "      <td>0.039150</td>\n",
       "      <td>-0.033328</td>\n",
       "      <td>-0.799610</td>\n",
       "      <td>-0.526270</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2082</th>\n",
       "      <td>lord-of-the-rings:-the-two-towers</td>\n",
       "      <td>-0.90276</td>\n",
       "      <td>-0.536410</td>\n",
       "      <td>0.547400</td>\n",
       "      <td>0.027731</td>\n",
       "      <td>0.240160</td>\n",
       "      <td>0.646940</td>\n",
       "      <td>0.556820</td>\n",
       "      <td>0.671870</td>\n",
       "      <td>-0.154140</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011072</td>\n",
       "      <td>-0.624740</td>\n",
       "      <td>-1.244000</td>\n",
       "      <td>0.444220</td>\n",
       "      <td>0.094792</td>\n",
       "      <td>0.235260</td>\n",
       "      <td>-0.486640</td>\n",
       "      <td>-0.900060</td>\n",
       "      <td>0.700660</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14857</th>\n",
       "      <td>veggietales:-an-easter-carol</td>\n",
       "      <td>-0.89932</td>\n",
       "      <td>-0.181590</td>\n",
       "      <td>0.286890</td>\n",
       "      <td>0.325210</td>\n",
       "      <td>0.415780</td>\n",
       "      <td>0.102350</td>\n",
       "      <td>1.009500</td>\n",
       "      <td>0.561860</td>\n",
       "      <td>0.181790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007994</td>\n",
       "      <td>0.022141</td>\n",
       "      <td>-0.369650</td>\n",
       "      <td>0.624300</td>\n",
       "      <td>0.385850</td>\n",
       "      <td>-0.194670</td>\n",
       "      <td>-0.348440</td>\n",
       "      <td>-1.065300</td>\n",
       "      <td>-0.385290</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3507</th>\n",
       "      <td>veggietales:-princess-and-the-pop-star</td>\n",
       "      <td>-0.87710</td>\n",
       "      <td>0.050889</td>\n",
       "      <td>0.216530</td>\n",
       "      <td>0.350860</td>\n",
       "      <td>0.157550</td>\n",
       "      <td>0.089658</td>\n",
       "      <td>0.811930</td>\n",
       "      <td>0.641120</td>\n",
       "      <td>0.127370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052120</td>\n",
       "      <td>-0.080362</td>\n",
       "      <td>-0.380380</td>\n",
       "      <td>0.578050</td>\n",
       "      <td>0.063761</td>\n",
       "      <td>0.266530</td>\n",
       "      <td>-0.025403</td>\n",
       "      <td>-1.097200</td>\n",
       "      <td>-0.475860</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7366</th>\n",
       "      <td>long-road-home</td>\n",
       "      <td>-0.86751</td>\n",
       "      <td>0.231150</td>\n",
       "      <td>-0.099818</td>\n",
       "      <td>0.376160</td>\n",
       "      <td>0.111280</td>\n",
       "      <td>-0.097527</td>\n",
       "      <td>0.151840</td>\n",
       "      <td>0.032990</td>\n",
       "      <td>-0.041717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450050</td>\n",
       "      <td>0.042515</td>\n",
       "      <td>0.012705</td>\n",
       "      <td>0.430390</td>\n",
       "      <td>-0.812640</td>\n",
       "      <td>-0.034013</td>\n",
       "      <td>-0.354870</td>\n",
       "      <td>-0.456470</td>\n",
       "      <td>-0.357620</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7892</th>\n",
       "      <td>lord-of-the-rings-(1978)</td>\n",
       "      <td>-0.85127</td>\n",
       "      <td>0.030672</td>\n",
       "      <td>0.610890</td>\n",
       "      <td>-0.355720</td>\n",
       "      <td>0.031534</td>\n",
       "      <td>0.245350</td>\n",
       "      <td>0.426510</td>\n",
       "      <td>0.451850</td>\n",
       "      <td>-0.192350</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056485</td>\n",
       "      <td>-0.635160</td>\n",
       "      <td>-0.908320</td>\n",
       "      <td>0.585410</td>\n",
       "      <td>-0.081083</td>\n",
       "      <td>-0.205710</td>\n",
       "      <td>-0.186130</td>\n",
       "      <td>-0.507870</td>\n",
       "      <td>0.475860</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7310</th>\n",
       "      <td>veggietales:-sumo-of-the-opera</td>\n",
       "      <td>-0.84191</td>\n",
       "      <td>-0.484940</td>\n",
       "      <td>0.249600</td>\n",
       "      <td>0.455610</td>\n",
       "      <td>0.123930</td>\n",
       "      <td>0.365500</td>\n",
       "      <td>0.730970</td>\n",
       "      <td>0.596670</td>\n",
       "      <td>0.176920</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.215970</td>\n",
       "      <td>-0.096354</td>\n",
       "      <td>-0.354610</td>\n",
       "      <td>0.573400</td>\n",
       "      <td>0.134710</td>\n",
       "      <td>0.236850</td>\n",
       "      <td>-0.191980</td>\n",
       "      <td>-1.072700</td>\n",
       "      <td>-0.401110</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6112</th>\n",
       "      <td>veggietales:-a-snoodle's-tale</td>\n",
       "      <td>-0.83151</td>\n",
       "      <td>-0.250730</td>\n",
       "      <td>0.278540</td>\n",
       "      <td>0.604660</td>\n",
       "      <td>0.303600</td>\n",
       "      <td>0.205770</td>\n",
       "      <td>0.700570</td>\n",
       "      <td>0.464310</td>\n",
       "      <td>0.143470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083677</td>\n",
       "      <td>-0.048367</td>\n",
       "      <td>-0.452550</td>\n",
       "      <td>0.714230</td>\n",
       "      <td>0.151420</td>\n",
       "      <td>0.063321</td>\n",
       "      <td>-0.072653</td>\n",
       "      <td>-1.380500</td>\n",
       "      <td>-0.721030</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4709</th>\n",
       "      <td>veggietales:-dave-and-the-giant-pickle</td>\n",
       "      <td>-0.82846</td>\n",
       "      <td>-0.255810</td>\n",
       "      <td>0.210390</td>\n",
       "      <td>0.450240</td>\n",
       "      <td>0.193200</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.871040</td>\n",
       "      <td>0.469220</td>\n",
       "      <td>0.348680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108570</td>\n",
       "      <td>0.156610</td>\n",
       "      <td>-0.071942</td>\n",
       "      <td>0.630250</td>\n",
       "      <td>0.068402</td>\n",
       "      <td>-0.080349</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>-1.183800</td>\n",
       "      <td>-0.629680</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9168</th>\n",
       "      <td>the-general</td>\n",
       "      <td>-0.82610</td>\n",
       "      <td>0.284020</td>\n",
       "      <td>0.557710</td>\n",
       "      <td>-0.119510</td>\n",
       "      <td>0.254220</td>\n",
       "      <td>-0.357400</td>\n",
       "      <td>0.099667</td>\n",
       "      <td>-0.256230</td>\n",
       "      <td>0.447370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.424490</td>\n",
       "      <td>-0.321400</td>\n",
       "      <td>0.454570</td>\n",
       "      <td>-0.017991</td>\n",
       "      <td>-0.511970</td>\n",
       "      <td>-0.005566</td>\n",
       "      <td>-0.253070</td>\n",
       "      <td>-0.214640</td>\n",
       "      <td>0.088291</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15532</th>\n",
       "      <td>veggietales:-the-little-house-that-stood</td>\n",
       "      <td>-0.82559</td>\n",
       "      <td>-0.033532</td>\n",
       "      <td>0.199580</td>\n",
       "      <td>0.361200</td>\n",
       "      <td>0.199930</td>\n",
       "      <td>0.246320</td>\n",
       "      <td>0.553790</td>\n",
       "      <td>0.417730</td>\n",
       "      <td>0.109990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044062</td>\n",
       "      <td>0.138060</td>\n",
       "      <td>-0.414760</td>\n",
       "      <td>0.573210</td>\n",
       "      <td>-0.245650</td>\n",
       "      <td>0.260930</td>\n",
       "      <td>-0.181010</td>\n",
       "      <td>-1.054000</td>\n",
       "      <td>-0.373020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4354</th>\n",
       "      <td>thumbelina</td>\n",
       "      <td>-0.82533</td>\n",
       "      <td>0.204980</td>\n",
       "      <td>-0.014548</td>\n",
       "      <td>-0.062709</td>\n",
       "      <td>0.018984</td>\n",
       "      <td>0.454860</td>\n",
       "      <td>0.739590</td>\n",
       "      <td>0.467130</td>\n",
       "      <td>-0.422620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155370</td>\n",
       "      <td>-0.283810</td>\n",
       "      <td>0.095883</td>\n",
       "      <td>-0.012208</td>\n",
       "      <td>-0.345910</td>\n",
       "      <td>-0.000313</td>\n",
       "      <td>0.445440</td>\n",
       "      <td>-0.328780</td>\n",
       "      <td>-0.427070</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8407</th>\n",
       "      <td>dambusters-raid,-the</td>\n",
       "      <td>-0.82491</td>\n",
       "      <td>0.268850</td>\n",
       "      <td>0.013284</td>\n",
       "      <td>-0.120180</td>\n",
       "      <td>-0.171070</td>\n",
       "      <td>-0.063850</td>\n",
       "      <td>-0.334900</td>\n",
       "      <td>-0.460020</td>\n",
       "      <td>0.232060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.439240</td>\n",
       "      <td>-0.328770</td>\n",
       "      <td>0.047198</td>\n",
       "      <td>0.455770</td>\n",
       "      <td>-0.136700</td>\n",
       "      <td>-0.125360</td>\n",
       "      <td>-0.197400</td>\n",
       "      <td>-0.538330</td>\n",
       "      <td>-0.320760</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4985</th>\n",
       "      <td>silent-enemy,-the---1958</td>\n",
       "      <td>-0.81801</td>\n",
       "      <td>0.630590</td>\n",
       "      <td>-0.271070</td>\n",
       "      <td>-0.061160</td>\n",
       "      <td>0.053389</td>\n",
       "      <td>-0.202790</td>\n",
       "      <td>-0.130170</td>\n",
       "      <td>-0.199120</td>\n",
       "      <td>-0.017938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.689730</td>\n",
       "      <td>-0.515850</td>\n",
       "      <td>-0.017471</td>\n",
       "      <td>0.292000</td>\n",
       "      <td>-0.327980</td>\n",
       "      <td>-0.411640</td>\n",
       "      <td>-0.396810</td>\n",
       "      <td>-0.340620</td>\n",
       "      <td>-0.516240</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7422</th>\n",
       "      <td>cry-danger</td>\n",
       "      <td>-0.81606</td>\n",
       "      <td>0.712040</td>\n",
       "      <td>0.274470</td>\n",
       "      <td>-0.429470</td>\n",
       "      <td>0.171350</td>\n",
       "      <td>0.044376</td>\n",
       "      <td>0.111230</td>\n",
       "      <td>-0.232930</td>\n",
       "      <td>-0.030312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.449760</td>\n",
       "      <td>0.116690</td>\n",
       "      <td>-0.132040</td>\n",
       "      <td>-0.116650</td>\n",
       "      <td>-0.691010</td>\n",
       "      <td>-0.347260</td>\n",
       "      <td>-0.197150</td>\n",
       "      <td>-0.613170</td>\n",
       "      <td>-0.473530</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14260</th>\n",
       "      <td>king-of-the-hill-season-4</td>\n",
       "      <td>1.14690</td>\n",
       "      <td>-0.178390</td>\n",
       "      <td>0.596740</td>\n",
       "      <td>-0.401460</td>\n",
       "      <td>0.585060</td>\n",
       "      <td>0.340420</td>\n",
       "      <td>0.221970</td>\n",
       "      <td>0.343760</td>\n",
       "      <td>-0.466840</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004191</td>\n",
       "      <td>-0.529450</td>\n",
       "      <td>0.192950</td>\n",
       "      <td>0.223590</td>\n",
       "      <td>-0.821770</td>\n",
       "      <td>0.543290</td>\n",
       "      <td>0.012134</td>\n",
       "      <td>-0.156590</td>\n",
       "      <td>0.753370</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15610</th>\n",
       "      <td>the-real-housewives-of-miami-season-2</td>\n",
       "      <td>1.16740</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.139070</td>\n",
       "      <td>0.242950</td>\n",
       "      <td>0.746820</td>\n",
       "      <td>-0.699020</td>\n",
       "      <td>0.015309</td>\n",
       "      <td>0.187350</td>\n",
       "      <td>-0.362070</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.218880</td>\n",
       "      <td>-0.248630</td>\n",
       "      <td>-0.162820</td>\n",
       "      <td>-0.766710</td>\n",
       "      <td>-0.687330</td>\n",
       "      <td>0.690010</td>\n",
       "      <td>-0.085205</td>\n",
       "      <td>-0.972860</td>\n",
       "      <td>1.008100</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3033</th>\n",
       "      <td>those-who-kill-season-1</td>\n",
       "      <td>1.16740</td>\n",
       "      <td>0.131820</td>\n",
       "      <td>0.261140</td>\n",
       "      <td>0.223210</td>\n",
       "      <td>0.387930</td>\n",
       "      <td>-0.443170</td>\n",
       "      <td>0.182370</td>\n",
       "      <td>-0.127780</td>\n",
       "      <td>-0.391640</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.401750</td>\n",
       "      <td>-0.152100</td>\n",
       "      <td>0.613700</td>\n",
       "      <td>-0.521040</td>\n",
       "      <td>-0.181870</td>\n",
       "      <td>0.431400</td>\n",
       "      <td>-0.627160</td>\n",
       "      <td>-0.194010</td>\n",
       "      <td>0.119870</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16106</th>\n",
       "      <td>the-real-housewives-of-orange-county-season-7</td>\n",
       "      <td>1.17050</td>\n",
       "      <td>0.481570</td>\n",
       "      <td>0.035684</td>\n",
       "      <td>0.337140</td>\n",
       "      <td>0.590150</td>\n",
       "      <td>-0.469380</td>\n",
       "      <td>-0.117500</td>\n",
       "      <td>0.271650</td>\n",
       "      <td>-0.334760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094517</td>\n",
       "      <td>-0.165200</td>\n",
       "      <td>-0.148510</td>\n",
       "      <td>-0.407590</td>\n",
       "      <td>-0.644110</td>\n",
       "      <td>0.742360</td>\n",
       "      <td>-0.066314</td>\n",
       "      <td>-0.782930</td>\n",
       "      <td>0.910540</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5146</th>\n",
       "      <td>haunt</td>\n",
       "      <td>1.17090</td>\n",
       "      <td>0.834170</td>\n",
       "      <td>0.673610</td>\n",
       "      <td>-0.163950</td>\n",
       "      <td>-0.009396</td>\n",
       "      <td>-0.893890</td>\n",
       "      <td>0.495360</td>\n",
       "      <td>-0.527920</td>\n",
       "      <td>-0.415260</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.372660</td>\n",
       "      <td>-0.090349</td>\n",
       "      <td>0.822390</td>\n",
       "      <td>-0.257500</td>\n",
       "      <td>-0.337880</td>\n",
       "      <td>0.469250</td>\n",
       "      <td>-0.215040</td>\n",
       "      <td>-0.827860</td>\n",
       "      <td>0.214040</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1706</th>\n",
       "      <td>battlestar-galactica-season-4</td>\n",
       "      <td>1.18320</td>\n",
       "      <td>-0.905040</td>\n",
       "      <td>-0.725400</td>\n",
       "      <td>0.423260</td>\n",
       "      <td>0.119350</td>\n",
       "      <td>-0.166350</td>\n",
       "      <td>-0.208910</td>\n",
       "      <td>0.229740</td>\n",
       "      <td>-0.543080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.795430</td>\n",
       "      <td>-0.941590</td>\n",
       "      <td>-0.482060</td>\n",
       "      <td>0.669740</td>\n",
       "      <td>-0.160050</td>\n",
       "      <td>-0.256730</td>\n",
       "      <td>-0.046207</td>\n",
       "      <td>0.360100</td>\n",
       "      <td>-0.306730</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>the-originals:-the-complete-first-season</td>\n",
       "      <td>1.19770</td>\n",
       "      <td>0.158210</td>\n",
       "      <td>0.236630</td>\n",
       "      <td>0.391990</td>\n",
       "      <td>0.707590</td>\n",
       "      <td>-0.237230</td>\n",
       "      <td>0.362190</td>\n",
       "      <td>0.408210</td>\n",
       "      <td>-0.418480</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.136810</td>\n",
       "      <td>-0.142210</td>\n",
       "      <td>-0.394110</td>\n",
       "      <td>0.067704</td>\n",
       "      <td>-0.089378</td>\n",
       "      <td>0.108110</td>\n",
       "      <td>0.033257</td>\n",
       "      <td>-0.790460</td>\n",
       "      <td>0.123340</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2651</th>\n",
       "      <td>bates-motel---season-1</td>\n",
       "      <td>1.21110</td>\n",
       "      <td>-0.109060</td>\n",
       "      <td>0.320810</td>\n",
       "      <td>-0.077240</td>\n",
       "      <td>0.539530</td>\n",
       "      <td>-0.241470</td>\n",
       "      <td>0.432870</td>\n",
       "      <td>0.126140</td>\n",
       "      <td>-0.488340</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.621040</td>\n",
       "      <td>-0.056140</td>\n",
       "      <td>0.132720</td>\n",
       "      <td>-0.057954</td>\n",
       "      <td>-0.364720</td>\n",
       "      <td>-0.001400</td>\n",
       "      <td>0.057061</td>\n",
       "      <td>-0.216400</td>\n",
       "      <td>0.142520</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5834</th>\n",
       "      <td>the-real-housewives-of-nyc,-season-7</td>\n",
       "      <td>1.21900</td>\n",
       "      <td>0.412870</td>\n",
       "      <td>0.275360</td>\n",
       "      <td>0.441140</td>\n",
       "      <td>0.706610</td>\n",
       "      <td>-0.644220</td>\n",
       "      <td>-0.334720</td>\n",
       "      <td>0.210850</td>\n",
       "      <td>-0.176110</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001522</td>\n",
       "      <td>-0.139770</td>\n",
       "      <td>-0.201440</td>\n",
       "      <td>-0.660420</td>\n",
       "      <td>-0.725420</td>\n",
       "      <td>0.919310</td>\n",
       "      <td>-0.089235</td>\n",
       "      <td>-0.832680</td>\n",
       "      <td>1.003800</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2565</th>\n",
       "      <td>welcome-to-me</td>\n",
       "      <td>1.23090</td>\n",
       "      <td>-0.202370</td>\n",
       "      <td>0.236310</td>\n",
       "      <td>0.165770</td>\n",
       "      <td>-0.138440</td>\n",
       "      <td>-0.103350</td>\n",
       "      <td>-0.040206</td>\n",
       "      <td>-0.123350</td>\n",
       "      <td>0.463730</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.091813</td>\n",
       "      <td>0.550900</td>\n",
       "      <td>0.215580</td>\n",
       "      <td>-0.398790</td>\n",
       "      <td>-0.365160</td>\n",
       "      <td>0.846540</td>\n",
       "      <td>0.014016</td>\n",
       "      <td>-0.573680</td>\n",
       "      <td>-0.330620</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17452</th>\n",
       "      <td>the-real-housewives-of-new-jersey-season-4</td>\n",
       "      <td>1.23140</td>\n",
       "      <td>0.376500</td>\n",
       "      <td>0.256320</td>\n",
       "      <td>0.252850</td>\n",
       "      <td>0.647020</td>\n",
       "      <td>-0.626280</td>\n",
       "      <td>0.103030</td>\n",
       "      <td>0.301890</td>\n",
       "      <td>-0.230690</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.110440</td>\n",
       "      <td>-0.152120</td>\n",
       "      <td>-0.267190</td>\n",
       "      <td>-0.690860</td>\n",
       "      <td>-0.740330</td>\n",
       "      <td>0.707320</td>\n",
       "      <td>-0.015462</td>\n",
       "      <td>-0.787640</td>\n",
       "      <td>1.018900</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5764</th>\n",
       "      <td>the-real-housewives-of-new-jersey-season-5</td>\n",
       "      <td>1.23230</td>\n",
       "      <td>0.396960</td>\n",
       "      <td>0.247110</td>\n",
       "      <td>0.263190</td>\n",
       "      <td>0.676910</td>\n",
       "      <td>-0.625620</td>\n",
       "      <td>0.066835</td>\n",
       "      <td>0.326600</td>\n",
       "      <td>-0.230250</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.154910</td>\n",
       "      <td>-0.148190</td>\n",
       "      <td>-0.203930</td>\n",
       "      <td>-0.665880</td>\n",
       "      <td>-0.729880</td>\n",
       "      <td>0.733620</td>\n",
       "      <td>-0.018312</td>\n",
       "      <td>-0.824670</td>\n",
       "      <td>1.030100</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1833</th>\n",
       "      <td>the-originals:-season-2</td>\n",
       "      <td>1.23300</td>\n",
       "      <td>-0.111030</td>\n",
       "      <td>0.427730</td>\n",
       "      <td>0.241830</td>\n",
       "      <td>0.828310</td>\n",
       "      <td>-0.153750</td>\n",
       "      <td>0.187970</td>\n",
       "      <td>0.457500</td>\n",
       "      <td>-0.405750</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.334250</td>\n",
       "      <td>0.199350</td>\n",
       "      <td>-0.127340</td>\n",
       "      <td>-0.051066</td>\n",
       "      <td>-0.363880</td>\n",
       "      <td>0.074221</td>\n",
       "      <td>0.023650</td>\n",
       "      <td>-0.974330</td>\n",
       "      <td>0.206530</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4638</th>\n",
       "      <td>the-real-housewives-of-nyc,-season-6</td>\n",
       "      <td>1.23400</td>\n",
       "      <td>0.431990</td>\n",
       "      <td>0.280380</td>\n",
       "      <td>0.443990</td>\n",
       "      <td>0.766750</td>\n",
       "      <td>-0.656840</td>\n",
       "      <td>-0.364610</td>\n",
       "      <td>0.215200</td>\n",
       "      <td>-0.119410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018959</td>\n",
       "      <td>-0.082507</td>\n",
       "      <td>-0.231980</td>\n",
       "      <td>-0.688660</td>\n",
       "      <td>-0.662690</td>\n",
       "      <td>0.949290</td>\n",
       "      <td>-0.187340</td>\n",
       "      <td>-0.839300</td>\n",
       "      <td>1.016700</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4975</th>\n",
       "      <td>the-real-housewives-of-new-jersey,-season-6</td>\n",
       "      <td>1.25020</td>\n",
       "      <td>0.184670</td>\n",
       "      <td>0.197750</td>\n",
       "      <td>0.384350</td>\n",
       "      <td>0.684340</td>\n",
       "      <td>-0.673410</td>\n",
       "      <td>-0.178250</td>\n",
       "      <td>0.393750</td>\n",
       "      <td>-0.211430</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.173900</td>\n",
       "      <td>-0.002723</td>\n",
       "      <td>-0.161130</td>\n",
       "      <td>-0.623620</td>\n",
       "      <td>-0.576070</td>\n",
       "      <td>0.819050</td>\n",
       "      <td>-0.286020</td>\n",
       "      <td>-0.982250</td>\n",
       "      <td>1.016800</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4130</th>\n",
       "      <td>burning-blue</td>\n",
       "      <td>1.26580</td>\n",
       "      <td>0.234860</td>\n",
       "      <td>0.727960</td>\n",
       "      <td>-0.249140</td>\n",
       "      <td>0.168210</td>\n",
       "      <td>0.149250</td>\n",
       "      <td>0.337670</td>\n",
       "      <td>-0.236520</td>\n",
       "      <td>0.014404</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087687</td>\n",
       "      <td>0.301840</td>\n",
       "      <td>-0.076565</td>\n",
       "      <td>0.216580</td>\n",
       "      <td>-0.086369</td>\n",
       "      <td>-0.289820</td>\n",
       "      <td>0.293220</td>\n",
       "      <td>-0.611960</td>\n",
       "      <td>0.412680</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9717</th>\n",
       "      <td>the-real-housewives-of-orange-county,-season-10</td>\n",
       "      <td>1.27350</td>\n",
       "      <td>0.418980</td>\n",
       "      <td>0.039835</td>\n",
       "      <td>0.422640</td>\n",
       "      <td>0.625430</td>\n",
       "      <td>-0.492210</td>\n",
       "      <td>-0.355090</td>\n",
       "      <td>0.212780</td>\n",
       "      <td>-0.245020</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.146560</td>\n",
       "      <td>-0.036669</td>\n",
       "      <td>-0.037425</td>\n",
       "      <td>-0.401850</td>\n",
       "      <td>-0.716800</td>\n",
       "      <td>0.844410</td>\n",
       "      <td>-0.214790</td>\n",
       "      <td>-0.945300</td>\n",
       "      <td>1.001300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6210</th>\n",
       "      <td>the-real-housewives-of-atlanta,-season-7</td>\n",
       "      <td>1.27400</td>\n",
       "      <td>0.191190</td>\n",
       "      <td>0.464280</td>\n",
       "      <td>0.391530</td>\n",
       "      <td>0.739870</td>\n",
       "      <td>-0.835530</td>\n",
       "      <td>-0.095523</td>\n",
       "      <td>-0.055842</td>\n",
       "      <td>-0.361770</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128070</td>\n",
       "      <td>-0.268570</td>\n",
       "      <td>-0.096458</td>\n",
       "      <td>-0.635770</td>\n",
       "      <td>-0.709030</td>\n",
       "      <td>0.782790</td>\n",
       "      <td>-0.061847</td>\n",
       "      <td>-1.004400</td>\n",
       "      <td>0.993380</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4034</th>\n",
       "      <td>the-real-housewives-of-orange-county,-season-9</td>\n",
       "      <td>1.29700</td>\n",
       "      <td>0.486190</td>\n",
       "      <td>0.045519</td>\n",
       "      <td>0.409280</td>\n",
       "      <td>0.593950</td>\n",
       "      <td>-0.550490</td>\n",
       "      <td>-0.292610</td>\n",
       "      <td>0.106800</td>\n",
       "      <td>-0.358620</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.201050</td>\n",
       "      <td>-0.023162</td>\n",
       "      <td>-0.001108</td>\n",
       "      <td>-0.420300</td>\n",
       "      <td>-0.720380</td>\n",
       "      <td>0.815510</td>\n",
       "      <td>-0.180450</td>\n",
       "      <td>-0.991080</td>\n",
       "      <td>0.986920</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15328</th>\n",
       "      <td>the-real-housewives-of-atlanta-season-1</td>\n",
       "      <td>1.31320</td>\n",
       "      <td>0.362020</td>\n",
       "      <td>0.445810</td>\n",
       "      <td>0.538820</td>\n",
       "      <td>0.751500</td>\n",
       "      <td>-0.878770</td>\n",
       "      <td>-0.015895</td>\n",
       "      <td>-0.237410</td>\n",
       "      <td>-0.506730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160340</td>\n",
       "      <td>-0.506630</td>\n",
       "      <td>-0.248550</td>\n",
       "      <td>-0.610500</td>\n",
       "      <td>-0.573550</td>\n",
       "      <td>0.600760</td>\n",
       "      <td>0.212070</td>\n",
       "      <td>-0.869270</td>\n",
       "      <td>1.161000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5100</th>\n",
       "      <td>hot-in-cleveland</td>\n",
       "      <td>1.32370</td>\n",
       "      <td>0.047066</td>\n",
       "      <td>1.081100</td>\n",
       "      <td>-1.050300</td>\n",
       "      <td>-0.819230</td>\n",
       "      <td>-0.128290</td>\n",
       "      <td>1.513600</td>\n",
       "      <td>0.293170</td>\n",
       "      <td>0.513080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470160</td>\n",
       "      <td>-1.026400</td>\n",
       "      <td>0.433050</td>\n",
       "      <td>0.037364</td>\n",
       "      <td>-1.592000</td>\n",
       "      <td>-0.066578</td>\n",
       "      <td>-0.394040</td>\n",
       "      <td>-0.362830</td>\n",
       "      <td>-0.646810</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17370</th>\n",
       "      <td>the-real-housewives-of-atlanta-season-2</td>\n",
       "      <td>1.33190</td>\n",
       "      <td>0.402280</td>\n",
       "      <td>0.427840</td>\n",
       "      <td>0.488960</td>\n",
       "      <td>0.785010</td>\n",
       "      <td>-0.886970</td>\n",
       "      <td>-0.033815</td>\n",
       "      <td>-0.198020</td>\n",
       "      <td>-0.513490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140750</td>\n",
       "      <td>-0.458270</td>\n",
       "      <td>-0.260290</td>\n",
       "      <td>-0.633000</td>\n",
       "      <td>-0.583480</td>\n",
       "      <td>0.635640</td>\n",
       "      <td>0.176200</td>\n",
       "      <td>-0.897700</td>\n",
       "      <td>1.150300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>bates-motel---season-2</td>\n",
       "      <td>1.34120</td>\n",
       "      <td>-0.129030</td>\n",
       "      <td>0.310230</td>\n",
       "      <td>-0.124000</td>\n",
       "      <td>0.585340</td>\n",
       "      <td>-0.251130</td>\n",
       "      <td>0.318630</td>\n",
       "      <td>0.138010</td>\n",
       "      <td>-0.451610</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.675910</td>\n",
       "      <td>0.043466</td>\n",
       "      <td>0.159720</td>\n",
       "      <td>-0.060490</td>\n",
       "      <td>-0.342590</td>\n",
       "      <td>0.061215</td>\n",
       "      <td>-0.083953</td>\n",
       "      <td>-0.378680</td>\n",
       "      <td>0.122820</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3457</th>\n",
       "      <td>the-real-housewives-of-atlanta-season-6</td>\n",
       "      <td>1.34910</td>\n",
       "      <td>0.353980</td>\n",
       "      <td>0.434850</td>\n",
       "      <td>0.501280</td>\n",
       "      <td>0.823410</td>\n",
       "      <td>-0.911560</td>\n",
       "      <td>-0.048018</td>\n",
       "      <td>-0.234150</td>\n",
       "      <td>-0.495260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175150</td>\n",
       "      <td>-0.422110</td>\n",
       "      <td>-0.244800</td>\n",
       "      <td>-0.547600</td>\n",
       "      <td>-0.528970</td>\n",
       "      <td>0.642580</td>\n",
       "      <td>0.045606</td>\n",
       "      <td>-0.897130</td>\n",
       "      <td>1.168800</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7709</th>\n",
       "      <td>the-real-housewives-of-atlanta-season-5</td>\n",
       "      <td>1.36240</td>\n",
       "      <td>0.353400</td>\n",
       "      <td>0.484560</td>\n",
       "      <td>0.525470</td>\n",
       "      <td>0.807220</td>\n",
       "      <td>-0.904590</td>\n",
       "      <td>-0.023278</td>\n",
       "      <td>-0.240320</td>\n",
       "      <td>-0.565310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123710</td>\n",
       "      <td>-0.451190</td>\n",
       "      <td>-0.208910</td>\n",
       "      <td>-0.551400</td>\n",
       "      <td>-0.589090</td>\n",
       "      <td>0.641340</td>\n",
       "      <td>0.157690</td>\n",
       "      <td>-0.880540</td>\n",
       "      <td>1.212300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>scandal-season-4</td>\n",
       "      <td>1.37270</td>\n",
       "      <td>-0.620220</td>\n",
       "      <td>0.244340</td>\n",
       "      <td>0.170360</td>\n",
       "      <td>-0.072841</td>\n",
       "      <td>-0.818730</td>\n",
       "      <td>0.029379</td>\n",
       "      <td>0.107710</td>\n",
       "      <td>-0.313400</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.211830</td>\n",
       "      <td>0.037793</td>\n",
       "      <td>0.005665</td>\n",
       "      <td>0.436800</td>\n",
       "      <td>-0.259920</td>\n",
       "      <td>0.176610</td>\n",
       "      <td>-0.234810</td>\n",
       "      <td>-0.723180</td>\n",
       "      <td>0.107300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16415</th>\n",
       "      <td>the-real-housewives-of-atlanta-season-4</td>\n",
       "      <td>1.39080</td>\n",
       "      <td>0.329470</td>\n",
       "      <td>0.491890</td>\n",
       "      <td>0.517230</td>\n",
       "      <td>0.771830</td>\n",
       "      <td>-0.911690</td>\n",
       "      <td>-0.012968</td>\n",
       "      <td>-0.268920</td>\n",
       "      <td>-0.557810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143630</td>\n",
       "      <td>-0.440630</td>\n",
       "      <td>-0.254470</td>\n",
       "      <td>-0.569120</td>\n",
       "      <td>-0.621220</td>\n",
       "      <td>0.630400</td>\n",
       "      <td>0.166470</td>\n",
       "      <td>-0.856460</td>\n",
       "      <td>1.183400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3485</th>\n",
       "      <td>honeymoon</td>\n",
       "      <td>1.40790</td>\n",
       "      <td>0.577860</td>\n",
       "      <td>1.207500</td>\n",
       "      <td>0.402120</td>\n",
       "      <td>0.374460</td>\n",
       "      <td>-0.225620</td>\n",
       "      <td>0.154880</td>\n",
       "      <td>-0.613170</td>\n",
       "      <td>0.025380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.221420</td>\n",
       "      <td>0.032765</td>\n",
       "      <td>0.331680</td>\n",
       "      <td>0.427440</td>\n",
       "      <td>-0.361120</td>\n",
       "      <td>0.415420</td>\n",
       "      <td>0.074006</td>\n",
       "      <td>-0.004337</td>\n",
       "      <td>0.121440</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>salem-season-1</td>\n",
       "      <td>1.52960</td>\n",
       "      <td>0.416100</td>\n",
       "      <td>0.329070</td>\n",
       "      <td>0.176260</td>\n",
       "      <td>0.219680</td>\n",
       "      <td>-0.373850</td>\n",
       "      <td>0.430080</td>\n",
       "      <td>-0.067465</td>\n",
       "      <td>-0.306190</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.234650</td>\n",
       "      <td>-0.086584</td>\n",
       "      <td>0.126970</td>\n",
       "      <td>-0.224030</td>\n",
       "      <td>-0.633190</td>\n",
       "      <td>-0.051605</td>\n",
       "      <td>-0.050489</td>\n",
       "      <td>-0.172860</td>\n",
       "      <td>0.142200</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852</th>\n",
       "      <td>salem-season-2</td>\n",
       "      <td>1.66500</td>\n",
       "      <td>0.448740</td>\n",
       "      <td>0.164770</td>\n",
       "      <td>0.006425</td>\n",
       "      <td>0.282420</td>\n",
       "      <td>-0.346210</td>\n",
       "      <td>0.285890</td>\n",
       "      <td>-0.001282</td>\n",
       "      <td>-0.251310</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.426310</td>\n",
       "      <td>0.065890</td>\n",
       "      <td>0.261690</td>\n",
       "      <td>-0.266210</td>\n",
       "      <td>-0.638100</td>\n",
       "      <td>0.081216</td>\n",
       "      <td>-0.197640</td>\n",
       "      <td>-0.266340</td>\n",
       "      <td>0.091809</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17989 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0        1         2    \\\n",
       "5128                 veggietales:-the-little-drummer-boy -1.19390 -0.052365   \n",
       "16906                            veggietales:-noah's-ark -1.11910 -0.294990   \n",
       "6037    veggietales:-the-league-of-incredible-vegetables -1.10510 -0.209130   \n",
       "9235                                           caribbean -1.09580  0.290460   \n",
       "6439                              veggietales:-pistachio -1.08910 -0.321500   \n",
       "14000                veggietales:-the-penniless-princess -1.07000 -0.146680   \n",
       "4433                      veggietales:-lord-of-the-beans -1.05330 -0.614600   \n",
       "5984                       veggietales:-madame-blueberry -0.96960 -0.263260   \n",
       "16421                   veggietales:-beauty-and-the-beet -0.96147 -0.376670   \n",
       "7547             history-rediscovered:-submarines-at-war -0.95485  0.469910   \n",
       "600                                        taking-chance -0.94820 -0.390910   \n",
       "3043                                             soldier -0.93105  0.693380   \n",
       "6042                                     pimpernel-smith -0.93023  0.665780   \n",
       "2918                                            stargate -0.91425 -0.284440   \n",
       "8212                blackbeard-the-pirate---1952---color -0.91269  0.427350   \n",
       "5757   veggietales:-minnesota-cuke-and-the-search-for... -0.91106 -0.240680   \n",
       "2082                   lord-of-the-rings:-the-two-towers -0.90276 -0.536410   \n",
       "14857                       veggietales:-an-easter-carol -0.89932 -0.181590   \n",
       "3507              veggietales:-princess-and-the-pop-star -0.87710  0.050889   \n",
       "7366                                      long-road-home -0.86751  0.231150   \n",
       "7892                            lord-of-the-rings-(1978) -0.85127  0.030672   \n",
       "7310                      veggietales:-sumo-of-the-opera -0.84191 -0.484940   \n",
       "6112                       veggietales:-a-snoodle's-tale -0.83151 -0.250730   \n",
       "4709              veggietales:-dave-and-the-giant-pickle -0.82846 -0.255810   \n",
       "9168                                         the-general -0.82610  0.284020   \n",
       "15532           veggietales:-the-little-house-that-stood -0.82559 -0.033532   \n",
       "4354                                          thumbelina -0.82533  0.204980   \n",
       "8407                                dambusters-raid,-the -0.82491  0.268850   \n",
       "4985                            silent-enemy,-the---1958 -0.81801  0.630590   \n",
       "7422                                          cry-danger -0.81606  0.712040   \n",
       "...                                                  ...      ...       ...   \n",
       "14260                          king-of-the-hill-season-4  1.14690 -0.178390   \n",
       "15610              the-real-housewives-of-miami-season-2  1.16740  0.498350   \n",
       "3033                             those-who-kill-season-1  1.16740  0.131820   \n",
       "16106      the-real-housewives-of-orange-county-season-7  1.17050  0.481570   \n",
       "5146                                               haunt  1.17090  0.834170   \n",
       "1706                       battlestar-galactica-season-4  1.18320 -0.905040   \n",
       "1519            the-originals:-the-complete-first-season  1.19770  0.158210   \n",
       "2651                              bates-motel---season-1  1.21110 -0.109060   \n",
       "5834                the-real-housewives-of-nyc,-season-7  1.21900  0.412870   \n",
       "2565                                       welcome-to-me  1.23090 -0.202370   \n",
       "17452         the-real-housewives-of-new-jersey-season-4  1.23140  0.376500   \n",
       "5764          the-real-housewives-of-new-jersey-season-5  1.23230  0.396960   \n",
       "1833                             the-originals:-season-2  1.23300 -0.111030   \n",
       "4638                the-real-housewives-of-nyc,-season-6  1.23400  0.431990   \n",
       "4975         the-real-housewives-of-new-jersey,-season-6  1.25020  0.184670   \n",
       "4130                                        burning-blue  1.26580  0.234860   \n",
       "9717     the-real-housewives-of-orange-county,-season-10  1.27350  0.418980   \n",
       "6210            the-real-housewives-of-atlanta,-season-7  1.27400  0.191190   \n",
       "4034      the-real-housewives-of-orange-county,-season-9  1.29700  0.486190   \n",
       "15328            the-real-housewives-of-atlanta-season-1  1.31320  0.362020   \n",
       "5100                                    hot-in-cleveland  1.32370  0.047066   \n",
       "17370            the-real-housewives-of-atlanta-season-2  1.33190  0.402280   \n",
       "1664                              bates-motel---season-2  1.34120 -0.129030   \n",
       "3457             the-real-housewives-of-atlanta-season-6  1.34910  0.353980   \n",
       "7709             the-real-housewives-of-atlanta-season-5  1.36240  0.353400   \n",
       "832                                     scandal-season-4  1.37270 -0.620220   \n",
       "16415            the-real-housewives-of-atlanta-season-4  1.39080  0.329470   \n",
       "3485                                           honeymoon  1.40790  0.577860   \n",
       "1828                                      salem-season-1  1.52960  0.416100   \n",
       "2852                                      salem-season-2  1.66500  0.448740   \n",
       "\n",
       "            3         4         5         6         7         8         9    \\\n",
       "5128   0.220690  0.405330  0.247000  0.155670  0.742000  0.380340  0.182870   \n",
       "16906  0.441100  0.514340  0.403550  0.353930  0.866120  0.596170  0.252490   \n",
       "6037   0.330890  0.189740  0.106640  0.092642  0.649720  0.409550  0.193740   \n",
       "9235   1.020700  0.008382 -0.246890 -0.036772 -0.188250  0.095978  0.702370   \n",
       "6439   0.517720  0.668640  0.568390  0.259110  0.543620  0.483110  0.418360   \n",
       "14000  0.226450  0.289110  0.219090  0.221150  0.886730  0.535690  0.096234   \n",
       "4433   0.356070  0.477120  0.291580  0.228700  0.750190  0.576410  0.064485   \n",
       "5984   0.361300  0.334050  0.517200  0.441120  0.958090  0.653950  0.310360   \n",
       "16421  0.241670  0.429800  0.293260  0.032279  1.142100  0.701820  0.175340   \n",
       "7547  -0.110400 -0.125500 -0.073741 -0.007412  0.049664 -0.483600 -0.141090   \n",
       "600   -0.065807  0.334980  0.270210  0.233740  0.057580  0.088500  0.074139   \n",
       "3043  -0.176810 -0.009913  0.504000  0.114380 -0.240380 -0.483640  0.071218   \n",
       "6042   0.251390  0.082006  0.514270 -0.504470  0.157630  0.049282  0.045563   \n",
       "2918   0.317040 -0.346990 -0.374920 -0.566580  1.112500 -0.517720 -0.047896   \n",
       "8212   0.221810 -0.252050  0.039966 -0.058824 -0.058828  0.309860  0.196010   \n",
       "5757   0.268170  0.431240  0.098384  0.183720  0.911880  0.562160  0.305780   \n",
       "2082   0.547400  0.027731  0.240160  0.646940  0.556820  0.671870 -0.154140   \n",
       "14857  0.286890  0.325210  0.415780  0.102350  1.009500  0.561860  0.181790   \n",
       "3507   0.216530  0.350860  0.157550  0.089658  0.811930  0.641120  0.127370   \n",
       "7366  -0.099818  0.376160  0.111280 -0.097527  0.151840  0.032990 -0.041717   \n",
       "7892   0.610890 -0.355720  0.031534  0.245350  0.426510  0.451850 -0.192350   \n",
       "7310   0.249600  0.455610  0.123930  0.365500  0.730970  0.596670  0.176920   \n",
       "6112   0.278540  0.604660  0.303600  0.205770  0.700570  0.464310  0.143470   \n",
       "4709   0.210390  0.450240  0.193200  0.146000  0.871040  0.469220  0.348680   \n",
       "9168   0.557710 -0.119510  0.254220 -0.357400  0.099667 -0.256230  0.447370   \n",
       "15532  0.199580  0.361200  0.199930  0.246320  0.553790  0.417730  0.109990   \n",
       "4354  -0.014548 -0.062709  0.018984  0.454860  0.739590  0.467130 -0.422620   \n",
       "8407   0.013284 -0.120180 -0.171070 -0.063850 -0.334900 -0.460020  0.232060   \n",
       "4985  -0.271070 -0.061160  0.053389 -0.202790 -0.130170 -0.199120 -0.017938   \n",
       "7422   0.274470 -0.429470  0.171350  0.044376  0.111230 -0.232930 -0.030312   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "14260  0.596740 -0.401460  0.585060  0.340420  0.221970  0.343760 -0.466840   \n",
       "15610  0.139070  0.242950  0.746820 -0.699020  0.015309  0.187350 -0.362070   \n",
       "3033   0.261140  0.223210  0.387930 -0.443170  0.182370 -0.127780 -0.391640   \n",
       "16106  0.035684  0.337140  0.590150 -0.469380 -0.117500  0.271650 -0.334760   \n",
       "5146   0.673610 -0.163950 -0.009396 -0.893890  0.495360 -0.527920 -0.415260   \n",
       "1706  -0.725400  0.423260  0.119350 -0.166350 -0.208910  0.229740 -0.543080   \n",
       "1519   0.236630  0.391990  0.707590 -0.237230  0.362190  0.408210 -0.418480   \n",
       "2651   0.320810 -0.077240  0.539530 -0.241470  0.432870  0.126140 -0.488340   \n",
       "5834   0.275360  0.441140  0.706610 -0.644220 -0.334720  0.210850 -0.176110   \n",
       "2565   0.236310  0.165770 -0.138440 -0.103350 -0.040206 -0.123350  0.463730   \n",
       "17452  0.256320  0.252850  0.647020 -0.626280  0.103030  0.301890 -0.230690   \n",
       "5764   0.247110  0.263190  0.676910 -0.625620  0.066835  0.326600 -0.230250   \n",
       "1833   0.427730  0.241830  0.828310 -0.153750  0.187970  0.457500 -0.405750   \n",
       "4638   0.280380  0.443990  0.766750 -0.656840 -0.364610  0.215200 -0.119410   \n",
       "4975   0.197750  0.384350  0.684340 -0.673410 -0.178250  0.393750 -0.211430   \n",
       "4130   0.727960 -0.249140  0.168210  0.149250  0.337670 -0.236520  0.014404   \n",
       "9717   0.039835  0.422640  0.625430 -0.492210 -0.355090  0.212780 -0.245020   \n",
       "6210   0.464280  0.391530  0.739870 -0.835530 -0.095523 -0.055842 -0.361770   \n",
       "4034   0.045519  0.409280  0.593950 -0.550490 -0.292610  0.106800 -0.358620   \n",
       "15328  0.445810  0.538820  0.751500 -0.878770 -0.015895 -0.237410 -0.506730   \n",
       "5100   1.081100 -1.050300 -0.819230 -0.128290  1.513600  0.293170  0.513080   \n",
       "17370  0.427840  0.488960  0.785010 -0.886970 -0.033815 -0.198020 -0.513490   \n",
       "1664   0.310230 -0.124000  0.585340 -0.251130  0.318630  0.138010 -0.451610   \n",
       "3457   0.434850  0.501280  0.823410 -0.911560 -0.048018 -0.234150 -0.495260   \n",
       "7709   0.484560  0.525470  0.807220 -0.904590 -0.023278 -0.240320 -0.565310   \n",
       "832    0.244340  0.170360 -0.072841 -0.818730  0.029379  0.107710 -0.313400   \n",
       "16415  0.491890  0.517230  0.771830 -0.911690 -0.012968 -0.268920 -0.557810   \n",
       "3485   1.207500  0.402120  0.374460 -0.225620  0.154880 -0.613170  0.025380   \n",
       "1828   0.329070  0.176260  0.219680 -0.373850  0.430080 -0.067465 -0.306190   \n",
       "2852   0.164770  0.006425  0.282420 -0.346210  0.285890 -0.001282 -0.251310   \n",
       "\n",
       "       ...       92        93        94        95        96        97   \\\n",
       "5128   ... -0.074601  0.100630 -0.584120  0.474680 -0.276830  0.519280   \n",
       "16906  ...  0.064793  0.298160 -0.536720  0.797080  0.161680 -0.094865   \n",
       "6037   ...  0.120320 -0.133850 -0.484600  0.476060 -0.020432  0.147540   \n",
       "9235   ...  0.302040 -0.366680 -0.067651  0.420240 -0.320750 -0.342030   \n",
       "6439   ... -0.127140  0.243270 -0.493600  0.749870  0.413340 -0.286370   \n",
       "14000  ...  0.000779 -0.035394 -0.527440  0.517770 -0.144500 -0.009809   \n",
       "4433   ... -0.074266 -0.163830 -0.520610  0.834990  0.340570 -0.211410   \n",
       "5984   ...  0.104210  0.051946 -0.315170  0.505240 -0.151040 -0.156950   \n",
       "16421  ...  0.320670  0.172290 -0.466030  0.537150  0.077558  0.187760   \n",
       "7547   ...  0.438480 -0.919770  0.043590  0.726340 -0.170780 -0.501150   \n",
       "600    ...  0.195580 -0.612050 -0.258890  0.202120 -0.069473 -0.343810   \n",
       "3043   ...  0.577630 -0.494280 -0.599140  0.524450 -0.330870 -0.628520   \n",
       "6042   ...  0.611730 -0.281780 -0.228160  0.226770 -0.292150  0.087501   \n",
       "2918   ... -0.085940 -0.661050 -0.832220 -0.037020  0.002793  0.172750   \n",
       "8212   ...  0.962390 -0.272760 -0.043752 -0.056918 -0.524680 -0.069396   \n",
       "5757   ...  0.266760  0.051816 -0.149290  0.556910  0.139170  0.039150   \n",
       "2082   ... -0.011072 -0.624740 -1.244000  0.444220  0.094792  0.235260   \n",
       "14857  ...  0.007994  0.022141 -0.369650  0.624300  0.385850 -0.194670   \n",
       "3507   ...  0.052120 -0.080362 -0.380380  0.578050  0.063761  0.266530   \n",
       "7366   ...  0.450050  0.042515  0.012705  0.430390 -0.812640 -0.034013   \n",
       "7892   ... -0.056485 -0.635160 -0.908320  0.585410 -0.081083 -0.205710   \n",
       "7310   ... -0.215970 -0.096354 -0.354610  0.573400  0.134710  0.236850   \n",
       "6112   ...  0.083677 -0.048367 -0.452550  0.714230  0.151420  0.063321   \n",
       "4709   ...  0.108570  0.156610 -0.071942  0.630250  0.068402 -0.080349   \n",
       "9168   ...  0.424490 -0.321400  0.454570 -0.017991 -0.511970 -0.005566   \n",
       "15532  ...  0.044062  0.138060 -0.414760  0.573210 -0.245650  0.260930   \n",
       "4354   ...  0.155370 -0.283810  0.095883 -0.012208 -0.345910 -0.000313   \n",
       "8407   ...  0.439240 -0.328770  0.047198  0.455770 -0.136700 -0.125360   \n",
       "4985   ...  0.689730 -0.515850 -0.017471  0.292000 -0.327980 -0.411640   \n",
       "7422   ...  0.449760  0.116690 -0.132040 -0.116650 -0.691010 -0.347260   \n",
       "...    ...       ...       ...       ...       ...       ...       ...   \n",
       "14260  ... -0.004191 -0.529450  0.192950  0.223590 -0.821770  0.543290   \n",
       "15610  ... -0.218880 -0.248630 -0.162820 -0.766710 -0.687330  0.690010   \n",
       "3033   ... -0.401750 -0.152100  0.613700 -0.521040 -0.181870  0.431400   \n",
       "16106  ... -0.094517 -0.165200 -0.148510 -0.407590 -0.644110  0.742360   \n",
       "5146   ... -0.372660 -0.090349  0.822390 -0.257500 -0.337880  0.469250   \n",
       "1706   ...  0.795430 -0.941590 -0.482060  0.669740 -0.160050 -0.256730   \n",
       "1519   ... -0.136810 -0.142210 -0.394110  0.067704 -0.089378  0.108110   \n",
       "2651   ... -0.621040 -0.056140  0.132720 -0.057954 -0.364720 -0.001400   \n",
       "5834   ... -0.001522 -0.139770 -0.201440 -0.660420 -0.725420  0.919310   \n",
       "2565   ... -0.091813  0.550900  0.215580 -0.398790 -0.365160  0.846540   \n",
       "17452  ... -0.110440 -0.152120 -0.267190 -0.690860 -0.740330  0.707320   \n",
       "5764   ... -0.154910 -0.148190 -0.203930 -0.665880 -0.729880  0.733620   \n",
       "1833   ... -0.334250  0.199350 -0.127340 -0.051066 -0.363880  0.074221   \n",
       "4638   ...  0.018959 -0.082507 -0.231980 -0.688660 -0.662690  0.949290   \n",
       "4975   ... -0.173900 -0.002723 -0.161130 -0.623620 -0.576070  0.819050   \n",
       "4130   ...  0.087687  0.301840 -0.076565  0.216580 -0.086369 -0.289820   \n",
       "9717   ... -0.146560 -0.036669 -0.037425 -0.401850 -0.716800  0.844410   \n",
       "6210   ... -0.128070 -0.268570 -0.096458 -0.635770 -0.709030  0.782790   \n",
       "4034   ... -0.201050 -0.023162 -0.001108 -0.420300 -0.720380  0.815510   \n",
       "15328  ...  0.160340 -0.506630 -0.248550 -0.610500 -0.573550  0.600760   \n",
       "5100   ...  0.470160 -1.026400  0.433050  0.037364 -1.592000 -0.066578   \n",
       "17370  ...  0.140750 -0.458270 -0.260290 -0.633000 -0.583480  0.635640   \n",
       "1664   ... -0.675910  0.043466  0.159720 -0.060490 -0.342590  0.061215   \n",
       "3457   ...  0.175150 -0.422110 -0.244800 -0.547600 -0.528970  0.642580   \n",
       "7709   ...  0.123710 -0.451190 -0.208910 -0.551400 -0.589090  0.641340   \n",
       "832    ... -0.211830  0.037793  0.005665  0.436800 -0.259920  0.176610   \n",
       "16415  ...  0.143630 -0.440630 -0.254470 -0.569120 -0.621220  0.630400   \n",
       "3485   ...  0.221420  0.032765  0.331680  0.427440 -0.361120  0.415420   \n",
       "1828   ... -0.234650 -0.086584  0.126970 -0.224030 -0.633190 -0.051605   \n",
       "2852   ... -0.426310  0.065890  0.261690 -0.266210 -0.638100  0.081216   \n",
       "\n",
       "            98        99        100  101  \n",
       "5128  -0.062354 -0.822050 -0.577450  NaN  \n",
       "16906  0.024814 -1.224600 -0.725510  NaN  \n",
       "6037  -0.251300 -0.716380 -0.159310  NaN  \n",
       "9235  -0.439280 -0.448760 -0.037929  NaN  \n",
       "6439  -0.051256 -1.170100 -0.707940  NaN  \n",
       "14000 -0.052265 -0.915840 -0.416480  NaN  \n",
       "4433  -0.251420 -1.416200 -0.444940  NaN  \n",
       "5984   0.028734 -1.285700 -0.636860  NaN  \n",
       "16421 -0.022171 -1.058200 -0.513350  NaN  \n",
       "7547  -0.317210 -0.303260 -0.107350  NaN  \n",
       "600   -0.037812 -0.774500 -0.564460  NaN  \n",
       "3043   0.293440 -0.189690 -0.429660  NaN  \n",
       "6042  -0.615940 -0.147060 -0.217480  NaN  \n",
       "2918  -0.095076  0.489050 -0.487500  NaN  \n",
       "8212   0.109300 -0.479240 -0.025095  NaN  \n",
       "5757  -0.033328 -0.799610 -0.526270  NaN  \n",
       "2082  -0.486640 -0.900060  0.700660  NaN  \n",
       "14857 -0.348440 -1.065300 -0.385290  NaN  \n",
       "3507  -0.025403 -1.097200 -0.475860  NaN  \n",
       "7366  -0.354870 -0.456470 -0.357620  NaN  \n",
       "7892  -0.186130 -0.507870  0.475860  NaN  \n",
       "7310  -0.191980 -1.072700 -0.401110  NaN  \n",
       "6112  -0.072653 -1.380500 -0.721030  NaN  \n",
       "4709   0.033700 -1.183800 -0.629680  NaN  \n",
       "9168  -0.253070 -0.214640  0.088291  NaN  \n",
       "15532 -0.181010 -1.054000 -0.373020  NaN  \n",
       "4354   0.445440 -0.328780 -0.427070  NaN  \n",
       "8407  -0.197400 -0.538330 -0.320760  NaN  \n",
       "4985  -0.396810 -0.340620 -0.516240  NaN  \n",
       "7422  -0.197150 -0.613170 -0.473530  NaN  \n",
       "...         ...       ...       ...  ...  \n",
       "14260  0.012134 -0.156590  0.753370  NaN  \n",
       "15610 -0.085205 -0.972860  1.008100  NaN  \n",
       "3033  -0.627160 -0.194010  0.119870  NaN  \n",
       "16106 -0.066314 -0.782930  0.910540  NaN  \n",
       "5146  -0.215040 -0.827860  0.214040  NaN  \n",
       "1706  -0.046207  0.360100 -0.306730  NaN  \n",
       "1519   0.033257 -0.790460  0.123340  NaN  \n",
       "2651   0.057061 -0.216400  0.142520  NaN  \n",
       "5834  -0.089235 -0.832680  1.003800  NaN  \n",
       "2565   0.014016 -0.573680 -0.330620  NaN  \n",
       "17452 -0.015462 -0.787640  1.018900  NaN  \n",
       "5764  -0.018312 -0.824670  1.030100  NaN  \n",
       "1833   0.023650 -0.974330  0.206530  NaN  \n",
       "4638  -0.187340 -0.839300  1.016700  NaN  \n",
       "4975  -0.286020 -0.982250  1.016800  NaN  \n",
       "4130   0.293220 -0.611960  0.412680  NaN  \n",
       "9717  -0.214790 -0.945300  1.001300  NaN  \n",
       "6210  -0.061847 -1.004400  0.993380  NaN  \n",
       "4034  -0.180450 -0.991080  0.986920  NaN  \n",
       "15328  0.212070 -0.869270  1.161000  NaN  \n",
       "5100  -0.394040 -0.362830 -0.646810  NaN  \n",
       "17370  0.176200 -0.897700  1.150300  NaN  \n",
       "1664  -0.083953 -0.378680  0.122820  NaN  \n",
       "3457   0.045606 -0.897130  1.168800  NaN  \n",
       "7709   0.157690 -0.880540  1.212300  NaN  \n",
       "832   -0.234810 -0.723180  0.107300  NaN  \n",
       "16415  0.166470 -0.856460  1.183400  NaN  \n",
       "3485   0.074006 -0.004337  0.121440  NaN  \n",
       "1828  -0.050489 -0.172860  0.142200  NaN  \n",
       "2852  -0.197640 -0.266340  0.091809  NaN  \n",
       "\n",
       "[17989 rows x 102 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.sort_values(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>dolphin-tale-(2011)</td>\n",
       "      <td>0.346010</td>\n",
       "      <td>-1.21530</td>\n",
       "      <td>0.340410</td>\n",
       "      <td>0.342540</td>\n",
       "      <td>0.244270</td>\n",
       "      <td>-0.124290</td>\n",
       "      <td>0.199470</td>\n",
       "      <td>0.111490</td>\n",
       "      <td>-0.222460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.518440</td>\n",
       "      <td>-0.356960</td>\n",
       "      <td>-0.189450</td>\n",
       "      <td>-0.030467</td>\n",
       "      <td>0.153040</td>\n",
       "      <td>0.479830</td>\n",
       "      <td>-0.377510</td>\n",
       "      <td>-0.314870</td>\n",
       "      <td>0.159240</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>ballykissangel-season-4</td>\n",
       "      <td>0.294050</td>\n",
       "      <td>-1.06340</td>\n",
       "      <td>-0.509860</td>\n",
       "      <td>-0.511100</td>\n",
       "      <td>0.668010</td>\n",
       "      <td>-0.346700</td>\n",
       "      <td>0.276550</td>\n",
       "      <td>0.849240</td>\n",
       "      <td>-0.303180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.537220</td>\n",
       "      <td>-1.038800</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>0.533110</td>\n",
       "      <td>-0.734940</td>\n",
       "      <td>0.322320</td>\n",
       "      <td>-0.654090</td>\n",
       "      <td>-0.153000</td>\n",
       "      <td>-0.665370</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>magic-in-the-moonlight</td>\n",
       "      <td>0.290870</td>\n",
       "      <td>-1.02390</td>\n",
       "      <td>0.041772</td>\n",
       "      <td>0.047178</td>\n",
       "      <td>0.011426</td>\n",
       "      <td>0.503290</td>\n",
       "      <td>-0.295080</td>\n",
       "      <td>-0.024971</td>\n",
       "      <td>0.268970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573580</td>\n",
       "      <td>-0.134020</td>\n",
       "      <td>0.009749</td>\n",
       "      <td>-0.219710</td>\n",
       "      <td>0.291190</td>\n",
       "      <td>0.666080</td>\n",
       "      <td>-0.491910</td>\n",
       "      <td>-0.690620</td>\n",
       "      <td>0.032322</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>ballykissangel-season-3</td>\n",
       "      <td>0.133780</td>\n",
       "      <td>-0.99584</td>\n",
       "      <td>-0.526500</td>\n",
       "      <td>-0.530670</td>\n",
       "      <td>0.571570</td>\n",
       "      <td>-0.424000</td>\n",
       "      <td>0.269610</td>\n",
       "      <td>0.819640</td>\n",
       "      <td>-0.225040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525430</td>\n",
       "      <td>-1.036100</td>\n",
       "      <td>0.051450</td>\n",
       "      <td>0.440010</td>\n",
       "      <td>-0.677340</td>\n",
       "      <td>0.291310</td>\n",
       "      <td>-0.675990</td>\n",
       "      <td>-0.125890</td>\n",
       "      <td>-0.712250</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2535</th>\n",
       "      <td>elsa-&amp;-fred</td>\n",
       "      <td>0.153820</td>\n",
       "      <td>-0.98498</td>\n",
       "      <td>0.336290</td>\n",
       "      <td>-0.416870</td>\n",
       "      <td>-0.156080</td>\n",
       "      <td>-0.085197</td>\n",
       "      <td>0.190580</td>\n",
       "      <td>-0.083176</td>\n",
       "      <td>0.181380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.339600</td>\n",
       "      <td>-0.258520</td>\n",
       "      <td>0.193290</td>\n",
       "      <td>-0.299680</td>\n",
       "      <td>0.021829</td>\n",
       "      <td>0.245140</td>\n",
       "      <td>-0.679390</td>\n",
       "      <td>-0.238150</td>\n",
       "      <td>0.104180</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7084</th>\n",
       "      <td>parenthood</td>\n",
       "      <td>-0.020399</td>\n",
       "      <td>-0.97926</td>\n",
       "      <td>0.530570</td>\n",
       "      <td>-0.290400</td>\n",
       "      <td>0.303190</td>\n",
       "      <td>-0.433550</td>\n",
       "      <td>0.216080</td>\n",
       "      <td>0.907090</td>\n",
       "      <td>0.174200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.131430</td>\n",
       "      <td>-0.219640</td>\n",
       "      <td>-0.270230</td>\n",
       "      <td>0.049698</td>\n",
       "      <td>0.345020</td>\n",
       "      <td>0.504330</td>\n",
       "      <td>-0.589540</td>\n",
       "      <td>-0.728440</td>\n",
       "      <td>0.266360</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1670</th>\n",
       "      <td>ballykissangel-season-5</td>\n",
       "      <td>0.234530</td>\n",
       "      <td>-0.97406</td>\n",
       "      <td>-0.543030</td>\n",
       "      <td>-0.511390</td>\n",
       "      <td>0.702520</td>\n",
       "      <td>-0.333430</td>\n",
       "      <td>0.241710</td>\n",
       "      <td>0.837990</td>\n",
       "      <td>-0.357520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536210</td>\n",
       "      <td>-1.081400</td>\n",
       "      <td>0.102900</td>\n",
       "      <td>0.565180</td>\n",
       "      <td>-0.662990</td>\n",
       "      <td>0.322180</td>\n",
       "      <td>-0.661990</td>\n",
       "      <td>-0.201630</td>\n",
       "      <td>-0.620240</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>ballykissangel-season-6</td>\n",
       "      <td>0.192480</td>\n",
       "      <td>-0.97108</td>\n",
       "      <td>-0.644340</td>\n",
       "      <td>-0.557940</td>\n",
       "      <td>0.770590</td>\n",
       "      <td>-0.378840</td>\n",
       "      <td>0.232130</td>\n",
       "      <td>0.898700</td>\n",
       "      <td>-0.240380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.584620</td>\n",
       "      <td>-1.022500</td>\n",
       "      <td>0.017666</td>\n",
       "      <td>0.551760</td>\n",
       "      <td>-0.543840</td>\n",
       "      <td>0.312710</td>\n",
       "      <td>-0.878890</td>\n",
       "      <td>-0.186210</td>\n",
       "      <td>-0.691700</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1744</th>\n",
       "      <td>_duplicate_dolphin-tale-2</td>\n",
       "      <td>0.198280</td>\n",
       "      <td>-0.95839</td>\n",
       "      <td>0.067340</td>\n",
       "      <td>0.259550</td>\n",
       "      <td>0.152170</td>\n",
       "      <td>0.018251</td>\n",
       "      <td>0.424540</td>\n",
       "      <td>0.073161</td>\n",
       "      <td>0.297960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507800</td>\n",
       "      <td>-0.243910</td>\n",
       "      <td>0.287240</td>\n",
       "      <td>-0.223820</td>\n",
       "      <td>-0.021888</td>\n",
       "      <td>0.409410</td>\n",
       "      <td>-0.467230</td>\n",
       "      <td>-0.399190</td>\n",
       "      <td>-0.153930</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6390</th>\n",
       "      <td>dolphin-tale-2</td>\n",
       "      <td>0.464780</td>\n",
       "      <td>-0.94600</td>\n",
       "      <td>0.362550</td>\n",
       "      <td>0.284970</td>\n",
       "      <td>0.105530</td>\n",
       "      <td>-0.097234</td>\n",
       "      <td>0.153650</td>\n",
       "      <td>0.186220</td>\n",
       "      <td>-0.073721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.472220</td>\n",
       "      <td>-0.265960</td>\n",
       "      <td>-0.011108</td>\n",
       "      <td>0.102090</td>\n",
       "      <td>-0.093313</td>\n",
       "      <td>0.695400</td>\n",
       "      <td>-0.725310</td>\n",
       "      <td>-0.269940</td>\n",
       "      <td>0.165530</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>annie</td>\n",
       "      <td>0.153970</td>\n",
       "      <td>-0.94488</td>\n",
       "      <td>-0.062939</td>\n",
       "      <td>0.445840</td>\n",
       "      <td>0.191720</td>\n",
       "      <td>0.436570</td>\n",
       "      <td>0.418390</td>\n",
       "      <td>0.064427</td>\n",
       "      <td>-0.247860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059718</td>\n",
       "      <td>0.252790</td>\n",
       "      <td>0.454990</td>\n",
       "      <td>0.424590</td>\n",
       "      <td>0.294980</td>\n",
       "      <td>0.318200</td>\n",
       "      <td>0.041725</td>\n",
       "      <td>-0.517670</td>\n",
       "      <td>-0.267330</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>when-the-game-stands-tall</td>\n",
       "      <td>0.316720</td>\n",
       "      <td>-0.94254</td>\n",
       "      <td>0.085712</td>\n",
       "      <td>0.375330</td>\n",
       "      <td>0.122100</td>\n",
       "      <td>0.257720</td>\n",
       "      <td>0.523790</td>\n",
       "      <td>0.051536</td>\n",
       "      <td>0.341050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091834</td>\n",
       "      <td>0.085982</td>\n",
       "      <td>0.565360</td>\n",
       "      <td>0.670160</td>\n",
       "      <td>0.095864</td>\n",
       "      <td>0.115820</td>\n",
       "      <td>-0.481080</td>\n",
       "      <td>-0.624500</td>\n",
       "      <td>0.430660</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2981</th>\n",
       "      <td>two-weeks-notice</td>\n",
       "      <td>-0.132450</td>\n",
       "      <td>-0.91895</td>\n",
       "      <td>0.361600</td>\n",
       "      <td>0.022252</td>\n",
       "      <td>0.236440</td>\n",
       "      <td>-0.631070</td>\n",
       "      <td>0.638880</td>\n",
       "      <td>0.308370</td>\n",
       "      <td>-0.185250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.611070</td>\n",
       "      <td>-0.124620</td>\n",
       "      <td>-0.035273</td>\n",
       "      <td>-0.061694</td>\n",
       "      <td>-0.054208</td>\n",
       "      <td>0.095180</td>\n",
       "      <td>0.058807</td>\n",
       "      <td>-0.530480</td>\n",
       "      <td>-0.213710</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3216</th>\n",
       "      <td>larry-crowne</td>\n",
       "      <td>-0.206660</td>\n",
       "      <td>-0.91711</td>\n",
       "      <td>0.072095</td>\n",
       "      <td>-0.470040</td>\n",
       "      <td>0.048091</td>\n",
       "      <td>-0.826470</td>\n",
       "      <td>-0.201500</td>\n",
       "      <td>0.032002</td>\n",
       "      <td>-0.155420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.715600</td>\n",
       "      <td>-0.210930</td>\n",
       "      <td>-0.040540</td>\n",
       "      <td>-0.291160</td>\n",
       "      <td>0.333550</td>\n",
       "      <td>0.096930</td>\n",
       "      <td>-0.191050</td>\n",
       "      <td>-0.134230</td>\n",
       "      <td>-0.182190</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>22-jump-street</td>\n",
       "      <td>0.676620</td>\n",
       "      <td>-0.91498</td>\n",
       "      <td>0.196700</td>\n",
       "      <td>0.169690</td>\n",
       "      <td>0.333210</td>\n",
       "      <td>0.099109</td>\n",
       "      <td>0.324770</td>\n",
       "      <td>-0.081610</td>\n",
       "      <td>0.279850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106810</td>\n",
       "      <td>-0.270520</td>\n",
       "      <td>-0.229700</td>\n",
       "      <td>-0.012904</td>\n",
       "      <td>0.009360</td>\n",
       "      <td>-0.085899</td>\n",
       "      <td>0.139860</td>\n",
       "      <td>-0.815420</td>\n",
       "      <td>-0.270420</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>the-best-of-me</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.91143</td>\n",
       "      <td>0.810330</td>\n",
       "      <td>0.343120</td>\n",
       "      <td>0.434360</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.402960</td>\n",
       "      <td>-0.286130</td>\n",
       "      <td>0.271720</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.506200</td>\n",
       "      <td>0.284910</td>\n",
       "      <td>0.582150</td>\n",
       "      <td>0.188740</td>\n",
       "      <td>-0.425150</td>\n",
       "      <td>0.330020</td>\n",
       "      <td>-0.921770</td>\n",
       "      <td>-1.167200</td>\n",
       "      <td>0.062326</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>ballykissangel-season-1</td>\n",
       "      <td>0.111000</td>\n",
       "      <td>-0.91099</td>\n",
       "      <td>-0.578590</td>\n",
       "      <td>-0.501090</td>\n",
       "      <td>0.612570</td>\n",
       "      <td>-0.318790</td>\n",
       "      <td>0.336380</td>\n",
       "      <td>0.882090</td>\n",
       "      <td>-0.258430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600910</td>\n",
       "      <td>-1.169900</td>\n",
       "      <td>0.010465</td>\n",
       "      <td>0.401500</td>\n",
       "      <td>-0.603370</td>\n",
       "      <td>0.213050</td>\n",
       "      <td>-0.544310</td>\n",
       "      <td>-0.106420</td>\n",
       "      <td>-0.682050</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1706</th>\n",
       "      <td>battlestar-galactica-season-4</td>\n",
       "      <td>1.183200</td>\n",
       "      <td>-0.90504</td>\n",
       "      <td>-0.725400</td>\n",
       "      <td>0.423260</td>\n",
       "      <td>0.119350</td>\n",
       "      <td>-0.166350</td>\n",
       "      <td>-0.208910</td>\n",
       "      <td>0.229740</td>\n",
       "      <td>-0.543080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.795430</td>\n",
       "      <td>-0.941590</td>\n",
       "      <td>-0.482060</td>\n",
       "      <td>0.669740</td>\n",
       "      <td>-0.160050</td>\n",
       "      <td>-0.256730</td>\n",
       "      <td>-0.046207</td>\n",
       "      <td>0.360100</td>\n",
       "      <td>-0.306730</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2033</th>\n",
       "      <td>battlestar-galactica-season-3</td>\n",
       "      <td>1.055900</td>\n",
       "      <td>-0.89351</td>\n",
       "      <td>-0.720050</td>\n",
       "      <td>0.412110</td>\n",
       "      <td>0.085968</td>\n",
       "      <td>-0.212000</td>\n",
       "      <td>-0.227350</td>\n",
       "      <td>0.242560</td>\n",
       "      <td>-0.462240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.775890</td>\n",
       "      <td>-0.953010</td>\n",
       "      <td>-0.472710</td>\n",
       "      <td>0.628750</td>\n",
       "      <td>-0.159470</td>\n",
       "      <td>-0.245450</td>\n",
       "      <td>-0.070093</td>\n",
       "      <td>0.338980</td>\n",
       "      <td>-0.306950</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>ballykissangel-season-2</td>\n",
       "      <td>0.187260</td>\n",
       "      <td>-0.85399</td>\n",
       "      <td>-0.631520</td>\n",
       "      <td>-0.587410</td>\n",
       "      <td>0.691380</td>\n",
       "      <td>-0.365080</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.932310</td>\n",
       "      <td>-0.279610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.539080</td>\n",
       "      <td>-1.121900</td>\n",
       "      <td>-0.002220</td>\n",
       "      <td>0.402740</td>\n",
       "      <td>-0.638550</td>\n",
       "      <td>0.251080</td>\n",
       "      <td>-0.646130</td>\n",
       "      <td>-0.151480</td>\n",
       "      <td>-0.699530</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4515</th>\n",
       "      <td>19-kids-and-counting-season-15</td>\n",
       "      <td>0.373040</td>\n",
       "      <td>-0.85399</td>\n",
       "      <td>0.326240</td>\n",
       "      <td>-0.067104</td>\n",
       "      <td>0.411840</td>\n",
       "      <td>-0.430930</td>\n",
       "      <td>-0.272100</td>\n",
       "      <td>0.491620</td>\n",
       "      <td>-0.116690</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.289770</td>\n",
       "      <td>-0.313080</td>\n",
       "      <td>-0.028944</td>\n",
       "      <td>0.623300</td>\n",
       "      <td>-0.473230</td>\n",
       "      <td>1.073500</td>\n",
       "      <td>-0.567790</td>\n",
       "      <td>-0.732140</td>\n",
       "      <td>0.534450</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>the-homesman</td>\n",
       "      <td>0.441740</td>\n",
       "      <td>-0.85202</td>\n",
       "      <td>-0.263110</td>\n",
       "      <td>0.289150</td>\n",
       "      <td>0.213340</td>\n",
       "      <td>0.468460</td>\n",
       "      <td>0.126710</td>\n",
       "      <td>-0.326740</td>\n",
       "      <td>0.250200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.275920</td>\n",
       "      <td>0.222660</td>\n",
       "      <td>0.458570</td>\n",
       "      <td>0.054106</td>\n",
       "      <td>-0.345000</td>\n",
       "      <td>-0.219680</td>\n",
       "      <td>-0.553560</td>\n",
       "      <td>-0.449300</td>\n",
       "      <td>-0.126700</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>and-so-it-goes</td>\n",
       "      <td>0.590990</td>\n",
       "      <td>-0.85195</td>\n",
       "      <td>0.065879</td>\n",
       "      <td>0.262450</td>\n",
       "      <td>-0.009538</td>\n",
       "      <td>0.034869</td>\n",
       "      <td>0.054148</td>\n",
       "      <td>-0.129880</td>\n",
       "      <td>0.291870</td>\n",
       "      <td>...</td>\n",
       "      <td>0.351060</td>\n",
       "      <td>-0.050863</td>\n",
       "      <td>0.392520</td>\n",
       "      <td>0.143330</td>\n",
       "      <td>-0.131070</td>\n",
       "      <td>0.009270</td>\n",
       "      <td>-0.729080</td>\n",
       "      <td>-0.600290</td>\n",
       "      <td>-0.097227</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>battlestar-galactica-season-1</td>\n",
       "      <td>1.071700</td>\n",
       "      <td>-0.84397</td>\n",
       "      <td>-0.784330</td>\n",
       "      <td>0.449860</td>\n",
       "      <td>0.089941</td>\n",
       "      <td>-0.122500</td>\n",
       "      <td>-0.216270</td>\n",
       "      <td>0.270300</td>\n",
       "      <td>-0.474800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816370</td>\n",
       "      <td>-1.028700</td>\n",
       "      <td>-0.477300</td>\n",
       "      <td>0.618150</td>\n",
       "      <td>-0.096168</td>\n",
       "      <td>-0.296730</td>\n",
       "      <td>0.022697</td>\n",
       "      <td>0.337690</td>\n",
       "      <td>-0.332930</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3712</th>\n",
       "      <td>19-kids-and-counting-season-14</td>\n",
       "      <td>0.279580</td>\n",
       "      <td>-0.82941</td>\n",
       "      <td>0.101440</td>\n",
       "      <td>0.007456</td>\n",
       "      <td>0.376350</td>\n",
       "      <td>-0.431700</td>\n",
       "      <td>-0.284960</td>\n",
       "      <td>0.296750</td>\n",
       "      <td>-0.022661</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.375810</td>\n",
       "      <td>-0.351220</td>\n",
       "      <td>-0.119080</td>\n",
       "      <td>0.627980</td>\n",
       "      <td>-0.469140</td>\n",
       "      <td>1.164000</td>\n",
       "      <td>-0.582140</td>\n",
       "      <td>-0.732270</td>\n",
       "      <td>0.499270</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>night-at-the-museum</td>\n",
       "      <td>0.542780</td>\n",
       "      <td>-0.82645</td>\n",
       "      <td>-0.065089</td>\n",
       "      <td>0.353120</td>\n",
       "      <td>0.509560</td>\n",
       "      <td>0.144360</td>\n",
       "      <td>0.255680</td>\n",
       "      <td>0.693270</td>\n",
       "      <td>0.367980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676880</td>\n",
       "      <td>-0.048863</td>\n",
       "      <td>-0.086648</td>\n",
       "      <td>-0.028571</td>\n",
       "      <td>-0.079083</td>\n",
       "      <td>0.560210</td>\n",
       "      <td>-0.249130</td>\n",
       "      <td>-0.049173</td>\n",
       "      <td>-0.320910</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>unknown-(2011)</td>\n",
       "      <td>0.073504</td>\n",
       "      <td>-0.82643</td>\n",
       "      <td>0.081969</td>\n",
       "      <td>0.229590</td>\n",
       "      <td>0.258430</td>\n",
       "      <td>-0.544410</td>\n",
       "      <td>-0.369740</td>\n",
       "      <td>-1.028000</td>\n",
       "      <td>0.463530</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.148630</td>\n",
       "      <td>-0.401930</td>\n",
       "      <td>0.133940</td>\n",
       "      <td>-0.230820</td>\n",
       "      <td>0.111490</td>\n",
       "      <td>0.091364</td>\n",
       "      <td>0.053081</td>\n",
       "      <td>-0.074049</td>\n",
       "      <td>-0.343980</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>21-jump-street</td>\n",
       "      <td>0.749020</td>\n",
       "      <td>-0.80574</td>\n",
       "      <td>0.472570</td>\n",
       "      <td>-0.038414</td>\n",
       "      <td>0.187420</td>\n",
       "      <td>-0.311080</td>\n",
       "      <td>0.230690</td>\n",
       "      <td>-0.395380</td>\n",
       "      <td>0.331470</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.100300</td>\n",
       "      <td>-0.504120</td>\n",
       "      <td>-0.603230</td>\n",
       "      <td>-0.317380</td>\n",
       "      <td>0.059952</td>\n",
       "      <td>-0.140950</td>\n",
       "      <td>0.295240</td>\n",
       "      <td>-0.320220</td>\n",
       "      <td>-0.392600</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7267</th>\n",
       "      <td>19-kids-and-counting-season-12</td>\n",
       "      <td>0.273110</td>\n",
       "      <td>-0.80499</td>\n",
       "      <td>0.178160</td>\n",
       "      <td>0.031623</td>\n",
       "      <td>0.445770</td>\n",
       "      <td>-0.605280</td>\n",
       "      <td>-0.256280</td>\n",
       "      <td>0.393220</td>\n",
       "      <td>-0.102620</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.290450</td>\n",
       "      <td>-0.384140</td>\n",
       "      <td>-0.068507</td>\n",
       "      <td>0.543350</td>\n",
       "      <td>-0.538350</td>\n",
       "      <td>0.994240</td>\n",
       "      <td>-0.536870</td>\n",
       "      <td>-0.649750</td>\n",
       "      <td>0.585930</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>the-hundred-foot-journey-(theatrical)</td>\n",
       "      <td>0.348380</td>\n",
       "      <td>-0.80358</td>\n",
       "      <td>-0.039238</td>\n",
       "      <td>0.194060</td>\n",
       "      <td>-0.230350</td>\n",
       "      <td>0.327890</td>\n",
       "      <td>0.467510</td>\n",
       "      <td>-0.053023</td>\n",
       "      <td>0.339510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.379060</td>\n",
       "      <td>-0.412890</td>\n",
       "      <td>0.135850</td>\n",
       "      <td>-0.023189</td>\n",
       "      <td>0.320610</td>\n",
       "      <td>0.129620</td>\n",
       "      <td>-0.418920</td>\n",
       "      <td>-0.341420</td>\n",
       "      <td>0.057606</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10980</th>\n",
       "      <td>the-house-of-the-devil</td>\n",
       "      <td>0.653120</td>\n",
       "      <td>1.21840</td>\n",
       "      <td>0.821840</td>\n",
       "      <td>-0.081862</td>\n",
       "      <td>0.274490</td>\n",
       "      <td>-0.390090</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>-0.254810</td>\n",
       "      <td>-0.344880</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.366080</td>\n",
       "      <td>-0.099618</td>\n",
       "      <td>-0.122710</td>\n",
       "      <td>0.045520</td>\n",
       "      <td>-0.633030</td>\n",
       "      <td>0.474350</td>\n",
       "      <td>-0.422060</td>\n",
       "      <td>0.058069</td>\n",
       "      <td>-0.020305</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9545</th>\n",
       "      <td>house-of-seven-corpses---digitally-remastered</td>\n",
       "      <td>-0.287750</td>\n",
       "      <td>1.21910</td>\n",
       "      <td>0.220880</td>\n",
       "      <td>-0.358760</td>\n",
       "      <td>0.110030</td>\n",
       "      <td>-0.102660</td>\n",
       "      <td>0.650560</td>\n",
       "      <td>0.101980</td>\n",
       "      <td>0.052501</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.198170</td>\n",
       "      <td>0.098282</td>\n",
       "      <td>0.241600</td>\n",
       "      <td>0.135420</td>\n",
       "      <td>-1.011700</td>\n",
       "      <td>0.069292</td>\n",
       "      <td>-0.239500</td>\n",
       "      <td>-0.014740</td>\n",
       "      <td>-0.115080</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5078</th>\n",
       "      <td>cockneys-vs-zombies</td>\n",
       "      <td>0.434460</td>\n",
       "      <td>1.22770</td>\n",
       "      <td>0.360630</td>\n",
       "      <td>-0.306820</td>\n",
       "      <td>0.678790</td>\n",
       "      <td>-0.409140</td>\n",
       "      <td>0.112300</td>\n",
       "      <td>0.127970</td>\n",
       "      <td>-0.732510</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075561</td>\n",
       "      <td>-0.246390</td>\n",
       "      <td>0.335780</td>\n",
       "      <td>0.305320</td>\n",
       "      <td>-0.263370</td>\n",
       "      <td>0.148430</td>\n",
       "      <td>0.056520</td>\n",
       "      <td>-0.230540</td>\n",
       "      <td>0.175220</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14117</th>\n",
       "      <td>dead-end-road</td>\n",
       "      <td>0.276270</td>\n",
       "      <td>1.23440</td>\n",
       "      <td>0.308260</td>\n",
       "      <td>0.063378</td>\n",
       "      <td>0.261360</td>\n",
       "      <td>-0.525040</td>\n",
       "      <td>0.052046</td>\n",
       "      <td>-0.313790</td>\n",
       "      <td>-0.360550</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006900</td>\n",
       "      <td>0.182360</td>\n",
       "      <td>0.129280</td>\n",
       "      <td>0.276290</td>\n",
       "      <td>-0.860920</td>\n",
       "      <td>0.411430</td>\n",
       "      <td>-0.132440</td>\n",
       "      <td>0.057932</td>\n",
       "      <td>0.014218</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10805</th>\n",
       "      <td>the-poisoning</td>\n",
       "      <td>0.318350</td>\n",
       "      <td>1.23690</td>\n",
       "      <td>0.620960</td>\n",
       "      <td>-0.265810</td>\n",
       "      <td>0.220390</td>\n",
       "      <td>-0.834200</td>\n",
       "      <td>-0.110180</td>\n",
       "      <td>-0.574520</td>\n",
       "      <td>-0.379230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058495</td>\n",
       "      <td>0.577340</td>\n",
       "      <td>0.304420</td>\n",
       "      <td>-0.014835</td>\n",
       "      <td>-0.776210</td>\n",
       "      <td>0.557250</td>\n",
       "      <td>0.110740</td>\n",
       "      <td>-0.004656</td>\n",
       "      <td>-0.519130</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5327</th>\n",
       "      <td>dark-spirits</td>\n",
       "      <td>0.229590</td>\n",
       "      <td>1.23720</td>\n",
       "      <td>-0.115510</td>\n",
       "      <td>-0.158250</td>\n",
       "      <td>0.402220</td>\n",
       "      <td>-0.528920</td>\n",
       "      <td>0.466590</td>\n",
       "      <td>-0.285090</td>\n",
       "      <td>-0.275170</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.234990</td>\n",
       "      <td>-0.061648</td>\n",
       "      <td>0.220050</td>\n",
       "      <td>-0.029688</td>\n",
       "      <td>-0.754250</td>\n",
       "      <td>-0.069912</td>\n",
       "      <td>0.152440</td>\n",
       "      <td>0.216670</td>\n",
       "      <td>-0.169210</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7727</th>\n",
       "      <td>babysitter-massacre</td>\n",
       "      <td>-0.068865</td>\n",
       "      <td>1.24140</td>\n",
       "      <td>0.609320</td>\n",
       "      <td>-0.226250</td>\n",
       "      <td>0.463110</td>\n",
       "      <td>-0.311060</td>\n",
       "      <td>-0.028434</td>\n",
       "      <td>-0.667130</td>\n",
       "      <td>-0.586830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.331570</td>\n",
       "      <td>0.065704</td>\n",
       "      <td>-0.136800</td>\n",
       "      <td>0.185720</td>\n",
       "      <td>-0.915480</td>\n",
       "      <td>0.019702</td>\n",
       "      <td>0.413630</td>\n",
       "      <td>-0.137720</td>\n",
       "      <td>-0.141800</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10468</th>\n",
       "      <td>the-mirror</td>\n",
       "      <td>0.632140</td>\n",
       "      <td>1.24220</td>\n",
       "      <td>0.344830</td>\n",
       "      <td>0.270070</td>\n",
       "      <td>0.300360</td>\n",
       "      <td>-0.596070</td>\n",
       "      <td>-0.024114</td>\n",
       "      <td>-0.174280</td>\n",
       "      <td>0.206920</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.129700</td>\n",
       "      <td>0.577660</td>\n",
       "      <td>0.294040</td>\n",
       "      <td>0.435910</td>\n",
       "      <td>-0.915000</td>\n",
       "      <td>0.606460</td>\n",
       "      <td>-0.465340</td>\n",
       "      <td>-0.103170</td>\n",
       "      <td>-0.854010</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12225</th>\n",
       "      <td>house-of-boys</td>\n",
       "      <td>0.185350</td>\n",
       "      <td>1.25210</td>\n",
       "      <td>0.983610</td>\n",
       "      <td>-0.390620</td>\n",
       "      <td>0.062326</td>\n",
       "      <td>-0.666710</td>\n",
       "      <td>0.494770</td>\n",
       "      <td>-0.191150</td>\n",
       "      <td>-0.330830</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.200100</td>\n",
       "      <td>-0.043678</td>\n",
       "      <td>-0.310000</td>\n",
       "      <td>-0.154920</td>\n",
       "      <td>-0.193000</td>\n",
       "      <td>-0.199740</td>\n",
       "      <td>-0.183080</td>\n",
       "      <td>-0.161110</td>\n",
       "      <td>0.244080</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5756</th>\n",
       "      <td>american-ghost-hunter</td>\n",
       "      <td>0.458570</td>\n",
       "      <td>1.25820</td>\n",
       "      <td>0.045722</td>\n",
       "      <td>-0.417780</td>\n",
       "      <td>0.356100</td>\n",
       "      <td>-0.227020</td>\n",
       "      <td>0.267290</td>\n",
       "      <td>-0.531090</td>\n",
       "      <td>-0.076302</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.609130</td>\n",
       "      <td>0.319520</td>\n",
       "      <td>0.777950</td>\n",
       "      <td>0.320020</td>\n",
       "      <td>-1.048500</td>\n",
       "      <td>0.612760</td>\n",
       "      <td>0.173380</td>\n",
       "      <td>-0.224510</td>\n",
       "      <td>-0.317890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11451</th>\n",
       "      <td>shock</td>\n",
       "      <td>-0.124160</td>\n",
       "      <td>1.26160</td>\n",
       "      <td>0.428390</td>\n",
       "      <td>-0.345890</td>\n",
       "      <td>0.130740</td>\n",
       "      <td>-0.445970</td>\n",
       "      <td>-0.079599</td>\n",
       "      <td>-0.005649</td>\n",
       "      <td>-0.267340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.315050</td>\n",
       "      <td>-0.178040</td>\n",
       "      <td>0.183740</td>\n",
       "      <td>0.248750</td>\n",
       "      <td>-0.736380</td>\n",
       "      <td>0.703360</td>\n",
       "      <td>-0.395910</td>\n",
       "      <td>-0.011004</td>\n",
       "      <td>-0.222410</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11441</th>\n",
       "      <td>haunted</td>\n",
       "      <td>0.515690</td>\n",
       "      <td>1.26660</td>\n",
       "      <td>0.468900</td>\n",
       "      <td>-0.600770</td>\n",
       "      <td>-0.123510</td>\n",
       "      <td>-0.708450</td>\n",
       "      <td>0.684740</td>\n",
       "      <td>-0.364210</td>\n",
       "      <td>-0.551660</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.646810</td>\n",
       "      <td>0.142460</td>\n",
       "      <td>0.874660</td>\n",
       "      <td>0.223050</td>\n",
       "      <td>-0.986230</td>\n",
       "      <td>0.741740</td>\n",
       "      <td>-0.338550</td>\n",
       "      <td>-0.374600</td>\n",
       "      <td>-0.293180</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3224</th>\n",
       "      <td>7-nights-of-darkness</td>\n",
       "      <td>0.073396</td>\n",
       "      <td>1.27930</td>\n",
       "      <td>0.177420</td>\n",
       "      <td>-0.040782</td>\n",
       "      <td>0.497450</td>\n",
       "      <td>-0.584770</td>\n",
       "      <td>0.293290</td>\n",
       "      <td>-0.396850</td>\n",
       "      <td>-0.428970</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.282390</td>\n",
       "      <td>-0.111460</td>\n",
       "      <td>0.070229</td>\n",
       "      <td>0.269600</td>\n",
       "      <td>-0.859810</td>\n",
       "      <td>0.212880</td>\n",
       "      <td>0.098215</td>\n",
       "      <td>0.234330</td>\n",
       "      <td>-0.276420</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6579</th>\n",
       "      <td>leprechaun</td>\n",
       "      <td>-0.041875</td>\n",
       "      <td>1.28350</td>\n",
       "      <td>0.281950</td>\n",
       "      <td>0.038457</td>\n",
       "      <td>0.424200</td>\n",
       "      <td>-0.339820</td>\n",
       "      <td>-0.019335</td>\n",
       "      <td>-0.277120</td>\n",
       "      <td>-0.011810</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066274</td>\n",
       "      <td>0.652900</td>\n",
       "      <td>0.301220</td>\n",
       "      <td>0.251200</td>\n",
       "      <td>-1.219400</td>\n",
       "      <td>-0.160230</td>\n",
       "      <td>1.290700</td>\n",
       "      <td>-0.001679</td>\n",
       "      <td>-0.219830</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3669</th>\n",
       "      <td>smiley</td>\n",
       "      <td>0.404830</td>\n",
       "      <td>1.29300</td>\n",
       "      <td>0.197940</td>\n",
       "      <td>-0.084730</td>\n",
       "      <td>0.734840</td>\n",
       "      <td>-0.458780</td>\n",
       "      <td>0.357220</td>\n",
       "      <td>-0.282590</td>\n",
       "      <td>-0.185080</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.364940</td>\n",
       "      <td>-0.185820</td>\n",
       "      <td>0.230070</td>\n",
       "      <td>0.339690</td>\n",
       "      <td>-0.633890</td>\n",
       "      <td>0.111040</td>\n",
       "      <td>-0.098920</td>\n",
       "      <td>0.087116</td>\n",
       "      <td>-0.099673</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14241</th>\n",
       "      <td>hansel-&amp;-gretel</td>\n",
       "      <td>0.284940</td>\n",
       "      <td>1.30230</td>\n",
       "      <td>0.411830</td>\n",
       "      <td>-0.151990</td>\n",
       "      <td>0.337630</td>\n",
       "      <td>-0.499920</td>\n",
       "      <td>0.515140</td>\n",
       "      <td>-0.131930</td>\n",
       "      <td>-0.391120</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.292590</td>\n",
       "      <td>-0.356530</td>\n",
       "      <td>0.155450</td>\n",
       "      <td>0.369210</td>\n",
       "      <td>-0.497050</td>\n",
       "      <td>-0.031514</td>\n",
       "      <td>-0.016704</td>\n",
       "      <td>0.307630</td>\n",
       "      <td>0.050631</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8355</th>\n",
       "      <td>the-expedition</td>\n",
       "      <td>0.167440</td>\n",
       "      <td>1.30520</td>\n",
       "      <td>0.383370</td>\n",
       "      <td>-0.202280</td>\n",
       "      <td>0.390840</td>\n",
       "      <td>-0.395150</td>\n",
       "      <td>0.493710</td>\n",
       "      <td>-0.732500</td>\n",
       "      <td>-0.199460</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.311600</td>\n",
       "      <td>0.268580</td>\n",
       "      <td>0.221270</td>\n",
       "      <td>0.518170</td>\n",
       "      <td>-1.094200</td>\n",
       "      <td>0.022410</td>\n",
       "      <td>-0.060406</td>\n",
       "      <td>-0.198600</td>\n",
       "      <td>-0.366530</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10743</th>\n",
       "      <td>dirt-boy</td>\n",
       "      <td>0.199730</td>\n",
       "      <td>1.30550</td>\n",
       "      <td>0.498320</td>\n",
       "      <td>0.017503</td>\n",
       "      <td>0.519120</td>\n",
       "      <td>-0.489560</td>\n",
       "      <td>-0.001570</td>\n",
       "      <td>-0.563280</td>\n",
       "      <td>-0.351160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227520</td>\n",
       "      <td>0.319350</td>\n",
       "      <td>-0.039008</td>\n",
       "      <td>0.414800</td>\n",
       "      <td>-0.784100</td>\n",
       "      <td>0.171880</td>\n",
       "      <td>0.317990</td>\n",
       "      <td>0.264810</td>\n",
       "      <td>-0.184580</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>house</td>\n",
       "      <td>-0.163390</td>\n",
       "      <td>1.30970</td>\n",
       "      <td>0.163270</td>\n",
       "      <td>-0.168490</td>\n",
       "      <td>0.584300</td>\n",
       "      <td>-0.342430</td>\n",
       "      <td>0.741830</td>\n",
       "      <td>-0.217880</td>\n",
       "      <td>-0.322920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.315490</td>\n",
       "      <td>-0.654700</td>\n",
       "      <td>0.135470</td>\n",
       "      <td>0.088469</td>\n",
       "      <td>0.209350</td>\n",
       "      <td>0.741020</td>\n",
       "      <td>0.261690</td>\n",
       "      <td>-0.168590</td>\n",
       "      <td>-0.013979</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8163</th>\n",
       "      <td>the-wicked</td>\n",
       "      <td>0.726880</td>\n",
       "      <td>1.31150</td>\n",
       "      <td>0.572190</td>\n",
       "      <td>-0.220950</td>\n",
       "      <td>0.565800</td>\n",
       "      <td>-0.413390</td>\n",
       "      <td>0.504570</td>\n",
       "      <td>-0.182280</td>\n",
       "      <td>-0.699770</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.305430</td>\n",
       "      <td>0.156170</td>\n",
       "      <td>0.143930</td>\n",
       "      <td>0.063520</td>\n",
       "      <td>-0.794940</td>\n",
       "      <td>0.140930</td>\n",
       "      <td>0.134370</td>\n",
       "      <td>0.243140</td>\n",
       "      <td>0.040612</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6692</th>\n",
       "      <td>inkubus</td>\n",
       "      <td>0.101060</td>\n",
       "      <td>1.31830</td>\n",
       "      <td>0.339300</td>\n",
       "      <td>-0.326600</td>\n",
       "      <td>0.443050</td>\n",
       "      <td>-0.317240</td>\n",
       "      <td>0.332910</td>\n",
       "      <td>-0.373850</td>\n",
       "      <td>-0.133730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155490</td>\n",
       "      <td>-0.130960</td>\n",
       "      <td>0.203870</td>\n",
       "      <td>0.314300</td>\n",
       "      <td>-0.667320</td>\n",
       "      <td>-0.178170</td>\n",
       "      <td>0.146120</td>\n",
       "      <td>0.465730</td>\n",
       "      <td>-0.239560</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7429</th>\n",
       "      <td>devil-girl-from-mars---1954</td>\n",
       "      <td>-0.415550</td>\n",
       "      <td>1.32120</td>\n",
       "      <td>0.219530</td>\n",
       "      <td>-0.642320</td>\n",
       "      <td>0.192490</td>\n",
       "      <td>-0.095810</td>\n",
       "      <td>0.365140</td>\n",
       "      <td>-0.000282</td>\n",
       "      <td>-0.082966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294450</td>\n",
       "      <td>-0.145080</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.341220</td>\n",
       "      <td>-0.980880</td>\n",
       "      <td>0.074777</td>\n",
       "      <td>0.402840</td>\n",
       "      <td>-0.343140</td>\n",
       "      <td>-0.106790</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8239</th>\n",
       "      <td>hayride</td>\n",
       "      <td>-0.004426</td>\n",
       "      <td>1.35590</td>\n",
       "      <td>0.699870</td>\n",
       "      <td>0.140020</td>\n",
       "      <td>0.463300</td>\n",
       "      <td>-0.356150</td>\n",
       "      <td>0.489790</td>\n",
       "      <td>-0.327510</td>\n",
       "      <td>-0.146490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086660</td>\n",
       "      <td>-0.028786</td>\n",
       "      <td>-0.061596</td>\n",
       "      <td>0.424220</td>\n",
       "      <td>-0.934160</td>\n",
       "      <td>0.263110</td>\n",
       "      <td>0.098474</td>\n",
       "      <td>0.124430</td>\n",
       "      <td>-0.252220</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5488</th>\n",
       "      <td>st-francisville-experiment</td>\n",
       "      <td>0.594280</td>\n",
       "      <td>1.37460</td>\n",
       "      <td>0.159240</td>\n",
       "      <td>0.294830</td>\n",
       "      <td>0.310370</td>\n",
       "      <td>-0.320960</td>\n",
       "      <td>0.453130</td>\n",
       "      <td>-0.391520</td>\n",
       "      <td>-0.350300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.252530</td>\n",
       "      <td>0.080512</td>\n",
       "      <td>0.582530</td>\n",
       "      <td>0.621430</td>\n",
       "      <td>-0.921960</td>\n",
       "      <td>0.457340</td>\n",
       "      <td>-0.270080</td>\n",
       "      <td>-0.209620</td>\n",
       "      <td>-0.061024</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13967</th>\n",
       "      <td>eating-out</td>\n",
       "      <td>-0.015435</td>\n",
       "      <td>1.37880</td>\n",
       "      <td>1.093000</td>\n",
       "      <td>-1.394200</td>\n",
       "      <td>0.203670</td>\n",
       "      <td>-0.007629</td>\n",
       "      <td>1.539300</td>\n",
       "      <td>0.068152</td>\n",
       "      <td>-0.039998</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051227</td>\n",
       "      <td>0.461780</td>\n",
       "      <td>-0.666470</td>\n",
       "      <td>0.314500</td>\n",
       "      <td>-0.672650</td>\n",
       "      <td>0.142970</td>\n",
       "      <td>0.672780</td>\n",
       "      <td>-0.165270</td>\n",
       "      <td>0.298230</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9303</th>\n",
       "      <td>devil-girl-from-mars</td>\n",
       "      <td>-0.380250</td>\n",
       "      <td>1.40810</td>\n",
       "      <td>0.319630</td>\n",
       "      <td>-0.595160</td>\n",
       "      <td>0.255510</td>\n",
       "      <td>-0.155130</td>\n",
       "      <td>0.400970</td>\n",
       "      <td>-0.059640</td>\n",
       "      <td>-0.231070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271510</td>\n",
       "      <td>-0.217690</td>\n",
       "      <td>-0.141620</td>\n",
       "      <td>0.457150</td>\n",
       "      <td>-0.877570</td>\n",
       "      <td>-0.087209</td>\n",
       "      <td>0.448630</td>\n",
       "      <td>-0.237650</td>\n",
       "      <td>-0.085974</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16112</th>\n",
       "      <td>the-vampire's-ghost</td>\n",
       "      <td>0.259260</td>\n",
       "      <td>1.41050</td>\n",
       "      <td>0.169380</td>\n",
       "      <td>-0.191120</td>\n",
       "      <td>0.428920</td>\n",
       "      <td>-0.384320</td>\n",
       "      <td>0.676020</td>\n",
       "      <td>0.360910</td>\n",
       "      <td>-0.643190</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.250390</td>\n",
       "      <td>0.214680</td>\n",
       "      <td>-0.013039</td>\n",
       "      <td>0.257710</td>\n",
       "      <td>-0.543890</td>\n",
       "      <td>-0.067255</td>\n",
       "      <td>0.114840</td>\n",
       "      <td>-0.353730</td>\n",
       "      <td>-0.109080</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8842</th>\n",
       "      <td>reverb</td>\n",
       "      <td>0.385900</td>\n",
       "      <td>1.43540</td>\n",
       "      <td>0.336930</td>\n",
       "      <td>-0.180910</td>\n",
       "      <td>0.293460</td>\n",
       "      <td>-0.491040</td>\n",
       "      <td>0.545990</td>\n",
       "      <td>-0.438450</td>\n",
       "      <td>-0.465200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.070905</td>\n",
       "      <td>0.134770</td>\n",
       "      <td>0.228590</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>-0.692790</td>\n",
       "      <td>0.342540</td>\n",
       "      <td>-0.029640</td>\n",
       "      <td>0.162290</td>\n",
       "      <td>-0.425270</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4941</th>\n",
       "      <td>the-possessed</td>\n",
       "      <td>-0.176690</td>\n",
       "      <td>1.45420</td>\n",
       "      <td>-0.159550</td>\n",
       "      <td>-0.406830</td>\n",
       "      <td>0.222310</td>\n",
       "      <td>-0.475550</td>\n",
       "      <td>0.475400</td>\n",
       "      <td>-0.330620</td>\n",
       "      <td>-0.009964</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178060</td>\n",
       "      <td>0.010007</td>\n",
       "      <td>-0.149630</td>\n",
       "      <td>0.122780</td>\n",
       "      <td>-0.953900</td>\n",
       "      <td>0.514980</td>\n",
       "      <td>-0.212040</td>\n",
       "      <td>0.353900</td>\n",
       "      <td>-0.547180</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17486</th>\n",
       "      <td>zombie</td>\n",
       "      <td>0.089026</td>\n",
       "      <td>1.90000</td>\n",
       "      <td>0.473820</td>\n",
       "      <td>-0.378750</td>\n",
       "      <td>0.768290</td>\n",
       "      <td>0.114150</td>\n",
       "      <td>1.087200</td>\n",
       "      <td>0.324590</td>\n",
       "      <td>-1.219400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096370</td>\n",
       "      <td>0.162470</td>\n",
       "      <td>-0.578790</td>\n",
       "      <td>0.119900</td>\n",
       "      <td>-1.728900</td>\n",
       "      <td>0.504240</td>\n",
       "      <td>0.030732</td>\n",
       "      <td>1.086600</td>\n",
       "      <td>-0.166350</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17989 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 0         1        2    \\\n",
       "1389                             dolphin-tale-(2011)  0.346010 -1.21530   \n",
       "1227                         ballykissangel-season-4  0.294050 -1.06340   \n",
       "1465                          magic-in-the-moonlight  0.290870 -1.02390   \n",
       "1033                         ballykissangel-season-3  0.133780 -0.99584   \n",
       "2535                                     elsa-&-fred  0.153820 -0.98498   \n",
       "7084                                      parenthood -0.020399 -0.97926   \n",
       "1670                         ballykissangel-season-5  0.234530 -0.97406   \n",
       "1534                         ballykissangel-season-6  0.192480 -0.97108   \n",
       "1744                       _duplicate_dolphin-tale-2  0.198280 -0.95839   \n",
       "6390                                  dolphin-tale-2  0.464780 -0.94600   \n",
       "359                                            annie  0.153970 -0.94488   \n",
       "1264                       when-the-game-stands-tall  0.316720 -0.94254   \n",
       "2981                                two-weeks-notice -0.132450 -0.91895   \n",
       "3216                                    larry-crowne -0.206660 -0.91711   \n",
       "377                                   22-jump-street  0.676620 -0.91498   \n",
       "644                                   the-best-of-me  0.808270 -0.91143   \n",
       "562                          ballykissangel-season-1  0.111000 -0.91099   \n",
       "1706                   battlestar-galactica-season-4  1.183200 -0.90504   \n",
       "2033                   battlestar-galactica-season-3  1.055900 -0.89351   \n",
       "826                          ballykissangel-season-2  0.187260 -0.85399   \n",
       "4515                  19-kids-and-counting-season-15  0.373040 -0.85399   \n",
       "324                                     the-homesman  0.441740 -0.85202   \n",
       "1146                                  and-so-it-goes  0.590990 -0.85195   \n",
       "1228                   battlestar-galactica-season-1  1.071700 -0.84397   \n",
       "3712                  19-kids-and-counting-season-14  0.279580 -0.82941   \n",
       "1500                             night-at-the-museum  0.542780 -0.82645   \n",
       "1250                                  unknown-(2011)  0.073504 -0.82643   \n",
       "592                                   21-jump-street  0.749020 -0.80574   \n",
       "7267                  19-kids-and-counting-season-12  0.273110 -0.80499   \n",
       "168            the-hundred-foot-journey-(theatrical)  0.348380 -0.80358   \n",
       "...                                              ...       ...      ...   \n",
       "10980                         the-house-of-the-devil  0.653120  1.21840   \n",
       "9545   house-of-seven-corpses---digitally-remastered -0.287750  1.21910   \n",
       "5078                             cockneys-vs-zombies  0.434460  1.22770   \n",
       "14117                                  dead-end-road  0.276270  1.23440   \n",
       "10805                                  the-poisoning  0.318350  1.23690   \n",
       "5327                                    dark-spirits  0.229590  1.23720   \n",
       "7727                             babysitter-massacre -0.068865  1.24140   \n",
       "10468                                     the-mirror  0.632140  1.24220   \n",
       "12225                                  house-of-boys  0.185350  1.25210   \n",
       "5756                           american-ghost-hunter  0.458570  1.25820   \n",
       "11451                                          shock -0.124160  1.26160   \n",
       "11441                                        haunted  0.515690  1.26660   \n",
       "3224                            7-nights-of-darkness  0.073396  1.27930   \n",
       "6579                                      leprechaun -0.041875  1.28350   \n",
       "3669                                          smiley  0.404830  1.29300   \n",
       "14241                                hansel-&-gretel  0.284940  1.30230   \n",
       "8355                                  the-expedition  0.167440  1.30520   \n",
       "10743                                       dirt-boy  0.199730  1.30550   \n",
       "2019                                           house -0.163390  1.30970   \n",
       "8163                                      the-wicked  0.726880  1.31150   \n",
       "6692                                         inkubus  0.101060  1.31830   \n",
       "7429                     devil-girl-from-mars---1954 -0.415550  1.32120   \n",
       "8239                                         hayride -0.004426  1.35590   \n",
       "5488                      st-francisville-experiment  0.594280  1.37460   \n",
       "13967                                     eating-out -0.015435  1.37880   \n",
       "9303                            devil-girl-from-mars -0.380250  1.40810   \n",
       "16112                            the-vampire's-ghost  0.259260  1.41050   \n",
       "8842                                          reverb  0.385900  1.43540   \n",
       "4941                                   the-possessed -0.176690  1.45420   \n",
       "17486                                         zombie  0.089026  1.90000   \n",
       "\n",
       "            3         4         5         6         7         8         9    \\\n",
       "1389   0.340410  0.342540  0.244270 -0.124290  0.199470  0.111490 -0.222460   \n",
       "1227  -0.509860 -0.511100  0.668010 -0.346700  0.276550  0.849240 -0.303180   \n",
       "1465   0.041772  0.047178  0.011426  0.503290 -0.295080 -0.024971  0.268970   \n",
       "1033  -0.526500 -0.530670  0.571570 -0.424000  0.269610  0.819640 -0.225040   \n",
       "2535   0.336290 -0.416870 -0.156080 -0.085197  0.190580 -0.083176  0.181380   \n",
       "7084   0.530570 -0.290400  0.303190 -0.433550  0.216080  0.907090  0.174200   \n",
       "1670  -0.543030 -0.511390  0.702520 -0.333430  0.241710  0.837990 -0.357520   \n",
       "1534  -0.644340 -0.557940  0.770590 -0.378840  0.232130  0.898700 -0.240380   \n",
       "1744   0.067340  0.259550  0.152170  0.018251  0.424540  0.073161  0.297960   \n",
       "6390   0.362550  0.284970  0.105530 -0.097234  0.153650  0.186220 -0.073721   \n",
       "359   -0.062939  0.445840  0.191720  0.436570  0.418390  0.064427 -0.247860   \n",
       "1264   0.085712  0.375330  0.122100  0.257720  0.523790  0.051536  0.341050   \n",
       "2981   0.361600  0.022252  0.236440 -0.631070  0.638880  0.308370 -0.185250   \n",
       "3216   0.072095 -0.470040  0.048091 -0.826470 -0.201500  0.032002 -0.155420   \n",
       "377    0.196700  0.169690  0.333210  0.099109  0.324770 -0.081610  0.279850   \n",
       "644    0.810330  0.343120  0.434360  0.001475  0.402960 -0.286130  0.271720   \n",
       "562   -0.578590 -0.501090  0.612570 -0.318790  0.336380  0.882090 -0.258430   \n",
       "1706  -0.725400  0.423260  0.119350 -0.166350 -0.208910  0.229740 -0.543080   \n",
       "2033  -0.720050  0.412110  0.085968 -0.212000 -0.227350  0.242560 -0.462240   \n",
       "826   -0.631520 -0.587410  0.691380 -0.365080  0.305770  0.932310 -0.279610   \n",
       "4515   0.326240 -0.067104  0.411840 -0.430930 -0.272100  0.491620 -0.116690   \n",
       "324   -0.263110  0.289150  0.213340  0.468460  0.126710 -0.326740  0.250200   \n",
       "1146   0.065879  0.262450 -0.009538  0.034869  0.054148 -0.129880  0.291870   \n",
       "1228  -0.784330  0.449860  0.089941 -0.122500 -0.216270  0.270300 -0.474800   \n",
       "3712   0.101440  0.007456  0.376350 -0.431700 -0.284960  0.296750 -0.022661   \n",
       "1500  -0.065089  0.353120  0.509560  0.144360  0.255680  0.693270  0.367980   \n",
       "1250   0.081969  0.229590  0.258430 -0.544410 -0.369740 -1.028000  0.463530   \n",
       "592    0.472570 -0.038414  0.187420 -0.311080  0.230690 -0.395380  0.331470   \n",
       "7267   0.178160  0.031623  0.445770 -0.605280 -0.256280  0.393220 -0.102620   \n",
       "168   -0.039238  0.194060 -0.230350  0.327890  0.467510 -0.053023  0.339510   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "10980  0.821840 -0.081862  0.274490 -0.390090  0.001782 -0.254810 -0.344880   \n",
       "9545   0.220880 -0.358760  0.110030 -0.102660  0.650560  0.101980  0.052501   \n",
       "5078   0.360630 -0.306820  0.678790 -0.409140  0.112300  0.127970 -0.732510   \n",
       "14117  0.308260  0.063378  0.261360 -0.525040  0.052046 -0.313790 -0.360550   \n",
       "10805  0.620960 -0.265810  0.220390 -0.834200 -0.110180 -0.574520 -0.379230   \n",
       "5327  -0.115510 -0.158250  0.402220 -0.528920  0.466590 -0.285090 -0.275170   \n",
       "7727   0.609320 -0.226250  0.463110 -0.311060 -0.028434 -0.667130 -0.586830   \n",
       "10468  0.344830  0.270070  0.300360 -0.596070 -0.024114 -0.174280  0.206920   \n",
       "12225  0.983610 -0.390620  0.062326 -0.666710  0.494770 -0.191150 -0.330830   \n",
       "5756   0.045722 -0.417780  0.356100 -0.227020  0.267290 -0.531090 -0.076302   \n",
       "11451  0.428390 -0.345890  0.130740 -0.445970 -0.079599 -0.005649 -0.267340   \n",
       "11441  0.468900 -0.600770 -0.123510 -0.708450  0.684740 -0.364210 -0.551660   \n",
       "3224   0.177420 -0.040782  0.497450 -0.584770  0.293290 -0.396850 -0.428970   \n",
       "6579   0.281950  0.038457  0.424200 -0.339820 -0.019335 -0.277120 -0.011810   \n",
       "3669   0.197940 -0.084730  0.734840 -0.458780  0.357220 -0.282590 -0.185080   \n",
       "14241  0.411830 -0.151990  0.337630 -0.499920  0.515140 -0.131930 -0.391120   \n",
       "8355   0.383370 -0.202280  0.390840 -0.395150  0.493710 -0.732500 -0.199460   \n",
       "10743  0.498320  0.017503  0.519120 -0.489560 -0.001570 -0.563280 -0.351160   \n",
       "2019   0.163270 -0.168490  0.584300 -0.342430  0.741830 -0.217880 -0.322920   \n",
       "8163   0.572190 -0.220950  0.565800 -0.413390  0.504570 -0.182280 -0.699770   \n",
       "6692   0.339300 -0.326600  0.443050 -0.317240  0.332910 -0.373850 -0.133730   \n",
       "7429   0.219530 -0.642320  0.192490 -0.095810  0.365140 -0.000282 -0.082966   \n",
       "8239   0.699870  0.140020  0.463300 -0.356150  0.489790 -0.327510 -0.146490   \n",
       "5488   0.159240  0.294830  0.310370 -0.320960  0.453130 -0.391520 -0.350300   \n",
       "13967  1.093000 -1.394200  0.203670 -0.007629  1.539300  0.068152 -0.039998   \n",
       "9303   0.319630 -0.595160  0.255510 -0.155130  0.400970 -0.059640 -0.231070   \n",
       "16112  0.169380 -0.191120  0.428920 -0.384320  0.676020  0.360910 -0.643190   \n",
       "8842   0.336930 -0.180910  0.293460 -0.491040  0.545990 -0.438450 -0.465200   \n",
       "4941  -0.159550 -0.406830  0.222310 -0.475550  0.475400 -0.330620 -0.009964   \n",
       "17486  0.473820 -0.378750  0.768290  0.114150  1.087200  0.324590 -1.219400   \n",
       "\n",
       "       ...       92        93        94        95        96        97   \\\n",
       "1389   ...  0.518440 -0.356960 -0.189450 -0.030467  0.153040  0.479830   \n",
       "1227   ...  0.537220 -1.038800  0.046100  0.533110 -0.734940  0.322320   \n",
       "1465   ...  0.573580 -0.134020  0.009749 -0.219710  0.291190  0.666080   \n",
       "1033   ...  0.525430 -1.036100  0.051450  0.440010 -0.677340  0.291310   \n",
       "2535   ...  0.339600 -0.258520  0.193290 -0.299680  0.021829  0.245140   \n",
       "7084   ... -0.131430 -0.219640 -0.270230  0.049698  0.345020  0.504330   \n",
       "1670   ...  0.536210 -1.081400  0.102900  0.565180 -0.662990  0.322180   \n",
       "1534   ...  0.584620 -1.022500  0.017666  0.551760 -0.543840  0.312710   \n",
       "1744   ...  0.507800 -0.243910  0.287240 -0.223820 -0.021888  0.409410   \n",
       "6390   ...  0.472220 -0.265960 -0.011108  0.102090 -0.093313  0.695400   \n",
       "359    ...  0.059718  0.252790  0.454990  0.424590  0.294980  0.318200   \n",
       "1264   ...  0.091834  0.085982  0.565360  0.670160  0.095864  0.115820   \n",
       "2981   ...  0.611070 -0.124620 -0.035273 -0.061694 -0.054208  0.095180   \n",
       "3216   ...  0.715600 -0.210930 -0.040540 -0.291160  0.333550  0.096930   \n",
       "377    ...  0.106810 -0.270520 -0.229700 -0.012904  0.009360 -0.085899   \n",
       "644    ... -0.506200  0.284910  0.582150  0.188740 -0.425150  0.330020   \n",
       "562    ...  0.600910 -1.169900  0.010465  0.401500 -0.603370  0.213050   \n",
       "1706   ...  0.795430 -0.941590 -0.482060  0.669740 -0.160050 -0.256730   \n",
       "2033   ...  0.775890 -0.953010 -0.472710  0.628750 -0.159470 -0.245450   \n",
       "826    ...  0.539080 -1.121900 -0.002220  0.402740 -0.638550  0.251080   \n",
       "4515   ... -0.289770 -0.313080 -0.028944  0.623300 -0.473230  1.073500   \n",
       "324    ... -0.275920  0.222660  0.458570  0.054106 -0.345000 -0.219680   \n",
       "1146   ...  0.351060 -0.050863  0.392520  0.143330 -0.131070  0.009270   \n",
       "1228   ...  0.816370 -1.028700 -0.477300  0.618150 -0.096168 -0.296730   \n",
       "3712   ... -0.375810 -0.351220 -0.119080  0.627980 -0.469140  1.164000   \n",
       "1500   ...  0.676880 -0.048863 -0.086648 -0.028571 -0.079083  0.560210   \n",
       "1250   ... -0.148630 -0.401930  0.133940 -0.230820  0.111490  0.091364   \n",
       "592    ... -0.100300 -0.504120 -0.603230 -0.317380  0.059952 -0.140950   \n",
       "7267   ... -0.290450 -0.384140 -0.068507  0.543350 -0.538350  0.994240   \n",
       "168    ...  0.379060 -0.412890  0.135850 -0.023189  0.320610  0.129620   \n",
       "...    ...       ...       ...       ...       ...       ...       ...   \n",
       "10980  ... -0.366080 -0.099618 -0.122710  0.045520 -0.633030  0.474350   \n",
       "9545   ... -0.198170  0.098282  0.241600  0.135420 -1.011700  0.069292   \n",
       "5078   ... -0.075561 -0.246390  0.335780  0.305320 -0.263370  0.148430   \n",
       "14117  ... -0.006900  0.182360  0.129280  0.276290 -0.860920  0.411430   \n",
       "10805  ...  0.058495  0.577340  0.304420 -0.014835 -0.776210  0.557250   \n",
       "5327   ... -0.234990 -0.061648  0.220050 -0.029688 -0.754250 -0.069912   \n",
       "7727   ...  0.331570  0.065704 -0.136800  0.185720 -0.915480  0.019702   \n",
       "10468  ... -1.129700  0.577660  0.294040  0.435910 -0.915000  0.606460   \n",
       "12225  ... -0.200100 -0.043678 -0.310000 -0.154920 -0.193000 -0.199740   \n",
       "5756   ... -0.609130  0.319520  0.777950  0.320020 -1.048500  0.612760   \n",
       "11451  ...  0.315050 -0.178040  0.183740  0.248750 -0.736380  0.703360   \n",
       "11441  ... -0.646810  0.142460  0.874660  0.223050 -0.986230  0.741740   \n",
       "3224   ... -0.282390 -0.111460  0.070229  0.269600 -0.859810  0.212880   \n",
       "6579   ... -0.066274  0.652900  0.301220  0.251200 -1.219400 -0.160230   \n",
       "3669   ... -0.364940 -0.185820  0.230070  0.339690 -0.633890  0.111040   \n",
       "14241  ... -0.292590 -0.356530  0.155450  0.369210 -0.497050 -0.031514   \n",
       "8355   ... -0.311600  0.268580  0.221270  0.518170 -1.094200  0.022410   \n",
       "10743  ...  0.227520  0.319350 -0.039008  0.414800 -0.784100  0.171880   \n",
       "2019   ...  0.315490 -0.654700  0.135470  0.088469  0.209350  0.741020   \n",
       "8163   ... -0.305430  0.156170  0.143930  0.063520 -0.794940  0.140930   \n",
       "6692   ...  0.155490 -0.130960  0.203870  0.314300 -0.667320 -0.178170   \n",
       "7429   ...  0.294450 -0.145080 -0.019163  0.341220 -0.980880  0.074777   \n",
       "8239   ...  0.086660 -0.028786 -0.061596  0.424220 -0.934160  0.263110   \n",
       "5488   ... -0.252530  0.080512  0.582530  0.621430 -0.921960  0.457340   \n",
       "13967  ... -0.051227  0.461780 -0.666470  0.314500 -0.672650  0.142970   \n",
       "9303   ...  0.271510 -0.217690 -0.141620  0.457150 -0.877570 -0.087209   \n",
       "16112  ... -0.250390  0.214680 -0.013039  0.257710 -0.543890 -0.067255   \n",
       "8842   ... -0.070905  0.134770  0.228590  0.161290 -0.692790  0.342540   \n",
       "4941   ... -0.178060  0.010007 -0.149630  0.122780 -0.953900  0.514980   \n",
       "17486  ...  0.096370  0.162470 -0.578790  0.119900 -1.728900  0.504240   \n",
       "\n",
       "            98        99        100  101  \n",
       "1389  -0.377510 -0.314870  0.159240  NaN  \n",
       "1227  -0.654090 -0.153000 -0.665370  NaN  \n",
       "1465  -0.491910 -0.690620  0.032322  NaN  \n",
       "1033  -0.675990 -0.125890 -0.712250  NaN  \n",
       "2535  -0.679390 -0.238150  0.104180  NaN  \n",
       "7084  -0.589540 -0.728440  0.266360  NaN  \n",
       "1670  -0.661990 -0.201630 -0.620240  NaN  \n",
       "1534  -0.878890 -0.186210 -0.691700  NaN  \n",
       "1744  -0.467230 -0.399190 -0.153930  NaN  \n",
       "6390  -0.725310 -0.269940  0.165530  NaN  \n",
       "359    0.041725 -0.517670 -0.267330  NaN  \n",
       "1264  -0.481080 -0.624500  0.430660  NaN  \n",
       "2981   0.058807 -0.530480 -0.213710  NaN  \n",
       "3216  -0.191050 -0.134230 -0.182190  NaN  \n",
       "377    0.139860 -0.815420 -0.270420  NaN  \n",
       "644   -0.921770 -1.167200  0.062326  NaN  \n",
       "562   -0.544310 -0.106420 -0.682050  NaN  \n",
       "1706  -0.046207  0.360100 -0.306730  NaN  \n",
       "2033  -0.070093  0.338980 -0.306950  NaN  \n",
       "826   -0.646130 -0.151480 -0.699530  NaN  \n",
       "4515  -0.567790 -0.732140  0.534450  NaN  \n",
       "324   -0.553560 -0.449300 -0.126700  NaN  \n",
       "1146  -0.729080 -0.600290 -0.097227  NaN  \n",
       "1228   0.022697  0.337690 -0.332930  NaN  \n",
       "3712  -0.582140 -0.732270  0.499270  NaN  \n",
       "1500  -0.249130 -0.049173 -0.320910  NaN  \n",
       "1250   0.053081 -0.074049 -0.343980  NaN  \n",
       "592    0.295240 -0.320220 -0.392600  NaN  \n",
       "7267  -0.536870 -0.649750  0.585930  NaN  \n",
       "168   -0.418920 -0.341420  0.057606  NaN  \n",
       "...         ...       ...       ...  ...  \n",
       "10980 -0.422060  0.058069 -0.020305  NaN  \n",
       "9545  -0.239500 -0.014740 -0.115080  NaN  \n",
       "5078   0.056520 -0.230540  0.175220  NaN  \n",
       "14117 -0.132440  0.057932  0.014218  NaN  \n",
       "10805  0.110740 -0.004656 -0.519130  NaN  \n",
       "5327   0.152440  0.216670 -0.169210  NaN  \n",
       "7727   0.413630 -0.137720 -0.141800  NaN  \n",
       "10468 -0.465340 -0.103170 -0.854010  NaN  \n",
       "12225 -0.183080 -0.161110  0.244080  NaN  \n",
       "5756   0.173380 -0.224510 -0.317890  NaN  \n",
       "11451 -0.395910 -0.011004 -0.222410  NaN  \n",
       "11441 -0.338550 -0.374600 -0.293180  NaN  \n",
       "3224   0.098215  0.234330 -0.276420  NaN  \n",
       "6579   1.290700 -0.001679 -0.219830  NaN  \n",
       "3669  -0.098920  0.087116 -0.099673  NaN  \n",
       "14241 -0.016704  0.307630  0.050631  NaN  \n",
       "8355  -0.060406 -0.198600 -0.366530  NaN  \n",
       "10743  0.317990  0.264810 -0.184580  NaN  \n",
       "2019   0.261690 -0.168590 -0.013979  NaN  \n",
       "8163   0.134370  0.243140  0.040612  NaN  \n",
       "6692   0.146120  0.465730 -0.239560  NaN  \n",
       "7429   0.402840 -0.343140 -0.106790  NaN  \n",
       "8239   0.098474  0.124430 -0.252220  NaN  \n",
       "5488  -0.270080 -0.209620 -0.061024  NaN  \n",
       "13967  0.672780 -0.165270  0.298230  NaN  \n",
       "9303   0.448630 -0.237650 -0.085974  NaN  \n",
       "16112  0.114840 -0.353730 -0.109080  NaN  \n",
       "8842  -0.029640  0.162290 -0.425270  NaN  \n",
       "4941  -0.212040  0.353900 -0.547180  NaN  \n",
       "17486  0.030732  1.086600 -0.166350  NaN  \n",
       "\n",
       "[17989 rows x 102 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.sort_values(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, but there's 100.  Let's reduce this further with t-SNE and map the top 100 titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_titles = vectors[0]\n",
    "vectors = vectors.drop([0, 101], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(perplexity=40, n_components=2, init='pca', n_iter=10000)\n",
    "embeddings = tsne.fit_transform(vectors.values[:100, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pylab\n",
    "%matplotlib inline\n",
    "\n",
    "def plot(embeddings, labels):\n",
    "    pylab.figure(figsize=(20,20))\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = embeddings[i,:]\n",
    "        pylab.scatter(x, y)\n",
    "        pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',\n",
    "                       ha='right', va='bottom')\n",
    "    pylab.show()\n",
    "\n",
    "plot(embeddings, product_titles[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Host\n",
    "\n",
    "Deploy our model to a real-time endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "bt_endpoint = bt.deploy(initial_instance_count = 1,instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try generating predictions for a set of titles (some of which are real, some of which are made up)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"sherlock-season-1\", \n",
    "         \"sherlock-season-2\",\n",
    "         \"sherlock-season-5\",\n",
    "         'arbitrary-sherlock-holmes-string',\n",
    "         'the-imitation-game',\n",
    "         \"abcdefghijklmn\",\n",
    "         \"keeping-up-with-the-kardashians-season-1\"]\n",
    "\n",
    "payload = {\"instances\" : words}\n",
    "\n",
    "response = bt_endpoint.predict(json.dumps(payload))\n",
    "\n",
    "vecs_df = pd.DataFrame(json.loads(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate correlation and distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_df = pd.DataFrame(vecs_df['vector'].values.tolist(), index=vecs_df['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>word</th>\n",
       "      <th>sherlock-season-1</th>\n",
       "      <th>sherlock-season-2</th>\n",
       "      <th>sherlock-season-5</th>\n",
       "      <th>arbitrary-sherlock-holmes-string</th>\n",
       "      <th>the-imitation-game</th>\n",
       "      <th>abcdefghijklmn</th>\n",
       "      <th>keeping-up-with-the-kardashians-season-1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sherlock-season-1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971324</td>\n",
       "      <td>0.929037</td>\n",
       "      <td>0.620655</td>\n",
       "      <td>0.295161</td>\n",
       "      <td>0.202675</td>\n",
       "      <td>0.195227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sherlock-season-2</th>\n",
       "      <td>0.971324</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.935147</td>\n",
       "      <td>0.601866</td>\n",
       "      <td>0.269404</td>\n",
       "      <td>0.211248</td>\n",
       "      <td>0.201740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sherlock-season-5</th>\n",
       "      <td>0.929037</td>\n",
       "      <td>0.935147</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.629472</td>\n",
       "      <td>0.324804</td>\n",
       "      <td>0.204081</td>\n",
       "      <td>0.238709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arbitrary-sherlock-holmes-string</th>\n",
       "      <td>0.620655</td>\n",
       "      <td>0.601866</td>\n",
       "      <td>0.629472</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.395081</td>\n",
       "      <td>0.131239</td>\n",
       "      <td>0.060798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-imitation-game</th>\n",
       "      <td>0.295161</td>\n",
       "      <td>0.269404</td>\n",
       "      <td>0.324804</td>\n",
       "      <td>0.395081</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.361605</td>\n",
       "      <td>0.183265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abcdefghijklmn</th>\n",
       "      <td>0.202675</td>\n",
       "      <td>0.211248</td>\n",
       "      <td>0.204081</td>\n",
       "      <td>0.131239</td>\n",
       "      <td>0.361605</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.312562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keeping-up-with-the-kardashians-season-1</th>\n",
       "      <td>0.195227</td>\n",
       "      <td>0.201740</td>\n",
       "      <td>0.238709</td>\n",
       "      <td>0.060798</td>\n",
       "      <td>0.183265</td>\n",
       "      <td>0.312562</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "word                                      sherlock-season-1  \\\n",
       "word                                                          \n",
       "sherlock-season-1                                  1.000000   \n",
       "sherlock-season-2                                  0.971324   \n",
       "sherlock-season-5                                  0.929037   \n",
       "arbitrary-sherlock-holmes-string                   0.620655   \n",
       "the-imitation-game                                 0.295161   \n",
       "abcdefghijklmn                                     0.202675   \n",
       "keeping-up-with-the-kardashians-season-1           0.195227   \n",
       "\n",
       "word                                      sherlock-season-2  \\\n",
       "word                                                          \n",
       "sherlock-season-1                                  0.971324   \n",
       "sherlock-season-2                                  1.000000   \n",
       "sherlock-season-5                                  0.935147   \n",
       "arbitrary-sherlock-holmes-string                   0.601866   \n",
       "the-imitation-game                                 0.269404   \n",
       "abcdefghijklmn                                     0.211248   \n",
       "keeping-up-with-the-kardashians-season-1           0.201740   \n",
       "\n",
       "word                                      sherlock-season-5  \\\n",
       "word                                                          \n",
       "sherlock-season-1                                  0.929037   \n",
       "sherlock-season-2                                  0.935147   \n",
       "sherlock-season-5                                  1.000000   \n",
       "arbitrary-sherlock-holmes-string                   0.629472   \n",
       "the-imitation-game                                 0.324804   \n",
       "abcdefghijklmn                                     0.204081   \n",
       "keeping-up-with-the-kardashians-season-1           0.238709   \n",
       "\n",
       "word                                      arbitrary-sherlock-holmes-string  \\\n",
       "word                                                                         \n",
       "sherlock-season-1                                                 0.620655   \n",
       "sherlock-season-2                                                 0.601866   \n",
       "sherlock-season-5                                                 0.629472   \n",
       "arbitrary-sherlock-holmes-string                                  1.000000   \n",
       "the-imitation-game                                                0.395081   \n",
       "abcdefghijklmn                                                    0.131239   \n",
       "keeping-up-with-the-kardashians-season-1                          0.060798   \n",
       "\n",
       "word                                      the-imitation-game  abcdefghijklmn  \\\n",
       "word                                                                           \n",
       "sherlock-season-1                                   0.295161        0.202675   \n",
       "sherlock-season-2                                   0.269404        0.211248   \n",
       "sherlock-season-5                                   0.324804        0.204081   \n",
       "arbitrary-sherlock-holmes-string                    0.395081        0.131239   \n",
       "the-imitation-game                                  1.000000        0.361605   \n",
       "abcdefghijklmn                                      0.361605        1.000000   \n",
       "keeping-up-with-the-kardashians-season-1            0.183265        0.312562   \n",
       "\n",
       "word                                      keeping-up-with-the-kardashians-season-1  \n",
       "word                                                                                \n",
       "sherlock-season-1                                                         0.195227  \n",
       "sherlock-season-2                                                         0.201740  \n",
       "sherlock-season-5                                                         0.238709  \n",
       "arbitrary-sherlock-holmes-string                                          0.060798  \n",
       "the-imitation-game                                                        0.183265  \n",
       "abcdefghijklmn                                                            0.312562  \n",
       "keeping-up-with-the-kardashians-season-1                                  1.000000  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs_df = vecs_df.transpose()\n",
    "vecs_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sherlock-season-1: 0.0\n",
      "sherlock-season-2: 0.6969038400764177\n",
      "sherlock-season-5: 1.6660356722665621\n",
      "arbitrary-sherlock-holmes-string: 7.6473971154368225\n",
      "the-imitation-game: 13.525367997084116\n",
      "abcdefghijklmn: 11.864477970106547\n",
      "keeping-up-with-the-kardashians-season-1: 20.66670294081374\n"
     ]
    }
   ],
   "source": [
    "for column in vecs_df.columns:\n",
    "    print(column + ':', np.sum((vecs_df[column] - vecs_df['sherlock-season-1']) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative to 'sherlock-season-1':\n",
    "- 'sherlock-season-5' is made up, but relates well with 'sherlock-season-1' and 'sherlock-season-2'\n",
    "- 'arbitrary-sherlock-holmes-string' is also made up and relates less well but still fairly strong\n",
    "- 'the-imitation-game' is another popular Prime video title starring Benedict Cumberbatch and has a moderate relationship, but worse than the arbitrary Sherlock title\n",
    "- 'abcdefghijklmn' is made up and relates even worse\n",
    "- 'keeping-up-with-the-kardashians-season-1' somehow manages to relate even worse\n",
    "\n",
    "Clean-up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_endpoint.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "# Wrap-up\n",
    "\n",
    "- Built a recommender system on a large dataset quickly and accurately\n",
    "- Add more features to extend\n",
    "- Compare to other methods\n",
    "- Ensemble two models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
