{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker の Factorization Machines を使用したレコメンダ システムの構築\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## 背景\n",
    "\n",
    "- Factorization Machines とは、因数分解機です。（以下、Factorization Machines）\n",
    "- レコメンダ システムは、アマゾン、並びにネットフリックス Prizeの例からもみれるように、機械学習のカタリストとなりました。\n",
    "  - ネットフリックス Prizeとは：2006年から2009年にかけて、三年間にわたって行なわれた史上最大級のアルゴリズム・コンテスト。賞金100万ドルを賭けて，186ヶ国から4万チームが参戦して争われた壮大なアルゴリズム競争です。\n",
    "- ユーザー・アイテムの Matrix Factorization　（又は、Matrix Decompositionと言い、日本語では行列の分解） は、コア・中核的な手法です。\n",
    "  - 行列の分解とは：行列の行列の積への分解。何の為に分解するのか？行列を分解することで、計算を速く行えるようになる、という実際的なメリットがあったり、その行列の性質がわかったりするからです。\n",
    "- Factorization Machines は、linear prediction（線形予測） とペアワイズ フィーチャの相互作用の因数分解された表現を組み合わせます。\n",
    "\n",
    "$$\\hat{r} = w_0 + \\sum_{i} {w_i x_i} + \\sum_{i} {\\sum_{j > i} {\\langle v_i, v_j \\rangle x_i x_j}}$$\n",
    "\n",
    "- Amazon SageMaker の built-in Factorization Machines は高度にスケーラブルです。\n",
    "\n",
    "---\n",
    "\n",
    "## セットアップ\n",
    "\n",
    "以下のセルを実行する前に：\n",
    "1. マネージメントコンソールにて、SageMaker にてノートブックインスタンスを立ち上げて下さい。\n",
    "2. 作成時に、SageMaker の IAM ポリシーをそのノートブックインスタンスに追加して、S3 の　read/write　アクセスを許可するように設定します。\n",
    "\n",
    "セル１と２は：\n",
    "3. S3 バケット、セッション等の作成。(first code cell)\n",
    "4. 必要なライブラリのインポート (second code cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "base = 'DEMO-loft-recommender'\n",
    "prefix = 'sagemaker/' + base\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import json\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker.predictor import json_deserializer\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## データ\n",
    "\n",
    "[Amazon Reviews AWS Public Dataset](https://s3.amazonaws.com/amazon-reviews-pds/readme.html)\n",
    "Amazonカスタマーレビュー（商品レビュー）は、Amazonの象徴的な商品の1つです。 1995 年の最初のレビューから20 年以上の期間、何百万人ものお客様が、Amazon.com ウェブサイト上の商品に関する意見を述べ経験をシェアするために、1 億以上のレビューを投稿してきました。 これにより、Amazonカスタマーレビューは、自然言語処理（NLP）、情報検索（IR）、機械学習（ML）などの分野の学術研究者のための豊富な情報源となっています。 これに伴い、お客様の製品体験の理解に関する複数の分野におけるさらなる研究を目的として、このデータを公開しています。 具体的には、このデータセットは、顧客の評価と意見のサンプル、地理的・地域全体にわたる製品の認識の変動、およびレビューにおける販促意図またはバイアスを表すために構築されました。\n",
    "\n",
    "その内容は、\n",
    "- 1 から 5 つ星評価。\n",
    "- 2百万以上の お客様によるレビュー。\n",
    "- このノートブックでは、16万以上の デジタルビデオのレビューを使用して、機械学習を行います。 \n",
    "\n",
    "以下のセルを実行し、データセットをノートブックインスタンスの /tmp/recsys/ にダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /tmp/recsys/\n",
    "!aws s3 cp s3://amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz /tmp/recsys/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas の read_csv() を使い、メモリ内の dataframe にデータセットをロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/tmp/recsys/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz', delimiter='\\t',error_bad_lines=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットの列は下記の通りです:\n",
    "\n",
    "- `marketplace`: 2 文字の国コード (このデータセットは、全てUS,「米国」となっています）。\n",
    "- `customer_id`: ランダムに割り当てたお客様番号又はID。 \n",
    "- `review_id`: レビューをユニークに識別できるID。\n",
    "- `product_id`: Amazon 標準識別番号 (ASIN)。  \n",
    "- `product_parent`: そのASINの親。 複数のASIN（同じ商品のカラーバリエーションまたはフォーマットのバリエーション）を１つの親にまとめることができます。\n",
    "- `product_title`: 商品のタイトル。\n",
    "- `product_category`: グループレビューに使用できる幅広い製品カテゴリ（このデータセットの場合はデジタルビデオ）\n",
    "- `star_rating`: レビューの評価 (1 から 5 つ星)。\n",
    "- `helpful_votes`: レビューが役立つと投票された数。\n",
    "- `total_votes`: レビューが受け取った投票総数。\n",
    "- `vine`: レビューは [Vine](https://www.amazon.com/gp/vine/help) のプログラムの一部として書かれているか否か。\n",
    "- `verified_purchase`: 確認済みの購入からのレビューか否か。\n",
    "- `review_headline`: レビューのタイトル。\n",
    "- `review_body`: レビューのテキスト。\n",
    "- `review_date`: レビューが書き込まれた日付。\n",
    "\n",
    "以下のセルで、トレーニングに使用するフィールド・列だけを選び、使用しない列を削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['customer_id', 'product_id', 'product_title', 'star_rating', 'review_date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のセルを実行すると、ほとんどのユーザーは、ほとんどの映画を評価しないことが分かります。 \n",
    "\n",
    "例えば、customersの統計を見てみますと、50パーセンタイルまでカウントが１、すなわちユーザは1つの評価しか投稿していないということになります。\n",
    "2万のユニークなお客様の中でおおよそ半分は投稿数が１つということです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = df['customer_id'].value_counts() #customer_idを数え、その値を返します。\n",
    "products = df['product_id'].value_counts()   #product_idを数え、その値を返します。\n",
    "\n",
    "quantiles = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.96, 0.97, 0.98, 0.99, 1]\n",
    "print('customers count\\n', customers.size)\n",
    "print('customers\\n', customers.quantile(quantiles)) #上記のvalue_counts()で返された値を、統計してみてみます。\n",
    "print('products count \\n', products.size)\n",
    "print('products\\n', products.quantile(quantiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多数の映画を評価していないお客様を除外します。\n",
    "\n",
    "5つ以上投稿したお客様と、10つ以上のレビューを受けた映画だけをキープします。それにより、約14万のお客様に絞り込みました。映画の数は3万８千となりました。（以下、customer_index と product_index をセルにて実行することにより確認できます。\n",
    "\n",
    "そして、その customerId と productId を reduced_df としてメモリにセーブします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = customers[customers >= 5]\n",
    "products = products[products >= 10]\n",
    "\n",
    "reduced_df = df.merge(pd.DataFrame({'customer_id': customers.index})).merge(pd.DataFrame({'product_id': products.index}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "お客様と映画の連番インデックス（sequential index）を作成し user と item と呼びます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = reduced_df['customer_id'].value_counts()\n",
    "products = reduced_df['product_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_index = pd.DataFrame({'customer_id': customers.index, 'user': np.arange(customers.shape[0])})\n",
    "product_index = pd.DataFrame({'product_id': products.index, \n",
    "                              'item': np.arange(products.shape[0]) + customer_index.shape[0]})\n",
    "\n",
    "reduced_df = reduced_df.merge(customer_index).merge(product_index)\n",
    "reduced_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初のレビューからの日数を数え、days_since_firstを作成する（トレンドをキャプチャするfeatureとして使えます）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df['review_date'] = pd.to_datetime(reduced_df['review_date'])\n",
    "customer_first_date = reduced_df.groupby('customer_id')['review_date'].min().reset_index()\n",
    "customer_first_date.columns = ['customer_id', 'first_review_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = reduced_df.merge(customer_first_date)\n",
    "reduced_df['days_since_first'] = (reduced_df['review_date'] - reduced_df['first_review_date']).dt.days\n",
    "reduced_df['days_since_first'] = reduced_df['days_since_first'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のセルにて、学習用のデータととテスト用のデータに分けます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = reduced_df.groupby('customer_id').last().reset_index()\n",
    "\n",
    "train_df = reduced_df.merge(test_df[['customer_id', 'product_id']], \n",
    "                            on=['customer_id', 'product_id'], \n",
    "                            how='outer', \n",
    "                            indicator=True)\n",
    "train_df = train_df[(train_df['_merge'] == 'left_only')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Factorization Machinesは、以下のデータをインプットとして受け取ります。\n",
    "  - Sparse matrix（疎行列又は、スパース行列）。ほとんど0で構成されている行列。\n",
    "  - ターゲット変数は、映画に対するそのお客様（ユーザー）の評価です。\n",
    "  - ユーザーのワンホットエンコーディング ($N$ features)　この場合、$N$は140344。\n",
    "  - 映画（アイテム）のワンホットエンコーディング ($M$ features)　$M$は38385。\n",
    "\n",
    "|Rating|User1|User2|...|UserN|Movie1|Movie2|Movie3|...|MovieM|Feature1|Feature2|...|\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "|4|1|0|...|0|1|0|0|...|0|20|2.2|...|\n",
    "|5|1|0|...|0|0|1|0|...|0|17|9.1|...|\n",
    "|3|0|1|...|0|1|0|0|...|0|3|11.0|...|\n",
    "|4|0|1|...|0|0|0|1|...|0|15|6.4|...|\n",
    "\n",
    "\n",
    "- 完全な行列をメモリに保持したくありません。従って..\n",
    "  - 疎行列を作成する。\n",
    "  - 疎行列は、CPUで効率的に動作するように設計されています。 より密度の高い行列のトレーニングの一部は、GPUで並列化することができます。\n",
    "  \n",
    "以下の function で、drameframe から、scipy の csr_matrix に変換します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csr_matrix(df, num_users, num_items):\n",
    "    feature_dim = num_users + num_items + 1\n",
    "    data = np.concatenate([np.array([1] * df.shape[0]),\n",
    "                           np.array([1] * df.shape[0]),\n",
    "                           df['days_since_first'].values])\n",
    "    row = np.concatenate([np.arange(df.shape[0])] * 3)\n",
    "    col = np.concatenate([df['user'].values,\n",
    "                          df['item'].values,\n",
    "                          np.array([feature_dim - 1] * df.shape[0])])\n",
    "    return csr_matrix((data, (row, col)), \n",
    "                      shape=(df.shape[0], feature_dim), \n",
    "                      dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csr = to_csr_matrix(train_df, customer_index.shape[0], product_index.shape[0])\n",
    "test_csr = to_csr_matrix(test_df, customer_index.shape[0], product_index.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下、SageMakerのfactorization machinesが必要とするスパース・レコードIOに包まれたprotobufに変換します。そして、S3バケットにアップロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_s3_protobuf(csr, label, bucket, prefix, channel='train', splits=10):\n",
    "    indices = np.array_split(np.arange(csr.shape[0]), splits)\n",
    "    for i in range(len(indices)):\n",
    "        index = indices[i]\n",
    "        buf = io.BytesIO()\n",
    "        smac.write_spmatrix_to_sparse_tensor(buf, csr[index, ], label[index])\n",
    "        buf.seek(0)\n",
    "        boto3.client('s3').upload_fileobj(buf, bucket, '{}/{}/data-{}'.format(prefix, channel, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_s3_protobuf(train_csr, train_df['star_rating'].values.astype(np.float32), bucket, prefix)\n",
    "to_s3_protobuf(test_csr, test_df['star_rating'].values.astype(np.float32), bucket, prefix, channel='test', splits=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 学習\n",
    "\n",
    "- トレーニング・ジョブを実行するための [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk) estimatorを作成し、以下の項目を指定します。\n",
    "  - アルゴリズムが保存されているコンテナのイメージ\n",
    "  - IAM ロール\n",
    "  - ハードウェアのセットアップ\n",
    "  - アウトプットを保存する S3 のバケット\n",
    "  - アルゴリズムのハイパーパラメータ\n",
    "    - `feature_dim`: $N + M + 1$ (追加の feature は、トレントをキャプチャする `days_since_first`)\n",
    "    - `num_factors`: 因数分解された交互作用で減少させた dimension\n",
    "    - `epochs`: データセットを通過させる回数\n",
    "- `.fit()` は S3 の学習用とテスト用データを指定し、トレーニングジョブを開始します。\n",
    "- train_instance_count=4 とあるようにdistributedトレーニングの例となります。\n",
    "- この例では、cタイプのインスタンスを利用しています。\n",
    "- また、トレーニング終了後、このインスタンスは直ちに処分され、コスト軽減となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".."
     ]
    }
   ],
   "source": [
    "fm = sagemaker.estimator.Estimator(\n",
    "    sagemaker.amazon.amazon_estimator.get_image_uri(boto3.Session().region_name, 'factorization-machines', 'latest'),\n",
    "    role, \n",
    "    train_instance_count=4, \n",
    "    train_instance_type='ml.c5.2xlarge',\n",
    "    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "    base_job_name=base,\n",
    "    sagemaker_session=sess)\n",
    "\n",
    "fm.set_hyperparameters(\n",
    "    feature_dim=customer_index.shape[0] + product_index.shape[0] + 1,\n",
    "    predictor_type='regressor',\n",
    "    mini_batch_size=1000,\n",
    "    num_factors=256,\n",
    "    epochs=3)\n",
    "\n",
    "fm.fit({'train': sagemaker.s3_input('s3://{}/{}/train/'.format(bucket, prefix), distribution='ShardedByS3Key'), \n",
    "        'test': sagemaker.s3_input('s3://{}/{}/test/'.format(bucket, prefix), distribution='FullyReplicated')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ホスト・デプロイ\n",
    "\n",
    "学習を終えたモデルを、リアルタイムの本番環境へエンドポイントとしてデプロイします。\n",
    "- この際、mタイプのインスタンスを使用します。\n",
    "- initial_instance_count=1 とありますが、エンドポイント利用可能後、APIへのリクエストが増えると自動的にスケールアウトします。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fm_predictor = fm.deploy(instance_type='ml.m4.xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API呼び出しの際に使う、メモリ内のデータをシリアル化する必要があります。predictor にてシリアライザを指定します。　"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fm_serializer(df):\n",
    "    feature_dim = customer_index.shape[0] + product_index.shape[0] + 1\n",
    "    js = {'instances': []}\n",
    "    for index, data in df.iterrows():\n",
    "        js['instances'].append({'data': {'features': {'values': [1, 1, data['days_since_first']],\n",
    "                                                      'keys': [data['user'], data['item'], feature_dim - 1],\n",
    "                                                      'shape': [feature_dim]}}})\n",
    "    return json.dumps(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor.content_type = 'application/json'\n",
    "fm_predictor.serializer = fm_serializer\n",
    "fm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単一のユーザー・アイテムのリアルタイム予測は、以下の用に行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor.predict(test_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "エンドポイントを削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  ハンズオン終了時に、ノートブックインスタンスを停止して下さい! \n",
    "---\n",
    "\n",
    "# 最後に\n",
    "\n",
    "- Factorization Machinesは、大規模なデータセットに対するレコメンダ システムを迅速かつ正確に構築することを可能にします。\n",
    "- やってみること：\n",
    "  - 拡張する為に、フィーチャを追加することを試してみる。　\n",
    "  - Factorization Machines以外の他の方法と比較してみる。\n",
    "  - 2つのモデルをアンサンブルとして使ってみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
